# Redis 高可用

## 使用 Redis 作为缓存

用户的数据一般都是存储于数据库，数据库的数据是存放在磁盘上的，磁盘的读写速度可以说是计算机里最慢的硬件了。**当用户的请求，都访问数据库的话，请求数量一上来，数据库就会很容易奔溃，所以为了避免用户直接访问数据库，会用 Redis 作为缓存层**。

因为 Redis 是内存数据库，我们**可以将数据库的热点数据缓存在 Redis 里**，相当于数据缓存在内存，内存的读写速度比硬盘快好几个数量级，这样大大提高了系统性能。

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511110303065.png" alt="image-20220511110303065" style="zoom: 67%;" />

引入了缓存层，就会有缓存异常的三个问题，分别是**缓存雪崩、缓存击穿、缓存穿透**，同时也会存在**缓存一致性**的问题。

### 缓存雪崩

通常我们为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存。

![image-20220511110556058](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511110556058.png)

如果**有大量缓存数据在同一时间过期（失效）**，或者**Redis 故障宕机**时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是**全部请求都直接访问数据库，从而导致数据库的压力骤增**，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩**的问题。

![image-20220511110840013](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511110840013.png)

根据原因的不同，解决方法也不同。

#### 大量缓存同时过期的解决办法

针对大量数据同时过期而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- **均匀设置过期时间**：如果要给缓存数据设置过期时间，**应该避免将大量的数据设置成同一个过期时间**。

  我们可以在对缓存数据设置过期时间时，**给这些数据的过期时间加上一个随机数**，这样就保证数据不会在同一时间过期。

- **互斥锁**：当业务线程在处理用户请求时，**如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存**（从数据库读取数据，再将数据更新到 Redis 里），当缓存构建完成后，再释放锁。

  未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。

  实现互斥锁的时候，最好设置**超时时间**，不然第一个请求拿到了锁，然后这个请求发生了某种意外而一直阻塞，一直不释放锁，这时其他请求也一直拿不到锁，整个系统就会出现无响应的现象。

- **双 key 策略**：我们对缓存数据可以使用两个 key，一个是**主 key，会设置过期时间**，一个是**备 key，不会设置过期**，它们只是 key 不一样，但是 value 值是一样的，相当于给缓存数据做了个副本。

  当业务线程访问不到主 key 的缓存数据时，就直接返回备 key 的缓存数据，然后在更新缓存的时候，**同时更新主 key 和备 key 的数据。**

- **后台更新缓存**：业务线程不再负责更新缓存，缓存也不设置有效期，而是**让缓存永久有效，并将更新缓存的工作交由后台线程定时更新**。

  事实上，缓存数据不设置有效期，并不是意味着数据一直能在内存里，因为**当系统内存紧张的时候，有些缓存数据会被淘汰**，而在缓存被淘汰到下一次后台定时更新缓存的这段时间内，业务线程读取缓存失败就返回空值，业务的视角就以为是数据丢失了。

  解决上面的问题的方式有两种。

  * 第一种方式，后台线程不仅负责定时更新缓存，而且也负责**频繁地检测缓存是否有效**，检测到缓存失效了，原因可能是系统紧张而被淘汰的，于是就要马上从数据库读取数据，并更新到缓存。

    这种方式的检测时间间隔不能太长，太长也导致用户获取的数据是一个空值而不是真正的数据，所以检测的间隔最好是毫秒级的，但是总归是有个间隔时间，用户体验一般。 

  * 第二种方式，在业务线程发现缓存数据失效后（缓存数据被淘汰），**通过消息队列发送一条消息通知后台线程更新缓存**，后台线程收到消息后，在更新缓存前可以判断缓存是否存在，存在就不执行更新缓存操作；不存在就读取数据库数据，并将数据加载到缓存。

    这种方式相比第一种方式缓存的更新会更及时，用户体验也比较好。

#### Redis 故障宕机解决方法

针对 Redis 故障宕机而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- **服务熔断或请求限流机制**：因为 Redis 故障宕机而导致缓存雪崩问题时，我们可以启动**服务熔断**机制，**暂停业务应用对缓存服务的访问，直接返回错误**，不用再继续访问数据库，从而降低对数据库的访问压力，保证数据库系统的正常运行，然后等到 Redis 恢复正常后，再允许业务应用访问缓存服务。

  服务熔断机制是保护数据库的正常允许，但是暂停了业务应用访问缓存服系统，全部业务都无法正常工作，所以为了减少对业务的影响，我们可以启用**请求限流**机制，**只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务**，等到 Redis 恢复正常并把缓存预热完后，再解除请求限流的机制。

- **构建 Redis 缓存高可用集群**：服务熔断或请求限流机制是缓存雪崩发生后的应对方案，我们最好通过**主从节点的方式构建 Redis 缓存高可用集群**。

  如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务，避免了由于 Redis 故障宕机而导致的缓存雪崩问题。

### 缓冲击穿

我们的业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。

如果缓存中的**某个热点数据过期**了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是**缓存击穿**的问题。

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511143141725.png" alt="image-20220511143141725" style="zoom:67%;" />

我们发现，缓冲击穿也是数据过期，导致大量请求直接打在数据库上造成数据库崩溃，所以你可以把缓存击穿看作是缓存雪崩的一个子集。

应对缓存击穿可以采取前面说到两种方案：

- 互斥锁方案，保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。
- 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；

### 缓冲穿透

当用户访问的数据，**既不在缓存中，也不在数据库中**，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是**缓存穿透**的问题。

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511144305158.png" alt="image-20220511144305158" style="zoom:67%;" />

缓存穿透的发生一般有这两种情况：

- **业务误操作**：缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
- **黑客恶意攻击**：故意大量访问某些读取不存在数据的业务；

应对缓存穿透的方案，常见的方案有三种：

- 对非法请求做出限制：当有大量恶意请求访问不存在的数据的时候，也会发生缓存穿透，因此在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。
- 缓存空值或者默认值：当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。
- **使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在**：我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在。

布隆过滤器是经常使用的方法，**Redis 要使用布隆过滤器很简单，只需要使用 Bitmap 这种数据类型即可**。

### 缓存一致性问题

使用 Redis 做缓存，还有缓存一致性的问题。**由于引入了缓存，那么在数据更新时，不仅要更新数据库，而且要更新缓存，这两个更新操作存在前后的问题**：

- 先更新数据库，再更新缓存
- 先更新缓存，再更新数据库

无论是哪种，都有一定的问题，问题基本上都是由并发引起的。

* 先更新数据库，再更新缓存的问题：

  比如现在有请求 A 和请求 B 两个请求，同时更新同一条数据，则可能出现这样的顺序：

  ![image-20220512105448742](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512105448742.png)

  A 请求先将数据库的数据更新为 1，然后在更新缓存前，请求 B 将数据库的数据更新为 2，紧接着也把缓存更新为 2，然后 A 请求更新缓存为 1。

  此时，数据库中的数据是 2，而缓存中的数据却是 1，**出现了缓存和数据库中的数据不一致的现象**。

* 先更新缓存，再更新数据库，依然还是存在并发的问题：

  ![image-20220512105606152](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512105606152.png)

  

  A 请求先将缓存的数据更新为 1，然后在更新数据库前，B 请求来了， 将缓存的数据更新为 2，紧接着把数据库更新为 2，然后 A 请求将数据库的数据更新为 1。

  此时，数据库中的数据是 1，而缓存中的数据却是 2，**出现了缓存和数据库中的数据不一致的现象**。

所以，**无论是先更新数据库，再更新缓存，还是先更新缓存，再更新数据库，这两个方案都存在并发问题，当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象**。

#### 删除缓存

双更新是没有办法解决缓存一致性问题的，我们可以另辟蹊径：**在更新数据时，不更新缓存，而是删除缓存中的数据**，然后，**到读取数据时，发现缓存中没了数据之后，再从数据库中读取数据，更新到缓存中**，这种策略叫做 Cache Aside（旁路缓冲策略）。

这种策略又可以细分为读策略和写策略：

* 写策略：
  - 更新数据库中的数据
  - 删除缓存中的数据
* 读策略：
  - 如果读取的数据命中了缓存，则直接返回数据
  - 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户

![image-20220512111315680](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512111315680.png)

写策略没有看上去那么简单，也是有顺序的：

- 先删除缓存，再更新数据库

  假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。

  ![image-20220512112925227](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512112925227.png)

  最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致。

  可以看到，**先删除缓存，再更新数据库，在读 + 写并发的时候，还是会出现缓存和数据库的数据不一致的问题**。

- 先更新数据库，再删除缓存

  假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中。

  ![image-20220512113101503](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512113101503.png)

  最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库数据不一致。

  从上面的理论上分析，先更新数据库，再删除缓存也是会出现数据不一致性的问题，**但是在实际中，这个问题出现的概率并不高**，这是因为这个问题的出现需要满足三个条件：

  1. 缓存刚好已失效
  2. 读请求 + 写请求并发
  3. 更新数据库 + 删除缓存的时间，要比读数据库 + 写缓存时间短

  仔细想一下，条件 3 发生的概率其实是非常低的，因为写数据库一般会先加锁，所以**写数据库，通常是要比读数据库的时间更长的**。

#### 重试

无论是更新缓存，还是删除缓存的策略，它们都分为两个步骤，只要第二步发生失败，那么就会导致数据库和缓存不一致，**保证第二步成功执行，就是解决问题的关键**。

想一下，程序在执行过程中发生异常，最简单的解决办法是什么？

答案是：**重试**，无论是先操作缓存，还是先操作数据库，但凡后者执行失败了，我们就可以发起重试，尽可能地去做补偿。

##### 使用消息队列的重试机制

重试当然也存在问题：

- **立即重试很大概率还会失败**
- 重试次数设置多少才合理？因为重试会一直占用这个线程资源，无法服务其它客户端请求

因此，我们不能采取这种同步重试的方式，而应该采取异步重试的方式，**把重试请求写到消息队列中，然后由专门的消费者来重试，直到成功**。

到这里你可能会问，**写消息队列也有可能会失败吧？而且，引入消息队列，这又增加了更多的维护成本，这样做值得吗？**

这个问题很好，但我们思考这样一个问题：如果在执行失败的线程中一直重试，还没等执行成功，此时如果项目重启了，那这次重试请求也就丢失了，那这条数据就一直不一致了。

所以，这里我们必须把重试或第二步操作放到另一个服务中，这个服务用消息队列最为合适。这是因为消息队列的特性，正好符合我们的需求：

- **消息队列保证可靠性**：写到队列中的消息，成功消费之前不会丢失（重启项目也不担心）
- **消息队列保证消息成功投递**：下游从队列拉取消息，成功消费后才会删除消息，否则还会继续投递消息给消费者（符合我们重试的场景）

至于写队列失败和消息队列的维护成本问题：

- **写队列失败**：操作缓存和写消息队列，同时失败的概率其实是很小的
- **维护成本**：我们项目中一般都会用到消息队列，维护成本并没有新增很多

##### 使用订阅数据库日志的重试机制

如果确实不想在应用中去写消息队列，是否有更简单的方案，同时又可以保证一致性呢？

答案是：**订阅数据库变更日志，再操作缓存**。

具体来讲就是，我们的业务应用在修改数据时，只需修改数据库，无需操作缓存。至于什么时候操作缓存？这就和数据库的变更日志有关了。

拿 MySQL 举例，当一条数据发生修改时，MySQL 就会产生一条变更日志（binlog），我们可以订阅这个日志，拿到具体操作的数据，然后再根据这条数据，去删除对应的缓存。

订阅变更日志，目前也有了比较成熟的开源中间件，例如阿里的 canal，使用这种方案的优点在于：

- **无需考虑写消息队列失败情况**：只要写 MySQL 成功，Binlog 肯定会有
- **自动投递到下游队列**：canal 自动把数据库变更日志投递给下游的消息队列

当然，与此同时，我们需要投入精力去维护 canal 的高可用和稳定性。

#### 主从库延迟与延迟双删

未完待续...

## 主从复制

主从复制，是指**将一台 Redis 服务器的数据复制到其他的 Redis 服务器**，前者称为主节点（Master），后者称为从节点（Slave）。**数据的复制是单向的，只能由主节点到从节点**，这解决了多台服务器中不知道以哪台为主的问题。

![image-20220511150933227](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511150933227.png)

默认情况下，每台 Redis 服务器都是主节点，且**一个主节点可以有多个从节点（或没有从节点），但一个从节点只能有一个主节点**。

主从复制有以下优点：

1. 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。

2. 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。

3. 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写 Redis 数据时应用连接主节点，读 Redis 数据时应用连接从节点），分担服务器负载。

   尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高 Redis 服务器的并发量。

   ![image-20220511151001307](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511151001307.png)

4. **高可用基石**：除了上述作用以外，**主从复制还是哨兵和集群能够实施的基础**，因此说主从复制是 Redis 高可用的基础。

同步二字虽然简单，但是其背后的原理却是比较复杂的。

### 主从复制的原理

#### 第一次同步

多台服务器之间要通过什么方式来确定谁是主服务器，或者谁是从服务器呢？

我们可以使用 `REPLICAOF`（Redis 5.0 之前使用 `SLAVEOF`）命令形成主服务器和从服务器的关系。

比如，现在有服务器 A 和 服务器 B，我们在服务器 B 上执行下面这条命令：

```bash
# 在服务器 B 执行这条命令
REPLICAOF <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>
```

接着，服务器 B 就会变成服务器 A 的从服务器，然后与主服务器进行第一次同步。

主从服务器间的第一次同步的过程可分为三个阶段，示意图如下：

![image-20220511151332152](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220511151332152.png)

1. 建立链接、协商同步。

   执行了 `REPLICAOF` 命令后，从服务器就会发送 `PSYNC` 命令到主服务器，表示要进行数据同步。

   `PSYNC` 命令包含两个参数，分别是**主服务器的 runID** 和**复制进度 offset**。

   - runID，每个 Redis 服务器在启动时都会自动生产一个随机的 ID 来唯一标识自己。

     当从服务器和主服务器第一次同步时，因为不知道主服务器的 run ID，所以将其设置为 "?"。

   - offset，表示复制的进度，第一次同步时，其值为 -1。

   主服务器收到 `PSYNC` 命令后，会用 `FULLRESYNC` 作为响应命令返回给对方，并且这个响应命令会带上两个参数：主服务器的 runID 和主服务器目前的复制进度 offset。从主服务器收到响应后，从服务器会记录这两个值。

   `FULLRESYNC` 响应命令的意图是采用**全量复制**的方式，也就是主服务器会把所有的数据都同步给从服务器。所以，**第一阶段的工作是为了全量复制做准备**。

2. 主服务器同步数据给从服务器。

   主服务器要想同步给从服务器，首先会执行 `BGSAVE` 命令**生成 RDB 快照**，然后把快照文件发送给从服务器。

   从服务器受到 RDB 文件后，首先**清空当前所有数据，然后再加载 RDB 快照**。

   我们知道 RDB 的 `BGSAVE` 不会阻塞主进程，但是主进程新写入的数据就没有同步到 RDB 中了，这时如果直接发送过去就会导致主从服务器数据不一致。为了保证主从服务器的数据一致性，**主服务器会将在 RDB 文件生成后收到的写操作命令，写入到 replication buffer 缓冲区里**，这个缓冲区将在第三步被用到。

3. 主服务器发送新写操作命令给从服务器。

   在主服务器生成的 RDB 文件发送后，然后将 replication buffer 缓冲区里所记录的写操作命令发送给从服务器，然后从服务器重新执行这些操作。	

至此，第一次同步已经完成，现在主从服务器是一致的了。

#### 命令传播

主从服务器在完成第一次同步后，**双方之间就会维护一个 TCP 连接**。

![image-20220512103112865](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512103112865.png)

后续主服务器可以**通过连接继续将写操作命令传播给从服务器**，然后从服务器执行该命令，使得与主服务器的数据库状态相同；而且这个连接是长连接，目的是避免频繁的 TCP 连接和断开带来的性能开销。

上面的这个过程被称为**基于长连接的命令传播**，通过这种方式来保证第一次同步后的主从服务器的数据一致性。

#### 分摊主服务器的压力

在前面的分析中，我们可以知道主从服务器在第一次数据同步的过程中，主服务器会做两件耗时的操作：生成 RDB 文件和传输 RDB 文件。

主服务器是可以有多个从服务器的，如果从服务器数量非常多，而且都与主服务器进行全量同步的话，就会带来两个问题：

- 由于是通过 `BGSAVE` 命令来生成 RDB 文件的，那么主服务器就会忙于使用 fork() 创建子进程，如果主服务器的内存数据非大，在执行 fork() 函数时是会阻塞主线程的，从而使得 Redis 无法正常处理请求；
- 传输 RDB 文件会占用主服务器的网络带宽，会对主服务器响应命令请求产生影响。

为了解决这个问题，我们可以把**拥有从服务器的从服务器**当作主服务器，它不仅可以接收主服务器的同步数据，自己也可以同时作为主服务器的形式将数据同步给从服务器，组织形式如下图，就像老板-经理-员工的角色一样：

![image-20220512103254760](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512103254760.png)

要使得从服务器变成其他从服务器的从服务器，我们还是可以调用 `REPLICAOF` 命令，指定一个从服务器作为主服务器即可。

#### 增量复制

主从服务器在完成第一次同步后，就会基于长连接进行命令传播；**如果主从服务器间的网络连接断开了，那么就无法进行命令传播了，这时从服务器的数据就没办法和主服务器保持一致了，客户端就可能从从服务器读到旧的数据**。

如果网络在后来又恢复了，那么怎么保证主从一致性呢？

在 Redis 2.8 之前，如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器重新进行一次全量复制，很明显这样的开销太大了，必须要改进。

从 Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用**增量复制**的方式继续同步，也就是**只会把网络断开期间主服务器接收到的写操作命令，同步给从服务器**。

## 哨兵

主从复制方案的缺点就在于：**当 master 宕机时，我们需要手动把 slave 提升为 master**，这个过程也是需要花费时间的。虽然比恢复数据要快得多，但还是需要人工介入处理。一旦需要人工介入，就必须要算上人的反应时间、操作时间，所以，在这期间你的业务应用依旧会受到影响。

对于这种情况，我们需要一个故障自动切换机制，这就是我们经常听到的**哨兵**所具备的能力。

现在，我们可以引入一个观察者，**让这个观察者去实时监测 master 的健康状态**，这个观察者就是哨兵。

具体来说：

1. 哨兵每间隔一段时间，询问 master 是否正常
2. master 正常回复，表示状态正常，回复超时表示异常
3. 哨兵发现异常，发起主从切换

![image-20220512165710203](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512165710203.png)

有了这个方案，就不需要人去介入处理了，一切就变得自动化了。

但这里还有一个问题，如果 master 状态正常，但这个哨兵在询问 master 时，它们之间的**网络发生了问题**，那这个哨兵可能会误判，这个问题怎么解决？

答案是**部署多个哨兵**，让它们分布在不同的机器上，它们一起监测 master 的状态，流程就变成了这样：

1. 多个哨兵每间隔一段时间，询问 master 是否正常
2. master 正常回复，表示状态正常，回复超时表示异常
3. 一旦有一个哨兵判定 master 异常（不管是否是网络问题），就询问其它哨兵，如果多个哨兵（设置一个阈值）都认为 master 异常了，这才判定 master 确实发生了故障
4. 多个哨兵经过协商后，判定 master 故障，则发起主从切换

哨兵协商判定 master 异常后，这里还有一个问题：**由哪个哨兵来发起主从切换呢？**

答案是**通过选举机制，选出一个领导者，由领导者发起主从切换**。

选举机制的实现方式就和现实中一样：**投票**。在选举哨兵领导者时，我们可以制定这样一个选举规则：

1. 每个哨兵都询问其它哨兵，请求对方为自己投票
2. 每个哨兵只投票给第一个请求投票的哨兵，且只能投票一次
3. 首先拿到超过半数投票的哨兵，当选为领导者，发起主从切换

其实，这个选举的过程就是我们经常听到的，分布式系统领域中的**共识算法**。

这个算法还规定**节点的数量必须是奇数个**，这样可以保证系统中即使有节点发生了故障，剩余超过半数的节点状态正常，依旧可以提供正确的结果，也就是说，这个算法还兼容了存在故障节点的情况。

> 共识算法在分布式系统领域有很多，例如 Paxos、Raft，哨兵选举领导者这个场景，使用的是 Raft 共识算法，因为它足够简单，且易于实现。

现在，我们用多个哨兵共同监测 Redis 的状态，这样一来，就可以避免误判的问题了，架构模型就变成了这样：

![image-20220512170230030](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512170230030.png)

## 集群

随着时间的发展，业务体量开始迎来了爆炸性增长，此时上面的架构模型，还能够承担这么大的流量吗？

我们一起来分析一下：

1. 稳定性：Redis 故障宕机，我们有哨兵 + 副本，可以自动完成主从切换
2. 性能：读请求量增长，我们可以再部署多个 slave，读写分离，分担读压力
3. 性能：写请求量增长，**但我们只有一个 master 实例**，这个实例达到瓶颈怎么办？

看到了么，当你的写请求量越来越大时，一个 master 实例可能就无法承担这么大的写流量了。

要想完美解决这个问题，此时你就需要考虑使用分片**集群**了。

什么是分片集群？简单来讲，一个实例扛不住写压力，那我们可以**部署多个实例，然后把这些实例按照一定规则组织起来，把它们当成一个整体，对外提供服务**。

![image-20220512171450766](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512171450766.png)

这么多实例如何组织呢？我们制定规则如下：

1. 每个节点各自存储一部分数据，所有节点数据之和才是全量数据
2. 制定一个路由规则，对于不同的 key，把它路由到固定一个实例上进行读写

而分片集群根据路由规则所在位置的不同，还可以分为两大类：

1. 客户端分片。客户端分片指的是，key 的路由规则放在客户端来做，就是下面这样：

   ![image-20220512172506056](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512172506056.png)

   这个方案的缺点是，客户端需要维护这个路由规则，也就是说，你需要把路由规则写到你的业务代码中。

   Redis Cluster 采用的方案是把这个路由规则封装成一个模块，当需要使用时，集成这个模块就可以了。

   ![image-20220512172604776](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512172604776.png)

   Redis Cluster 内置了哨兵逻辑，无需再部署哨兵。

2. 服务端分片。这种方案指的是，路由规则不放在客户端来做，而是在客户端和服务端之间增加一个**中间代理层**，这个代理就是我们经常听到的 Proxy，数据的路由规则，就放在这个 Proxy 层来维护。

   Proxy 会把你的请求根据路由规则，转发到对应的 Redis 节点上，而且，当集群实例不足以支撑更大的流量请求时，还可以横向扩容，添加新的 Redis 实例提升性能，这一切对于你的客户端来说，都是透明无感知的。

   业界开源的 Redis 分片集群方案，例如 Twemproxy、Codis 就是采用的这种方案：

   ![image-20220512172922538](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220512172922538.png)
