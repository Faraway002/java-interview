[TOC]

# 场景题专题

## 大数据处理专题

大数据处理通常使用**分治**策略，因为这种情况下通常内存放不下海量的数据，因此需要**把一个文件中的内容按照某个特征划分为多个小文件，使得每个小文件大小不超过内存限制，这样就可以把这个小文件读到内存中进行处理了**。

### 如何从大量的 URL 中找出相同的 URL？

#### 题目描述

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64 B，内存限制是 4G，请找出 a、b 两个文件共同的 URL。

#### 解答思路

每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB，所以直接读取放在内存中不可行。

* 首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000` ，根据计算结果把遍历到的 URL 存储到 $a_0, a_1, a_2, ..., a_i, ..., a_{999}$，这样每个大小约为 300MB。
* 接着，使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 $b_0, b_1, b_2, ..., b_i, ..., b_{999}$ 中。

这样处理过后，**所有可能相同的 URL 都在对应的小文件中**，即 $a_0$ 对应 $b_0$，$a_i$ 对应 $b_i$，... ，$a_{999}$ 对应 $b_{999}$，不对应的小文件不可能有相同的 URL。

* 接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了，算法如下：
  * 遍历 $a_i$（$i ∈ [0,999]$），把 URL 存储到一个 HashSet 集合中；然后遍历 $b_i$ 中每个 URL，看在 HashSet 集合中是否存在。
* 由于 URL 前缀大部分是相同的，比如 `www.baidu.com/s/...` 和 `www.baidu.com/a/...`，这样一来，我们就可以使用**前缀树**，降低存储成本的同时，提高查询效率。

### 如何从大量词语中找出高频词？

#### 题目描述

有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16 B，内存大小限制是 1 MB，要求返回频数最高的 100 个词（Top 100）。

#### 解答思路

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

同时，这题还是经典的 topK 问题，涉及排序，因此我们使用堆这种数据结构来实现。

* 首先遍历大文件，对遍历到的每个词 x，执行 `hash(x) % 5000` ，将结果为 i 的词存放到文件 $a_i$ 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

* 接着**统计每个小文件中出现频数最高的 100 个词**。最简单的方式是使用 `HashMap` 来实现，其中 key 为词，value 为该词出现的频率。

  具体方法是：对于遍历到的词 x，执行 `map.put(x, map.getOrDefault(x, 0) + 1)`。

上面我们统计了每个小文件单词出现的频数。

* 接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。

  具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

> **拓展：类快排方式解决 topK 问题**
>
> 快排大家都知道，这里就不重复叙述。
>
> 针对 topK 问题，可以对快排进行改进：仅对部分数据进行递归计算。
>
> 比如，在 100 个数字中，找最大的 10 个，第一次循环的时候，pivot 被移动到了 80 的位置，则接下来仅需要在后面的 20 个数字中找最大的 10 个即可。
>
> 这样做的优势是，**理论最优时间复杂度可以达到  $O(n)$**，不过平均时间复杂度还是 `O(nlogn)`。需要说明的是，**通过这种方式，找出来的最大的 k 个数字之间，是无序的；而使用堆排序是有序的**。

> **拓展：使用 Bitmap 解决 topK 问题**
>
> 有时候 topK 问题会遇到数据量过大，内存无法全部加载。这个时候，可以考虑将数据存放至 bitmap 中，方便查询。
>
> 比如，给出 10 个 int 类型的数据，分别是 [13, 12, 11, 1, 2, 3, 4, 5, 6, 7]，int 类型的数据每个占据 4 个字节，那这个数组就占据了 40 个字节。
>
> 现在，把它们放到一个长度为 16 的 bitmap 中，结果就是[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]，在将空间占用降低至 4 字节的同时，也可以很方便的看出，最大的 3 个数字，分别是 11，12 和 13。
>
> 这种做法的优势，当然是降低了空间复杂度。不过需要注意一点，bitmap 比较适合**不重复且有范围**（比如，数据均在 0 ～ 10 亿之间）的数据的查询，如果不明确数据的范围，那么就不好分配 bitmap 空间。至于有重复数据的情况，可以考虑与 hash 等结构的混用。

### 如何找出某一天访问网站最多的 IP？

#### 题目描述

现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

#### 解答思路

本题其实于上一题类似，都是大数据量的 topK 问题，因此采用的方法也与上一题类似。

大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

### 如何从海量数据中找出不重复的数据？

#### 题目描述

在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

#### 解答思路

本题依然可以使用分治法，先划分为多个小文件，对每个小文件都用 HashSet 统计不重复的整数，再合并子结果。

本题比较推荐的是位图法，也就是使用 bitmap，对于这道题，我们用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 $2^{32}$ 个整数，总共所需内存为 $2^{32}*2b=1GB$。因此，当可用内存超过 1GB 时，可以采用位图法。

假设内存满足位图法需求，进行下面的操作：

* 遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

当然，本题中特别说明：**内存不足以容纳这 2.5 亿个整数**，2.5 亿个整数的内存大小为：2.5e8/1024/1024/1024 * 4=3.72GB， 如果内存大于 1GB，是可以通过位图法解决的。

### 如何在大量的数据中判断一个数是否存在？

#### 题目描述

给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

#### 解答思路

分治法 & 位图法。

### 如何从 5 亿个数据中找出中位数？

#### 题目描述

从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 `(N+1)/2` 个数；当样本数为偶数时，中位数为 第 `N/2` 个数与第 `1+N/2` 个数的均值。

#### 解答思路

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数，时间复杂度为 `O(NlogN)` 。

但是通常来说会有内存限制，而且时间复杂度略高，我们可以采取其他办法。

##### 方法一：双堆

维护两个堆，一个大顶堆，一个小顶堆。**大顶堆中最大的数小于等于小顶堆中最小的数，同时保证这两个堆中的元素个数的差不超过 1**。

若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。

这题其实是力扣 295，剑指 Offer 也有一模一样的题，我们在这里就不多叙述了。

##### 方法二：分治

几乎所有的大数据量题目都可以使用分治来解决，对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。

划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。

对于上一步得到的数据，可以用次高位的二进制继续将文件一分为二，如此划分下去，**直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数**。
