[TOC]

# Kafka 简介

## 1. 概述

Kafka 起初是由 Linkedin 公司采用 Scala 语言开发的一个多分区、多副本且基于 ZooKeeper 协调的分布式消息系统，现己被捐献给 Apache 基金会。 

目前 Kafka 已经定位为一个分布式流式处理平台，它以**高吞吐、可持久化、可水平扩展、支持流数据处理**等多种特性而被广泛使用。目前越来越多的开源分布式处理系统如 Cloudera、 Storm、 Spark、 Flink 等都支持与 Kafka 集成 。

我们通常是**把 Kafka 作为一个消息系统**，相比与其他消息系统，Kafka 具有以下**优点**：

- **高性能**：单机每秒处理几十上百万的消息量。即使存储了 TB 级别的消息，也保持稳定的性能。

  Kafka 的高性能主要依靠以下几点保证：

  - **零拷贝**：减少内核态到用户态的拷贝，磁盘通过 `sendfile` 实现 DMA 拷贝 Socket Buffer。
  - **顺序读写**：充分利用**磁盘顺序读写**的超高性能。
  - **页缓存 `mmap` **：将**磁盘文件映射到内存**, 用户通过修改内存就能修改磁盘文件。

- **高可用**：单节点支持上千个客户端，并保证零停机和零数据丢失。

  Kafka 是分布式，分区，复制和容错的。

- **持久化**：将消息持久化到磁盘。通过将数据持久化到硬盘以及 replication 防止数据丢失。

- **分布式**：所有的组件均为分布式的，无需停机即可扩展机器。

正是因其具有这些的优秀特性而广泛用于应用解耦、流量削峰、异步消息等场景，比如消息中间件、日志聚合、流处理等等。

## 2. Kafka 基本概念

一个典型的 kafka 架构包含若干 Producer、若干 Broker、若干 Consumer。如下图所示：

![image-20220726074947390](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220726074947390.png)

* **生产者（Producer）**：发送消息的一方。生产者负责创建消息，然后发送到 Kafka 中。

  在默认情况下，生产者**会把消息均衡地分布到主题的所有分区上**。消费者可以：

  - 直接指定消息的分区
  - 根据消息的 key 散列取模得出分区
  - 轮询指定分区

* **消费者（Consumer）**：接受消息的一方。消费者连接到 Kafka 上接受消息，进行相应的业务逻辑处理。

  消费者通过**偏移量**来区分已经读过的消息，从而消费消息。Kafka 会把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 自己上，如果消费者关闭或重启，它的**读取状态不会丢失**。

* **节点（Broker）**：对于 Kafka 而言，Broker 可以简单的看作是一个独立的 Kafka 服务节点或实例。大多数情况下也可把一个 Broker 看作是一台 Kafka 服务器，前提是这台服务器上只部署了一个 Kafka 实例。

  Broker 负责连接生产者和消费者，**单个 Broker 可以轻松处理数千个分区**以及**每秒百万级的消息量**。

  - Broker 接收来自生产者的消息，为消息设置偏移量，并提交**消息到磁盘保存**。
  - Broker 为消费者提供服务，响应读取分区的请求，**返回已经提交到磁盘上的消息**。

* **集群**：一个或多个 Broker 组成了 一个 Kafka 集群。

* **消费者组（Consumer Group）**：一个或多个消费者可以构成一个消费者组。消费者组保证**每个分区只能被一个消费者使用**，避免重复消费。

  如果组内**一个消费者失效**，组里的**其他消费者可以接管失效消费者的工作再平衡**，重新分区。

以上就是关于参与 Kafka 系统的机器的基本分类，下面是有关消息的基本概念：

* **消息（Message）**：Kafka 的数据单元称为消息。可以把消息看成是数据库里的一个“数据行”或一条“记录”。

* **主题（Topic）**：Kafka 通过主题将消息进行分类，类似数据库中的表。

* **分区（Partition）**：Topic 可以被分成若干 Partition 分布于 Kafka 集群中，方便扩容。

  分区在存储层面上可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志、文件的时候都会分配一个特定的偏移量（offset）。

  **offset 是消息在分区中的唯一标识，Kafka 通过它来保证消息在分区内的顺序性**。

  不过 **offset 并不跨越分区，也就是说，Kafka 保证的是分区有序而不是主题有序。**

  示意图：

  <img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220726180300327.png" alt="image-20220726180300327" style="zoom: 50%;" />

  每一条消息被发送到 Broker 之前，会根据分区规则选择存储到哪个具体的分区。**如果分区规则设定得合理，所有的消息都可以均匀地分配到不同的分区中**。

* **副本（Replica）**：Kafka 引入了多副本机制，通过增加副本数量可以提升容灾能力。

  副本特指每个分区的多个副本，同一分区的不同副本中保存的是相同的消息。

  副本之间有着**一主多从关系**，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本的消息同步。

  副本可以位于多个不同的 Broker 中，**当 leader 副本出现故障时，从 follower 副本中重新选举新的 leader 副本对外提供服务**。

  Kafka 通过该机制实现了**故障的自动转移**，当 Kafka 集群中某个 Broker 失效时仍然能保证服务可用。

  示意图如下：

  <img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220726180756241.png" alt="image-20220726180756241" style="zoom: 50%;" />

  > Kafka 消费者端也具备一定的容灾能力。Consumer 使用拉（Pull）模式从服务端拉取消息，并且**保存消费的具体位置**，当消费者看机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失 。 

* AR（Assigned Replicas）、ISR（In-Sync Replicas）、OSR（Out-of-Sync Replicas）：

  * 分区中的所有副本统称为 AR。
  * 所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内）组成 ISR。ISR 是 AR 集合的一个子集。
  * 前面所说的 “一定程度的同步” 是指可忍受的滞后范围，这个范围可以通过参数进行配置。与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR。

  根据前面的叙述，我们可以得到一个基本的结论：AR = OSR + ISR。

  但是在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，因此 AR = ISR。

  默认情况下，**当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader**，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。

* HW（High Watermark）：高水位线。它标识了一个特定的消息偏移量，**消费者只能拉取到这个 offset 之前的消息**。

  下图是一个日志文件，这个日志文件中有 9 条消息，第一条消息的 offset（也被称为 LogStartOffset）为 0，最后一条消息的 offset 为 8, offset 为 9 的消息用虚线框表示，代表下一条待写入的消息：

  <img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220726182041801.png" alt="image-20220726182041801" style="zoom:50%;" />

  这里 HW = 6，表示消费者只能拉取到 0 ~ 5 之间的消息，6 及其之后的消息对消费者来说是不可见的。

* LEO（Log End Offset）：它标识当前日志文件中下一条待写入消息的 offset。

  上图中的 9 就是这个日志文件的 LEO，也就是最后一条消息的 offset + 1。

  **分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW**。

  > 最小的 LEO 也就意味着这之后的消息都还没有与 leader 同步，因此作为 HW 是合理的。

## 3. Kafka 安装与启动

在 Linux 系统下，只需要下载官网的压缩包，`tar -zxvf`，然后配置一下环境变量即可。

Kafka 依赖 Zookeeper，但是不必担心，Kafka 的压缩包内置了 Zookeeper。

首先，启动 Kafka 之前，要先启动 zookeeper，使用后台进程启动即可，启动脚本位于 bin 下的 `zookeeper-server-start.sh`，相关配置文件位于 config 下的 `zookeeper.properties`，因此启动命令为：

```bash
$> ./bin/zookeeper-server-start.sh config/zookeeper.properties &
```

然后启动 Kafka 服务，脚本在 bin 下的 `kafka-server-start.sh`，配置文件在 config 下的 `server.properties` 中，因此启动命令为:

```bash
$> ./bin/kafka-server-start.sh config/server.properties &
```

### 3.1 Kafka 命令行简要使用

#### 3.1.1 创建 topic

这里创建一个 topic，名字叫 test-topic，下面的例子都用这个 topic：

```bash
$> ./bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.2 查看 topic

我们可以查看 topic：

```bash
$> ./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

也可以查看具体的某一个 topic：

```bash
$> ./bin/kafka-topics.sh --describe --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.3 发送消息

```bash
$> ./bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.4 消费消息

```bash
$> ./bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092
```

#### 3.1.5 删除 topic

```bash
$> ./bin/kafka-topics.sh --delete --topic test-topic --bootstrap-server localhost:9092
```

果 kafka 启动时加载的配置文件中 server.properties 没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把 topic 标记为 `marked for deletion`，即逻辑删除。

要想彻底删除，则可以登录 zookeeper 客户端

```bash
$> ./zkCli.sh
```

找到 topic 所在的目录：

```bash
$> ls /brokers/topics
```

找到要删除的 topic，执行：

```bash
$> rmr /brokers/topics/test-topic
```

## 4. Java 访问 Kafka

首先引入 maven 依赖：

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>${kafka.version}</version>
</dependency>
```

下面是一个生产者 Demo:

```java
import java.util.HashMap;
import java.util.Map;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class ProducerDemo {

    public static void main(String[] args) {
        Map<String, Object> props = new HashMap<>();
        props.put("bootstrap.servers", "106.55.250.68:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        String topic = "test-topic";
        Producer<String, String> producer = new KafkaProducer<>(props);
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 1"));
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 2"));
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 3"));

        producer.close();

    }

}
```

下面是一个消费者 Demo：

```java
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.List;
import java.util.Properties;

public class ConsumerDemo {

    public static void main(String[] args) {
        String topic = "test-topic";

        Properties props = new Properties();
        props.put("bootstrap.servers", "106.55.250.68:9092");
        props.put("group.id", "testGroup1");
        props.put("enable.auto.commit", "true");
        props.put("auto.commit.interval.ms", "1000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        try (Consumer<String, String> consumer = new KafkaConsumer<>(props)) {
            consumer.subscribe(List.of(topic));
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("partition = %d, offset = %d, key = %s, value = %s%n", record.partition(),
                            record.offset(), record.key(), record.value());
                }
            }
        }

    }
}
```

## 5. SpringBoot 整合 Kafka

在 SpringBoot 项目中，引入该依赖：

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

然后，在 application.yml 中可以添加以 `spring.kafka` 开头的配置项，全部配置项参见 `KafkaProperties` 类，下面是一个简单的示例：

```yaml
spring:
    kafka:
        bootstrap-servers: 106.55.250.68:9092
        consumer:
            group-id: consumer-group-test
            enable-auto-commit: true
            auto-commit-interval: 3000
```

接下来，如果要发送消息，则可以使用 `KafkaTemplate`，调用 `send(String topic，V data)` 即可发送一条消息到某个 topic 下，当然，也可以使用 `send(String topic, Integer partition, K key, V data)` 指定 partition。

如果要消费，就更简单了，只需要使用一个注解 `@KafkaListener`，作用于方法上，注解内写上你关注的 topic，它就会在接收到消息时自动调用该方法，把消息绑定在参数 `ConsumerRecord<String, Object> message` 上，比如：

```java
@KafkaListener(topics = {"test"})
public void handleMessage(ConsumerRecord<String, Object> record) {
    // ...
}
```

更多用法参见 Spring 官方文档。
