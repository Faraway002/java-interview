# Kafka 简介

## 1. 概述

Kafka 是一个分布式的基于发布\订阅的消息系统，有着强大的消息处理能力，相比与其他消息系统，具有以下特性：

- **快速数据持久化**，实现了 $O(1)$ 时间复杂度的数据持久化能力。
- **高吞吐**，能在普通的服务器上达到 10W 每秒的吞吐速率。
- **高可靠**，消息持久化以及副本系统的机制保证了消息的可靠性，消息可以多次消费。
- **高扩展**，与其他分布式系统一样，所有组件均支持分布式、自动实现负载均衡，可以快速便捷的扩容系统。
- **离线与实时处理能力并存**，提供了在线与离线的消息处理能力。

正是因其具有这些的优秀特性而广泛用于应用解耦、流量削峰、异步消息等场景，比如消息中间件、日志聚合、流处理等等。

## 2. Kafka 内的重要概念介绍

![image-20220628184119671](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220628184119671.png)

### 2.1 Broker

一个独立的 Kafka 实例，多个 Kafka Broker 组成一个 Kafka Cluster（集群）。

### 2.2 Topic

每条发布到 Kafka 的消息都有一个类别，这个类别被称为 Topic 。

* 物理上不同 Topic 的消息分开存储。
* 逻辑上一个 Topic 的消息虽然保存于一个或多个 Broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处。

Topic 是由使用方创建的，一般会有多个订阅者去消费对应主题的消息，也可以存在多个生产者往主题中写入消息。

### 2.3 Partition

Topic 物理上的分组，一个 Topic 可以分为多个 Partition，每个 Partition 是一个有序的队列。

Partition 中的每条消息都会被分配一个有序的 id（偏移量 offset）。

### 2.4 Producer 和 Consumer

Producer 是消息和数据的生产者，可以理解为往 Kafka **发消息的客户端**。

Consumer 是消息和数据的消费者，可以理解为从 Kafka **取消息的客户端**。

### 2.5 Consumer Group

每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 Group Name，若不指定 Group Name 则属于默认的 Group）。 

这是 Kafka 用来实现一个 Topic 消息的广播（发给所有的 Consumer ）和单播（发给任意一个 Consumer ）的手段。一个 Topic 可以有多个 Consumer Group，Topic 的消息会复制（不是真的复制，是概念上的）到所有的 Consumer Group，但每个 Consumer Group 只会把消息发给该 Consumer Group 中的一个 Consumer。

如果要实现广播，只要每个 Consumer 有一个独立的 Consumer Group 就可以了；如果要实现单播只要所有的 Consumer 在同一个 Consumer Group。

用 Consumer Group 还可以将 Consumer 进行自由的分组而不需要多次发送消息到不同的 Topic 。

## 3. Kafka 安装与启动

在 Linux 系统下，只需要下载官网的压缩包，`tar -zxvf`，然后配置一下环境变量即可。

Kafka 依赖 Zookeeper，但是不必担心，Kafka 的压缩包内置了 Zookeeper。

首先，启动 Kafka 之前，要先启动 zookeeper，使用后台进程启动即可，启动脚本位于 bin 下的 `zookeeper-server-start.sh`，相关配置文件位于 config 下的 `zookeeper.properties`，因此启动命令为：

```bash
$> ./bin/zookeeper-server-start.sh config/zookeeper.properties &
```

然后启动 Kafka 服务，脚本在 bin 下的 `kafka-server-start.sh`，配置文件在 config 下的 `server.properties` 中，因此启动命令为:

```bash
$> ./bin/kafka-server-start.sh config/server.properties &
```

### 3.1 Kafka 命令行简要使用

#### 3.1.1 创建 topic

这里创建一个 topic，名字叫 test-topic，下面的例子都用这个 topic：

```bash
$> ./bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.2 查看 topic

我们可以查看 topic：

```bash
$> ./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

也可以查看具体的某一个 topic：

```bash
$> ./bin/kafka-topics.sh --describe --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.3 发送消息

```bash
$> ./bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092
```

#### 3.1.4 消费消息

```bash
$> ./bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092
```

#### 3.1.5 删除 topic

```bash
$> ./bin/kafka-topics.sh --delete --topic test-topic --bootstrap-server localhost:9092
```

果 kafka 启动时加载的配置文件中 server.properties 没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把 topic 标记为 `marked for deletion`，即逻辑删除。

要想彻底删除，则可以登录 zookeeper 客户端

```bash
$> ./zkCli.sh
```

找到 topic 所在的目录：

```bash
$> ls /brokers/topics
```

找到要删除的 topic，执行：

```bash
$> rmr /brokers/topics/test-topic
```

## 4. Java 访问 Kafka

首先引入 maven 依赖：

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>${kafka.version}</version>
</dependency>
```

下面是一个生产者 Demo:

```java
import java.util.HashMap;
import java.util.Map;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class ProducerDemo {

    public static void main(String[] args) {
        Map<String, Object> props = new HashMap<>();
        props.put("bootstrap.servers", "106.55.250.68:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        String topic = "test-topic";
        Producer<String, String> producer = new KafkaProducer<>(props);
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 1"));
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 2"));
        producer.send(new ProducerRecord<>(topic, "idea-key2", "java-message 3"));

        producer.close();

    }

}
```

下面是一个消费者 Demo：

```java
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.List;
import java.util.Properties;

public class ConsumerDemo {

    public static void main(String[] args) {
        String topic = "test-topic";

        Properties props = new Properties();
        props.put("bootstrap.servers", "106.55.250.68:9092");
        props.put("group.id", "testGroup1");
        props.put("enable.auto.commit", "true");
        props.put("auto.commit.interval.ms", "1000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        try (Consumer<String, String> consumer = new KafkaConsumer<>(props)) {
            consumer.subscribe(List.of(topic));
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("partition = %d, offset = %d, key = %s, value = %s%n", record.partition(),
                            record.offset(), record.key(), record.value());
                }
            }
        }

    }
}
```

## 5. SpringBoot 整合 Kafka

在 SpringBoot 项目中，引入该依赖：

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

然后，在 application.yml 中可以添加以 `spring.kafka` 开头的配置项，全部配置项参见 `KafkaProperties` 类，下面是一个简单的示例：

```yaml
spring:
    kafka:
        bootstrap-servers: 106.55.250.68:9092
        consumer:
            group-id: consumer-group-test
            enable-auto-commit: true
            auto-commit-interval: 3000
```

接下来，如果要发送消息，则可以使用 `KafkaTemplate`，调用 `send(String topic，V data)` 即可发送一条消息到某个 topic 下，当然，也可以使用 `send(String topic, Integer partition, K key, V data)` 指定 partition。

如果要消费，就更简单了，只需要使用一个注解 `@KafkaListener`，作用于方法上，注解内写上你关注的 topic，它就会在接收到消息时自动调用该方法，把消息绑定在参数 `ConsumerRecord<String, Object> message` 上，比如：

```java
@KafkaListener(topics = {"test"})
public void handleMessage(ConsumerRecord<String, Object> record) {
    // ...
}
```

更多用法参见 Spring 官方文档。
