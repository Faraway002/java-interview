[TOC]

# 消费者

与生产者对应的是消费者，应用程序可以通过 `KafkaConsumer` 来订阅主题，并从订阅的主题中拉取消息。

## 1. 消费者与消费者组

**消费者（Consumer）负责订阅 Kafka 中的主题（Topic），并且从订阅的主题上拉取消息**。

与其他一些消息中间件不同的是，在 Kafka 的消费理念中还有一层消费者组（ConsunmerGroup）的概念：**每个消费者都有一个对应的消费者组，当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者**。

如下图所示：

![image-20220727084457177](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727084457177.png)

如图所示，某个主题中共有 4 个分区（Partition）：P0、P1、P2、P3。有两个消费组 A 和 B 都订阅了这个主题，消费组 A 中有 4 个消费者（C0、C1、C2 和 C3），消费组 B 中有2个消费者（C4 和 C5）。

按照 Kafka 默认的规则，最后的分配结果是消费者组 A 中的每一个消费者分配到 1 个分区，消费者组 B 中的每一个消费者分配到 2 个分区，两个消费者组之间互不影响。

**每个消费者只能消费所分配到的分区中的消息，换言之，每一个分区只能被一个消费者组中的一个消费者所消费**。

我们再来看一下消费组内的消费者个数变化时所对应的分区分配的演变。假设目前某消费组内只有一个消费者 C0，订阅了一个主题，这个主题包含 7 个分区：P0、P1、P2、P3、P4、P5、P6。也就是说，这个消费者 C0 订阅了 7 个分区，具体分配情形如下图：

![image-20220727090411848](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727090411848.png)

此时消费组内又加入了一个新的消费者 C1，按照既定的逻辑，需要将原来消费者 C0 的部分分区分配给消费者 C1 消费，如图所示：

![image-20220727090431072](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727090431072.png)

消费者 C0 和 C1 各自负责消费所分配到的分区，彼此之间并无逻辑上的干扰。

紧接着消费组内又加入了一个新的消费者 C2，消费者 C0、C1 和 C2 按照下图中的方式各自负责消费所分配到的分区：

![image-20220727090530626](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727090530626.png)

消费者与消费组这种模型可以让整体的消费能力具备**横向伸缩性**，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。

**对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升**，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有**消费者分配不到任何分区**。如下图所示：

![image-20220727090621593](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727090621593.png)

以上分配逻辑都是基于**默认的分区分配策略**进行分析的，可以通过消费者客户端参数 `partition.assignment.strategy` 来设置消费者与订阅主题之间的分区分配策略，有关分区分配的更多细节我们之后再详细讲述。

### 1.1 消费者组对消息队列两种模式的支持

对于消息中间件而言，一般有两种消息投递模式：点对点（P2P）以及发布-订阅（Pub-Sub）模式。

* 点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。

* 发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题，主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。

  主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用。

Kafka 同时支持两种模式，这得益于消费者和消费者组模型：

* 如果**所有的消费者都隶属于同一个消费组**，那么所有的消息都会被均衡地投递给每一个消费者，即**每条消息只会被一个消费者处理**，这就相当于点对点模式的应用。
* 如果**所有的消费者都隶属于不同的消费组**，那么所有的消息都会被广播给所有的消费者，即**每条消息会被所有的消费者处理**，这就相当于发布/订阅模式的应用。

## 2. 客户端开发

在了解了消费者与消费组之间的概念之后，我们就可以着手进行消费者客户端的开发了。

在 Kafka 的历史中，消费者客户端同生产者客户端一样也经历了两个大版本：第一个是于 Kafka 开源之初使用 Scala 语言编写的客户端，我们可以称之为旧消费者客户端（Old Consumer）或 Scala 消费者客户端；第二个是从 Kafka 0.9.x 版本开始推出的使用 Java 编写的客户端，我们可以称之为新消费者客户端（New Consumer）或 Java 消费者客户端 ，它弥补了旧客户端中存在的诸多设计缺陷。

本节主要介绍目前流行的新消费者（Java 语言编写的）客户端 ，而旧消费者客户端己被淘汰，故不再做相应的介绍了。

一个正常的消费逻辑需要具备以下几个步骤：

1. 配置消费者客户端参数及创建相应的消费者实例。
2. 订阅主题。
3. 拉取消息并消费。
4. 提交消费位移。
5. 关闭消费者实例。

我们继续拿第一章的例子来讲解消费者：

```java
import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class ConsumerDemo {

    public static final String BROKER_LIST = "106.55.250.68:9092";

    public static final String TOPIC = "test-topic";

    public static final String GROUP_ID = "testGroup";

    public static final AtomicBoolean isRunning = new AtomicBoolean(true);

    public static Properties initConfiguration() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_LIST);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "consumer.client.id.demo");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        return props;
    }

    public static void main(String[] args) {
        Properties props = initConfiguration();

        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);) {
            consumer.subscribe(Arrays.asList(TOPIC));

            while (isRunning.get()) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("topic = " + record.topic() + ", partition = " + record.partition()
                            + ", offset = " + record.offset());
                    System.out.println("key = " + record.key() + ", value = " + record.value());
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

### 2.1 必要的参数配置与构建消费者实例

在创建真正的消费者实例之前需要做相应的参数配置，比如示例代码中的设置消费者所属的消费组的名称、连接地址等。

在 Kafka 消费者客户端 `KafkaConsumer` 中有 4 个参数是必填的：

* `bootstrap.servers`：该参数的释义和生产者客户端 `KafkaProducer` 中的相同，用来指定连接 Kafka 集群所需的 broker 地址清单，具体内容形式为 `hostl:portl,host2:post`，可以设置一个或多个地址，中间用英文逗号隔开。

  注意，这里并非需要设置集群中全部的 broker 地址，消费者会从现有的配置中查找到全部的 Kafka 集群成员。不过**建议至少要设置两个以上的 broker 地址信息**，以保证当其中任意一个宕机时，消费者仍然可以连接到 Kafka 集群上。

* `group.id`：消费者隶属的消费组的名称，默认是空的。如果设置为空，则会报出异常。

  一般而言，这个参数需要设置成具有一定的业务意义的名称。

* `key.deserializer` 和 `value.deserializer`：与生产者客户端 `KafkaProducer` 中的 `key.serializer` 和 `value.serializer` 参数对应。消费者从 broker 端获取的消息格式都是字节数组类型，所以**需要执行相应的反序列化操作才能还原成原有的对象格式**。这两个参数分别用来指定消息中 key 和 value 所需反序列化操作的反序列化器。

  注意这里必须填写反序列化器类的全限定名。

这里还配置了一个 `client.id` 指定 `KafkaConsumer` 对应的客户端 id，可以不设置，Kafka 会自动生成，形式为 "consumer-1"、"consumer-2" 这样的字符串。

因为这些配置太多了，而且容易写错，Kafka 在 `ConsumerConfig` 类里引入了一些常量，这些常量对应属性名称，详情请自行查看源代码。

### 2.2 订阅主题与分区

在创建好消费者之后，我们就需要为该消费者订阅相关的主题了，**一个消费者可以订阅一个或多个主题**。

在示例代码中，我们使用 `KafkaConsumer` 的 `subscribe` 方法订阅了一个主题，**对于这个方法而言，既可以以集合的形式订阅多个主题，也可以以正则表达式的形式订阅特定模式的主题**。它的重载如下：

```java
public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener);
public void subscribe(Collection<String> topics);
public void subscribe(Pattern pattern, ConsumerRebalanceListener listener);
public void subscribe(Pattern pattern);
```

* 对于消费者**使用集合的方式**来订阅主题而言，比较容易理解，订阅了什么主题就消费什么主题中的消息。**如果前后两次订阅了不同的主题，那么消费者以最后一次的为准**。
* 如果消费者采用的是**正则表达式的方式**订阅，在之后的过程中，**如果有人创建了新的主题，并且主题的名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息**。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。

细心的读者可能观察到在 `subscribe` 的重载方法中有一个参数类型是 `ConsumerRebalanceListener`，这个是用来设置相应的再均衡监昕器的，我们很快就会讲到。

除了使用 `subscribe` 订阅主题之外，还可以使用 `assign` 方法订阅特定的分区：

```java
public void assign(Collection<TopicPartition> partitions);
```

`TopicPartition` 用来表示分区，部分源码展示如下：

```java
public final class TopicPartition implements Serializable {
    // ...
  
    private final int partition;
    private final String topic;

    public TopicPartition(String topic, int partition) {
        this.partition = partition;
        this.topic = topic;
    }

    // ...
}
```

`TopicPartition` 类只有 2 个属性：topic 和 partition，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题/分区的概念映射起来。

如果我们事先并不知道主题中有多少个分区怎么办? `KafkaConsumer` 中的 `partitionsFor()` 方法可以用来查询指定主题的元数据信息：

```java
public List<PartitionInfo> partitionsFor(String topic);
```

其中 `PartitionInfo` 类型即为主题的分区元数据信息，此类的主要结构如下：

```java
public class Part工tioninfo {
  	private final String topic;
  	private final int partition;
  	private final Node leader;
  	private final Node[] replicas;
  	private final Node[] inSyncReplicas; 
   	private final Node[] offlineReplicas;
  
  	// ...
}
```

`PartitionInfo` 类中的属性 topic 表示主题名称，partition 代表分区编号，leader 代表分区的 leader 副本所在的位置，replicas 代表分区的 AR 集合，inSyncReplicas 代表分区的 ISR 集合，offlineReplicas 代表分区的 OSR 集合 。

通过这个 `partitionsFor` + `assign` 方法，我们可以实现订阅主题（全部分区）的功能，示例：

```java
List<TopicPartition> partitions = new ArrayList<>();
List<PartitionInfo> partitionInfos = new consumer.partitionsFor(topic);
if (partitionInfos != null) {
  	for (PartitionInfo pInfo : partitionInfos) {
      	partitions.add(new TopicPartition(pInfo.topic(), pInfo.partition()));
    }
}
consumer.assign(partitions);
```

集合订阅的三种方式分别代表了三种不同订阅状态：

* AUTO_TOPICS
* AUTO_PATTERN
* USER_ASSIGN

如果没有任何订阅，那么订阅状态为 NONE。同时，这三种状态是互斥的，一个消费者只能使用其中的一种。

> 通过 `subscribe()` 方法订阅主题具有**消费者自动再均衡**的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。 
>
> 而通过 `assign()` 方法订阅分区时，是不具备消费者自动均衡的功能的，其实这一点从 `assign()` 方法的参数中就可以看出端倪，两种类型的 `subscribe()` 都有 `ConsumerRebalanceListener` 类型参数的方法，而 `assign()` 方法却没有。 

既然有订阅，那么就有**取消订阅**，可以使用 `KafkaConsumer` 中的 `unsubscribe()` 方法来取消主题的订阅。

这个方法既可以取消通过集合方式实现的订阅，也可以取消通过正则表达式方式实现的订阅，还可以取消通过 `assign` 方式实现的订阅。

### 2.3 反序列化

Kafka 所提供的反序列化器有 `ByteBufferDeserializer`、`ByteArrayDeserializer`、`BytesDeserializer`、`DoubleDeserializer`、`FloatDeserializer`、`IntegerDeserializr`、`LongDeserializer`、`ShortDeserializer`、`StringDeserializer`，它们分别用于 `ByteBuffer`、`ByteArray`、`Bytes`、`Double`、`Float`、`Integer`、`Long`、`Short` 及 `String` 类型的反序列化。

这些序列化器也都实现了 `Deserializer` 接口：

```java
public interface Deserializer<T> extends Closeable {

    default void configure(Map<String, ?> configs, boolean isKey) {
        // intentionally left blank
    }

    T deserialize(String topic, byte[] data);
  
  	// ...
}
```

在讲生产者的时候，我们曾经自定义 `Company` 序列化器，接下来我们实现一下它的反序列化器：

```java
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.serialization.Deserializer;

public class CompanyDeserializer implements Deserializer<Company> {
    @Override
    public Company deserialize(String topic, byte[] data) {
        if (data == null) {
            return null;
        }
        if (data.length < 0) {
            throw new SerializationException("Size of data received by CompanyDeserializer is shorter than excepted!");
        }

        ByteBuffer buffer = ByteBuffer.wrap(data);
        int nameLen, addressLen;
        String name = null, address = null;

        nameLen = buffer.getInt();
        byte[] nameBytes = new byte[nameLen];
        buffer.get(nameBytes);
        addressLen = buffer.getInt();
        byte[] addressBytes = new byte[addressLen];

        try {
            name = new String(nameBytes, StandardCharsets.UTF_8);
            address = new String(addressBytes, StandardCharsets.UTF_8);
        } catch (Exception e) {
            e.printStackTrace();
        }
        return new Company(name, address);
    }
}
```

如无特殊需要，笔者还是不建议使用自定义的序列化器或反序列化器，因为这样会增加生产者与消费者之间的耦合度，在系统升级换代的时候很容易出错。 

自定义的类型有一个不得不面对的问题就是 `KafkaProducer` 和 `KafkaConsumer` 之间的序列化和反序列化的兼容性。对于 `StringSerializer` 来说，`KafkaConsumer` 可以顺其自然地采用 `StringDeserializer`，不过对于 Company 这种专用类型而言，某个上游应用采用 CompanySerializer 进行序列化之后，下游应用也必须实现对应的 CompanyDeserializer。再者，如果上游的 Company 类型改变，那么下游也需要跟着重新实现一个新的 CompanyDeserializer，后面所面临的难题可想而知。

在实际应用中，在 Kafka 提供的序列化器和反序列化器满足不了应用需求的前提下，推荐使用 Avro、 JSON、 ProtoBuf 等通用的序列化工具来包装。

本节最后我们演示一下使用 Protostuff 的序列化：

```xml
<dependency>
  	<groupId>io.protostuff</groupId>
  	<artifactId>protostuff-core</artifactId>
  	<version>1.8.0</version>
</dependency>
<dependency>
  	<groupId>io.protostuff</groupId>
  	<artifactId>protostuff-runtime</artifactId>
  	<version>1.8.0</version>
</dependency>
```

```java
public class ProtostuffSerializer implements Serializer<Company> {
    @Override
    public byte[] serialize(String topic, Company data) {
        if (data == null) {
            return  null;
        }

        Schema schema = RuntimeSchema.getSchema(data.getClass());
        LinkedBuffer buffer = LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE);
        byte[] protostuff = null;

        try {
            protostuff = ProtostuffIOUtil.toByteArray(data, schema, buffer);
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            buffer.clear();
        }

        return protostuff;
    }
}
```

```java
public class ProtostuffDeserializer implements Deserializer<Company> {
    @Override
    public Company deserialize(String topic, byte[] data) {
        if (data == null) {
            return  null;
        }

        Schema schema = RuntimeSchema.getSchema(Company.class);
        Company company = new Company();
        ProtobufIOUtil.mergeFrom(data, company, schema);

        return company;
    }
}
```

### 2.4 消息消费

**Kafka 中的消费是基于拉模式的**。

消息的消费一般有两种模式：推（push）模式和拉（pull）模式，其中，推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。

从我们的示例代码中可以看出，Kafka 的消息消费是一个不断轮询的过程，消费者所需要做的就是重复的调用 `poll` 方法，而 `poll` 方法返回的是所订阅的主题（分区）上的一组消息。

对于 `poll` 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空; 如果订阅的所有分区中都没有可供消费的消息，那么方法返回为空的消息集合。

`poll` 的具体定义如下：

```java
public ConsumerRecords<K, V> poll(final Duration timeout)
```

注意到 `poll` 方法里还有一个超时时间参数 timeout，用来控制 `poll` 方法的阻塞时间，**在消费者的缓冲区里没有可用数据时会发生阻塞**。

timeout 的设置取决于应用程序对响应速度的要求，比如需要在多长时间内将控制权移交给执行轮询的应用线程。可以**直接将 timeout 设置为 0，这样 `poll` 方法会立刻返回，而不管是否己经拉取到了消息**。如果应用线程唯一的工作就是从 Kafka 中拉取并消费消息，则可以将这个参数设置一个比较大的时间。

从 `poll` 方法获取到的是消息集合 `ConsumerRecords`，而 `ConsumerRecords` 的每一条消息的类型是 `ConsumerRecord`：

```java
public class ConsumerRecord<K, V> {
		// ...

    private final String topic;
    private final int partition;
    private final long offset;
    private final long timestamp;
    private final TimestampType timestampType;
    private final int serializedKeySize;
    private final int serializedValueSize;
    private final Headers headers;
    private final K key;
    private final V value;
    private final Optional<Integer> leaderEpoch;
  
  	// ...
}
```

其中：

* **topic 代表消息所属的主题**。
* **partition 代表消息所在的主题的分区编号**。
* **offset 代表消息所属分区的偏移量**。
* **timestamp 表示时间戳**。
* **timestampType 表示时间戳的类型**。它有 CreateTime 和 LogAppendTime 两种类型，前者表示消息创建的时间，后者表示消息追加到日志文件的时间，之后我们会详细介绍。
* **headers 字段是消息的头部**。 
* **key 是用来指定消息的键**。
* **value 是指消息的值**。
* **serializedKeySize 是 key 经过序列化后的大小**，如果 key 为空，该值为 -1。
* **serializedValueSize 是 value 经过序列化后的大小**，如果 value 为空，该值为 -1。

我们在消费消息的时候可以直接对 `ConsumerRecord` 中感兴趣的字段进行具体的业务逻辑处理。

我们的示例代码中，使用的是迭代器的方式（for-each 循环）遍历消息，除此之外，我们还可以**按照分区维度**来进行消费，这一点很有用，在手动提交 offset 的时候尤为明显，关于 offset 提交的内容我们很快就会介绍。

`ConsumerRecords` 提供了 `records(TopicPartition)` 的方法用来获取指定分区中的消息：

```java
public List<ConsumerRecord<K, V>> records(TopicPartition partition)
```

使用示例：

```java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
for (TopicPartition tp : records.partitions()) {
  	for (ConsumerRecord<String, String> record : records.records(tp)) {
      	System.out.println(record.partition() + ":" + record.value());
    }
}
```

除此之外，还可以**按照主题维度**来进行消费，这个方法是 `records(TopicPartition)` 的重载：

```JAVA
public Iterable<ConsumerRecord<K, V>> records(String topic)
```

但是，`ConsumerRecords` 并未提供类似 `partitions()` 这样的方法，我们只能根据最开始消费者订阅主题时所使用的主题列表来进行消费了：

```java
List<String> topicList = Arrays.asList(topic1, topic2);
try {
  	while (isRunning.get()) {
      	ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
      	for (String topic : topicList) {
          	for (ConsumerRecord<String, String> record : records.records(tp)) {
      					System.out.println(record.topic() + ":" + record.value());
    				}
        }
    }
} finally {
  	consumer.close();
}
```

在 `ConsumerRecords` 类中还提供了几个方法来方便开发人员对消息集进行处理：`count()` 方法用来计算出消息集中的消息个数；`isEmpty()` 方法用来判断消息集是否为空；`empty()` 方法用来获取一个空的消息集。

到目前为止，可以简单地认为 `poll()` 方法只是拉取一下消息而己，但其内部逻辑并不简单，它涉及消费位移、消费者协调器、组协调器、消费者的选举、分区分配的分发、再均衡的逻辑、心跳等内容，在后面的章节中会循序渐进地介绍这些内容。

## 3. 原理分析

### 3.1 位移提交

对于 Kafka 中的**分区**而言，它的每条消息都有唯一的 **offset，用来表示消息在分区中对应的位置**。

对于**消费者**而言，它也有一个 offset 的概念，消费者**使用 offset 来表示消费到分区中某个消息所在的位置**。

在每次**调用 `poll()` 方法时**，它返回的是**还没有被消费过的消息集**（当然这个前提是消息己经存储在 Kafka 中了，并且暂不考虑异常情况的发生）。

要做到这一点，就需要**记录上一次消费时的消费位移，并且这个消费位移必须做持久化保存**，而不是单单保存在内存中，否则消费者重启之后就无法知晓之前的消费位移。再考虑一种情况，当有新的消费者加入时，那么必然会有再均衡的动作，对于同一分区而言，它可能在再均衡动作之后分配给新的消费者，如果不持久化保存消费位移，那么这个新的消费者也无法知晓之前的消费位移 。

在旧消费者客户端中，消费位移是存储在 ZooKeeper 中的；而在新消费者客户端中，消费位移存储在 Kafka 内部的主题 `__consumer_offsets` 中。

我们把将**消费位移存储起来（持久化）的动作称为提交，消费者在消费完消息之后需要执行消费位移的提交**。

举个例子：

![image-20220727174430485](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220727174430485.png)

x 表示某一次拉取操作中此分区消息的最大偏移量，假设当前消费者已经消费了 x 位置的消息，那么我们就可以说消费者的消费位移为 x，图中也用了 `lastConsumedOffset` 这个单词来标识它。

不过需要注意的是，**当前消费者要提交的位移并不是 x，而是 x + 1，即下一条要拉取的消息的位置**。

在消费者中还有一个 committed offset 的概念，它表示已经提交过的消费位移。

`KafkaConsumer` 提供了 `position(TopicPartition)` 和 `committed(TopicPartition)` 两个方法来分别获取上面所说的 position 和 committed offset 的值。

为了论证 lastConsumedOffset、committed offset 和 position 之间的关系，我们使用上面的这两个方法来做相关演示。我们向某个主题中分区编号为 0 的分区发送若干消息，之后再创建一 个消费者去消费其中的消息，等待消费完这些消息之后就同步提交消费位移(调用 commitSync() 方法，这个方法的细节在下面详细介绍)，最后我们观察一下 lastConsumedOffset、committed offset 和 position的值。示例代码如代码清单 3-2所示。