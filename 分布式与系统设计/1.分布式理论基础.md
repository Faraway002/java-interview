[TOC]

# 分布式理论基础

## 1. 分布式概述

> 一个分布式系统是一些独立的计算机集合，但是对这个系统的用户来说，系统就像一台计算机一样。

分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间**仅仅通过消息传递进行通信和协调的系统**。

简单来说就是**一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务一样**。分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务，计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。

从分布式系统的概念中我们知道，各个主机之间通信和协调主要通过网络进行，所以分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。

### 1.1 分布式系统的主要特征

无论空间上如何分布，一个标准的分布式系统应该具有以下几个主要特征：

* **分布性**：分布式系统中的多台计算机之间在空间位置上可以随意分布，同时，机器的分布情况也会随时变动。

* **对等性**：分布式系统中的计算机没有主从之分，即没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。

  副本（Replica）是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种**冗余方式**。**在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理**：

  * **数据副本**是指在不同节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最为有效的手段；
  * 另一类副本是**服务副本**，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。

* **自治性**：分布式系统中的各个节点都包含自己的处理机和内存，各自具有独立的处理数据的功能。通常，彼此在地位上是平等的，无主次之分，既能自治地进行工作，又能利用共享的通信线路来传送信息，协调任务处理。

* **并发性**：在一个计算机网络中，程序运行过程的并发性操作是非常常见的行为。例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，**如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一**。

### 1.2 分布式系统面临的问题

分布式系统虽好，但也面临许多问题与挑战，典型的问题有：

- **缺乏全局时钟**：在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。

- **机器宕机**：机器宕机是最常见的异常之一。在大型集群中每日宕机发生的概率为千分之一左右，在实践中，一台宕机的机器恢复的时间通常认为是24 小时，一般需要人工介入重启机器。

- **网络异常**：这类异常其实有好几种：
  * **消息丢失**：两片节点之间彼此完全无法通信，即出现了**网络分区**问题，也就是我们常说的**脑裂**问题；这种情况下，实际上区域内部是可以通信的。
  * **消息乱序**：有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；
  * **数据错误**；
  * **不可靠的 TCP**：TCP 协议为应用层提供了可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于 TCP 协议则通信就是可靠的。TCP 协议只能保证同一个 TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。

- **分布式三态**：如果某个节点向另一个节点发起 RPC（Remote Procedure Call）调用，即某个节点 A 向另一个节点 B 发送一个消息，节点 B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点 A，那么这个 RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。

  而非分布式的系统中，只有两态，也就是成功或失败。

- **存储数据丢失**：对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。 *异常处理原则*：被大量工程实践所检验过的异常处理黄金原则是：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

### 1.3 衡量分布式系统的指标

* **性能**：

  有多种衡量方式：

  * 系统的吞吐能力：指系统在某一时间可以处理的数据总量，通常可以用**系统每秒处理的总的数据量来衡量**；
  * 系统的响应延迟：指系统完成某一功能需要使用的时间；
  * 系统的并发能力：指系统可以同时完成某一功能的能力，通常也用 QPS（Query Per Second）来衡量。

  上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高 QPS。

- **可用性**：系统的可用性（availability）指**系统在面对各种异常时可以正确提供服务的能力**。

  系统的可用性可以用**系统停服务的时间与正常服务的时间的比例**来衡量，也可以用**某功能的失败次数与成功次数**的比例来衡量。

  可用性是分布式的重要指标，衡量了系统的鲁棒性，是**系统容错能力**的体现。

- **可扩展性**：系统的可扩展性（scalability）指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。

- **一致性**：分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致性模型，对于用户使用来说使用起来越简单。

## 2. CAP 与 BASE 理论

本节介绍 CAP 与 BASE 理论，这两个理论是分布式系统、特别是分布式存储领域中被讨论的最多的理论。

### 2.1 CAP 理论

CAP 理论起源于 2000 年，由加州大学伯克利分校的 Eric Brewe r教授在分布式计算原理研讨会（PODC）上提出，因此 CAP 定理又被称作布鲁尔定理（Brewer’s theorem）。2 年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 发表了布鲁尔猜想的证明，CAP 理论正式成为分布式领域的定理。

CAP 也就是 **Consistency（一致性）**、**Availability（可用性）**、**Partition Tolerance（分区容错性）** 这三个单词首字母组合。

<img src="https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220624220334439.png" alt="image-20220624220334439" style="zoom:67%;" />

CAP 定理指出：**对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个**：

- **一致性（Consistency）** : 所有节点访问同一份最新的数据副本。
- **可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。
- **分区容错性（Partition tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务。

当然，CAP 并不是随意的选择其中的两个，作者在 2012 年重写了之前的论文，并指出：**当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1**。

也就是说，网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择，也就是说**分区容错性（Partition tolerance）是必须要实现的**。

因此，**分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构**。我们常见的框架，比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。

> **CA 的冲突**
>
> 举个例子：若系统出现脑裂，且系统中的某个节点在进行写操作，则为了保证 C， **必须要禁止其他节点的读写操作**，这就和 A 发生冲突了，这时是系统是不可能可用的。
>
> 反过来，如果为了保证 A，其他节点的读写操作正常的话，那就造成其他节点和当前正在写的节点数据不一致了，和 C 又发生冲突了。
>
> 所以，CA 本身就是冲突的。**既然 CA 是不可能达到的，那么 CAP 三者就更不可能同时达到了**。

请注意我们的前提，是需要保证 P 时，CA 才不能同时达到，只能选择 CP 或 AP。

那么，如果 P 不需要保证时，CA 是否可以达到呢？答案是这种情况不可能出现，因为在分布式系统内，**P 是必然的发生的**。如果不选 P，一旦发生分区错误，整个分布式系统就完全无法使用了，这是不符合实际需要的。所以，**对于分布式系统，我们只能能考虑当发生分区错误时，如何选择一致性和可用性**。

> **误解：分布式系统因为 CAP 定理放弃了 C 或者 A 中的其中一个**
>
> 上面的说法完全是错误的，因为，P 这种问题发生的概率非常低，所以**当没有出现分区问题的时候，系统就应该有完美的数据一致性和可用性**。

> **误解：CAP 的三个特性只有是和否两种极端选择**
>
> CAP 理论的三种特性不是二极管，不是一致和不一致，可用和不可用，分区和没分区的这类二选一的选项，而是这三种特性都是范围类型。

#### 2.1.1 CAP 的不足

CAP 是学术界理论，所以有很多问题它没有考虑到，比如：

1. CAP 定理没有考虑网络延迟的问题的，它认为一致性是立即生效的，但是，要保持一致性，是需要时间成本的，这就导致往往分布式系统多选择 AP 方式。

2. 在实践中以及后来 CAP 定理的提出者也承认，**一致性和可用性并不仅仅是二选一的问题，只是一些重要性的区别**。当强调一致性的时候，并不表示可用性是完全不可用的状态。

   比如，Zookeeper 只是在 master 出现问题的时候，才可能出现几十秒的不可用状态，而别的时候，都会以各种方式保证系统的可用性。而强调可用性的时候，也往往会采用一些技术手段，去保证数据最终是一致的。CAP 定理并没有给出这些情况的具体描述。

3. CAP 理论从工程角度来看只是一种状态的描述，它告诉大家当有错的时候，分布式系统可能处在什么状态。但是，状态是可能变化的。状态间如何转换，如何修补，如何恢复是没有提供方向的。

### 2.2 BASE 理论

BASE 理论起源于 2008 年， 由 eBay 的架构师 Dan Pritchett 在 ACM 上发表。

BASE 理论是对 CAP 理论的延伸，其核心思想是**即使无法做到强一致性（Strong Consistency，CAP 中的 C 就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）**。

1. **Basically Available（基本可用）**：分布式系统在出现不可预知故障的时候，**允许损失部分可用性**。

2. **Soft state（软状态）**：软状态也称为弱状态，和硬状态相对，是指**允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性**，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

3. **Eventually consistent（最终一致性）**：最终一致性强调的是系统中所有的数据副本，**在经过一段时间的同步后，最终能够达到一个一致的状态**。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

BASE 是对 CAP 中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于 CAP 定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），更具体地说，是对 CAP 中 AP 方案的一个补充。其基本思路就是：通过业务，牺牲强一致性而获得可用性，并允许数据在一段时间内是不一致的，但是最终达到一致性状态。

<img src="https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220624220148147.png" alt="image-20220624220148147" style="zoom: 67%;" />

>**BASE 和 ACID**
>
>ACID 是传统数据库常用的设计理念，追求**强一致性**模型；BASE 支持的是大型分布式系统，提出通过**牺牲强一致性**获得高可用性。
>
>ACID 和 BASE 代表了两种截然相反的设计哲学，在分布式系统设计的场景中，系统组件对一致性要求是不同的，因此 ACID 和 BASE 又会结合使用。

## 3. 分布式一致性（分布式共识）

**一致性算法的目的是保证在分布式系统中，多数据副本节点数据的一致性**，主要包含分布式事务，Paxos 算法，Raft 算法，ZAB 协议等。

我们首先会介绍 Paxos 和 Raft 等算法，分布式事务将会单独用一节讲解。

### 3.1 一致性的分类

我们可以把对系统一致性的要求分为三种：

1. **强一致性**：系统写入什么，读出来就是什么。
2. **弱一致性**：读取时不一定能够读到最新写入的值，也不保证多长时间后读取到的值是最新的，只是会尽量保证某个时刻到达数据一致的状态。
3. **最终一致性**：弱—致性的升级版，系统会保证在一定时间内最终达到数据一致的状态。

业界比较**推崇的是最终一致性**，但是**某些对数据一致要求十分严格的场景还是要保证强一致性**，比如银行转账。

### 3.2 拜占庭将军问题

问题描述：拜占庭帝国的军队在围攻一座城市，拜占庭帝国的军队分成许多小队，围着城市的不同方向，每支小队都有一个将军领导。这些将军们通过信使来传递消息。每位将军观察敌人的情况后，会给出一个各自的行动建议，但**最终所有小队都要达成一致的目标，或者进攻或者撤退，不然会因为敌军强大而被逐个击败**。

这里面存在的问题是：这些小队的将军有可能叛变了，或者小队派出的信使中途被敌军截杀，消息被更换，当然这其中的有问题的消息比较少。

这其实是分布式系统遇到问题的一个抽象，在分布式系统中，所有小队要达成的目标，类似分布式系统中的计算机协商，对某个操作达成共识的过程。其中信使中途被敌军截杀 ，篡改和将军叛变，对应**分布式网络中消息的丢失，发生错误，或中间人攻击**。

那么提出这个问题有什么意义呢？答案是：这个问题告诉我们**在不可靠信道上试图通过消息传递的方式达到一致性是不可能的**，所以所有的一致性算法的 **必要前提** 就是安全可靠的消息通道。

### 3.3 Paxos 算法

Paxos 算法是 Lamport 提出的一种**基于消息传递的分布式一致性算法**，使其获得 2013 年图灵奖，也是他提出了拜占庭将军问题。

Paxos 由 Lamport 于 1998 年在《The Part-Time Parliament》论文中首次公开，最初的描述使用希腊的一个小岛 Paxos 作为比喻，描述了 Paxos 小岛中通过决议的流程，并以此命名这个算法，但是这个描述理解起来比较有挑战性。后来在 2001 年，Lamport 觉得同行不能理解他的幽默感，于是重新发表了朴实的算法描述版本《Paxos Made Simple》。

自 Paxos 问世以来就持续垄断了分布式一致性算法，**Paxos 这个名词几乎等同于分布式一致性**。Google 的很多大型分布式系统都采用了 Paxos 算法来解决分布式一致性问题，如 Chubby、Megastore 以及 Spanner 等。开源的 ZooKeeper，以及 MySQL 5.7 推出的用来取代传统的主从复制的 MySQL Group Replication 等纷纷参考 Paxos 算法解决分布式一致性问题。

Paxos 算法假设没有拜占庭将军问题，即虽然有可能一个消息被传递了两次，但是绝对不会出现错误的消息。

在 Paxos 算法中，有四种角色，分别具有三种不同的行为，但多数情况，一个进程可能同时充当多种角色。

* Client：系统外部角色，请求发起者，不参与决策。
* Proposer：接收 Client 的请求， 提出提案（Proposal）。Proposal 信息包括提案编号（Proposal ID）和提议的值（Value）。
* Acceptor：参与决策，回应 Proposer 的提案。收到 Proposal 后可以接受提案，若 Proposal 获得多数 Acceptor 的接受，则称该 Proposal 被批准。
* Learners：不参与决策，从 Proposer / Acceptor 学习最新达成一致的提案（Value）。

Paxos 算法的基本思想是：Proposer 将发起提案（Value）给所有 Accpetor，超过半数 Accpetor 获得批准后，Proposer 将提案写入 Accpetor 内，最终所有 Accpetor 获得一致性的确定性取值，且后续不允许再修改。Learners 只能“学习”被批准的提案。

> **比喻**
>
> 可以理解为人大代表（Proposer）在人大向其它代表（Acceptor）提案，通过后让老百姓（Learner）落实。

Paxos 算法其实有三个版本，最基本的也就是 Basic Paxos，所以我们主要是讲述它。

#### 3.3.1 Basic Paxos

基本的 Paxos 算法分为三个阶段：

##### 3.3.1.1 第一阶段： Prepare 阶段

1. Proposer 提出一个提案，编号为 N（也就是 prepare(N)），发送给所有的 Acceptor。

   提议编号可理解为提议版本号，要求不能冲突，且每次生成必须是递增的。

2. 每个 Acceptor 都保存自己的 Accept 的最大提案编号 maxN，**当 Acceptor 收到 prepare(N) 请求时，会比较 N 与 maxN 的值，若 N 小于 maxN，则提案已过时，拒绝 prepare(N) 请求；若 N 大于等于 maxN，则接受提案，并将该 Acceptor 曾经接受过的编号最大的提案 proposal(myid, maxN, value) 反馈给 Proposer**。

   其中，myid 表示 Acceptor 的标识 ID，maxN 表示接受过的最大提案编号 maxN，value 表示提案内容。

   若当前 Acceptor 未曾 Accept 任何提议，会将 proposal(myid, null, null) 反馈给提议者。

Acceptor 在接收提案之后做出两个保证（Promise）：

1. 不再接受 Proposal Number **小于等于**当前请求的 Prepare 请求。
2. 不再接受 Proposal Number **小于**当前请求的 Propose 请求。

##### 3.3.1.2  第二阶段：Accept 阶段

1. **Proposer 发出 prepare(N) 后，若收到超过半数 Acceptor 的反馈，Proposal 则将真正的提案内容 proposal(N, value) 发送给 Acceptor**。

2. Acceptor 接受提议者发送的 proposal(N, value) 提案后，会比较自己曾经 Accept 过的最大提案编号 maxN 和反馈过的 prepare 的最大编号，若 N **大于**这两个编号，则当前表决者 Accept 该提案，并反馈给提议者；否则拒绝该提议。

   最后确认的提议基本上就是版本号最新的提议，而版本号最新的提议内容永远是被大多数 Acceptor 接收提案内容。

被拒绝的 Proposer 可以重新进入 Prepare 阶段，重新提出提案（此时编号又自增了）。

从这里我们可以看出，Basic Paxos 是有隐患的，如果两个 Proposer 交替 Prepare 成功，而 Accept 失败，就会形成活锁。

##### 3.3.1.3 第三阶段：Learn 阶段

Proposer 在收到多数 Acceptor 的 Accept 之后，标志着本次 Accept 成功，决议形成，将形成的决议发送给所有 Learner。

##### 3.3.1.4 整体流程

![image-20220625182734908](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625182734908.png)

* 获取一个Proposal ID n，为了保证 Proposal ID 唯一，可采用时间戳 + Server ID 生成；
* Proposer 向所有 Acceptor 广播 Prepare(n) 请求；
* Acceptor 比较 n 和 minProposal，如果 n > minProposal，则 minProposal = n，并且将 acceptedProposal 和 acceptedValue 返回；
* Proposer 接收到**过半数**回复后，如果发现有 acceptedValue 返回，将所有回复中 acceptedProposal 最大的 acceptedValue 作为本次提案的 Value，否则可以任意决定本次提案的 Value；

到这里可以进入第二阶段，广播 Accept(n, value) 到所有节点

- Acceptor 比较 n 和 minProposal，如果 n >= minProposal，则 acceptedProposal = minProposal = n，acceptedValue = value，本地持久化后，返回；否则，返回 minProposal。
- 提议者接收到过半数请求后，如果发现有返回值 result > n，表示有更新的提议，跳转到第一步；否则 Value 达成一致。

##### 3.3.1.5 实例

以下例子中，P 表示 Prepare 阶段，A 表示 Accept 阶段，X 和 Y 代表提案的 Value，S1 ~ S5 表示 5 台服务器，A.B 表示提案 Proposal ID，其中 A 为时间戳，B 为 Server ID。

 ![image-20220626150333613](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626150333613.png)

上面的例子中，3.1 被过半数的服务器所接受，返回了 X 值，X 值就被决定了，后来的 4.5 失效，学习到 X 值。

![image-20220626150812489](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626150812489.png)

上面的例子中，3.1 只被 S3 接收，而 4.5 又被通过，S3 返回曾经接受过的 X，X 被决定。

![image-20220626150706122](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626150706122.png)

上面的例子中，3.1 只有 S1 接受了，但是后来的 4.5 通过了，但是其中接受了的服务器没有返回任何值，因此 4.5 随意决定自己的 Value，也就是 Y。

![image-20220626151517630](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626151517630.png)

上面的例子就是我们讲过的活锁问题，在 3.1 落地之前，3.5 又被通过，然后 3.5 落地之前，4.1 又被通过，4.1 落地之前，5.5 又被通过...

##### 3.3.1.6 改进——Multi Paxos

Multi Paxos 引入了 Leader 概念，它是**唯一的 Proposer**，所有的请求都需经过 Leader。

**Multi Paxos 首先需要选举 Leader，Leader 的确定也是一次决议的形成，所以可执行一次 Basic Paxos 实例来选举出一个 Leader**。

选出 Leader 之后只能由 Leader 提交 Proposal，在 Leader 宕机之后服务临时不可用，需要重新选举 Leader 继续服务。

在系统中仅有一个 Leader 进行 Proposal 提交的情况下，Prepare 阶段可以跳过。Multi Paxos 通过改变 Prepare 阶段的作用范围至后面 Leader 提交的所有实例，从而使得 **Leader 的连续提交只需要执行一次 Prepare 阶段，后续只需要执行 Accept 阶段，将两阶段变为一阶段，提高了效率**。

为了区分连续提交的多个实例，每个实例使用一个 Instance ID 标识，Instance ID 由 Leader 本地递增生成即可。

Multi Paxos 允许有多个自认为是 Leader 的节点并发提交 Proposal 而不影响其安全性，这样的场景即退化为 Basic Paxos。

后面我们要讲的 ZAB 也是 Multi Paxos 的变形。

### 3.4 Raft 算法

不同于 Paxos 算法直接从分布式一致性问题出发推导出来，Raft 算法则是**从多副本状态机的角度提出，用于管理多副本状态机的日志复制**。

Raft 实现了和 Paxos 相同的功能，它将一致性分解为多个子问题：Leader 选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。同时，Raft 算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

强烈建议使用：[Raft 动画](http://thesecretlivesofdata.com/raft/) 网站来学习 Raft。 

#### 3.4.1 角色

Raft 将系统中的角色分为 领导者（Leader）、跟从者（Follower）和候选人（Candidate）:

- `Leader`：负责发起心跳，响应客户端，创建日志，同步日志。
- `Follower`：接受 Leader 的心跳和日志同步数据，投票给 Candidate。
- `Candidate`：Leader 选举过程中的临时角色，由 Follower 转化而来，发起投票参与竞选。

Raft 要求系统在任意时刻最多只有一个 Leader，正常工作期间只有 Leader 和 Follower。

角色状态转换图如下：

![image-20220626163120295](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626163120295.png)

Follower 只响应其他服务器的请求。如果 Follower 超时没有收到 Leader 的消息（证明 Leader 没有心跳了），它会成为一个 Candidate 并且开始一次 Leader 选举。收到大多数服务器投票的 Candidate 会成为新的 Leader，Leader 在宕机之前会一直保持 Leader 的状态。

#### 3.4.2 任期

**Raft 算法将时间分为一个个的任期**（term），每一个 term 的开始都是 Leader 选举。在成功选举 Leader 之后，Leader 会在整个 term 内管理整个集群。如果Leader 选举失败，该 term 就会因为没有 Leader 而结束，进而开始下一个任期。

每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号，term 号的作用是：

* **如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值**。
* **如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower**。
* **如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求**。

![image-20220626163325342](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626163325342.png)

#### 3.4.3  日志

在 Raft 算法中，**需要实现分布式一致性的数据被称作日志**。

![image-20220626164648702](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626164648702.png)

如上图所示，Raft 的日志以多条日志项（LogEntry）的形式来组织，**每个日志项包含一条命令、任期信息、日志项在日志中的位置信息**（索引值 LogIndex）。

- 指令：由客户端请求发送的执行指令，理解成客户端需要存储的日志数据即可。
- 索引值：日志项在日志中的位置，需要注意索引值是一个连续并且单调递增的整数。
- 任期编号：创建这条日志项的 Leader 的任期编号。

只有 Leader 才可以改变其他节点的 log。entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。

#### 3.4.4 Leader 选举

我们以三个节点的初始化的集群为例，讲一下这个过程。

**初始状态下，所有的节点都是 Follower**：

![image-20220626191625094](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626191625094.png)

Raft 每个节点初始化后的心跳超时时间都是随机的，如上所示，节点 C 的超时时间最短（120ms），任期编号都为 0，角色都是 Follower。

接下来，由于 C 超时时间最短，因此它很快意识到没有 Leader，于是自己成为 Candidate，并发起投票（自己会给自己投票），这个投票也叫做 RequestVote RPC 请求，此时 term，也就是任期编号会 + 1：

![image-20220626191814148](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626191814148.png)

Follow 收到请求投票信息后，如果该 Candidate 符合投票要求后，则将自己宝贵（因为每个任期内 Follower 只能投给先来的候选人一票，后面来的候选人则不能在投票给它了）的一票投给该 Candidate，同时更新 term。

![image-20220626191924311](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626191924311.png)

Candidate 的状态会持续到以下情况发生：

- 自己赢得选举。
- 其他节点赢得选举。
- 一轮选举结束，**无人胜出**。

赢得选举的条件是：**一个 Candidate 在一个任期内收到了来自集群内的多数选票 `（N/2+1）`，就可以成为 Leader**。

当然，在 Candidate 等待选票的时候，它可能收到其他节点声明自己是 Leader 的心跳，此时有两种情况：

- 该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。
- 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。

在这个例子中，C 已经收到了大多数的选票，当选 Leader：

![image-20220626192004494](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626192004494.png)

接下来，**Leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位**，也就是说，Leader 会定期向所有 Follower 告知自己是  Leader，同时**刷新 Follower 的超时时间**，防止 Follower 发起新的 Leader 选举。这个机制也被称作心跳机制。

![image-20220626192202054](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626192202054.png)

如果一个 Follower 在一个周期内没有收到心跳信息，就叫做选举超时，然后它就会认为此时没有可用的 Leader，并且开始进行一次选举以选出一个新的 Leader。

##### 3.4.4.1 随机超时

由于可能同一时刻出现多个 Candidate，导致没有 Candidate 获得大多数选票，如果没有其他手段来重新分配选票的话，那么可能会无限重复下去。

Raft 使用了随机的选举超时时间来避免上述情况：**每一个 Candidate 在发起选举后，都会随机化一个新的选举超时时间**。这种机制使得各个服务器能够分散开来，在大多数情况下只有一个服务器会率先超时；它会在其他服务器超时之前赢得选举。

#### 3.4.5 日志同步

一旦选出了 Leader，它就开始接受客户端的请求，每一个客户端的请求都包含一条需要被复制状态机（`Replicated State Mechine`）执行的命令。

Leader 收到客户端请求后，会生成一个日志项，也就是 entry，包含`<index, term, cmd>`，再将这个 entry 添加到自己的本地日志末尾后，向所有的节点广播该 entry，也就是发起一次 AppendEntries RPC，要求其他服务器复制这条 entry。

如果 Follower 接受该 entry，则会将 entry 添加到自己的日志后面，同时返回给 Leader 同意。

如果 Leader 收到了多数的成功响应，Leader 会将这个 entry 应用到自己的状态机中，之后可以成为这个 entry 是 committed 的，并且向客户端返回执行结果。

![image-20220626191403057](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626191403057.png)

当然，上面讲到的是理想情况下的结果，这个过程中难免会出现节点宕机等问题，Raft 是如何处理的呢？

##### 3.4.5.1 保持日志的一致性

上面讲到，在正常情况下，Leader 的 AppendEntries RPC 请求响应都成功的情况下，Leader 和 Follower 的日志保持一致性。然而在 Leader 突然宕机的情况下有可能会造成 Leader 与 Follower 日志不一致的情况，这种情况会随着后续 Leader 一些列宕机的情况下加剧问题的严重：

![image-20220626201331720](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626201331720.png)

如上图所示，当一个 Leader 成功当选时，Follower 有可能是 a-f 的情况：

1. a-b 表示 Follower 的日志项落后于当前 Leader；
2. c-d 表示 Follower 有些日志项没有被提交；
3. e-f 情况稍微有点复杂，以上两种情况它们都存在。

在 Raft 的日志机制中，为了简化日志一致性的行为，有以下两点非常重要的特性：

1. **如果在不同的日志中的两个条目拥有相同的索引和任期号，那么它们存储了相同的指令。**
2. **如果在不同的日志中的两个条目拥有相同的索引和任期号，那么它们之前的所有日志条目也全部相同。**

第一个特性是因为 Raft 日志项在日志中不会改变，因此只要日志项只要是索引值和任期号相同，就可以认为他们是存储了相同的指令数据信息。

第二个特性是因为 Leader 会通过强制覆盖的方式让 Follower 复制自己的日志来解决日志不一致的问题，Leader 在 AppendEntries RPC 请求过程中会附带需要复制的日志以及前一个日志项相关信息，如果 Follower 匹配不到包含相同索引位置和任期号的日志项，那么它就会拒绝接收新的日志条目，接着 Leader 会继续递减要复制的日志项索引值，直至找到相同索引和任期号的日志项，最后就直接覆盖 Follower 之后的日志项。可认为两个条目拥有相同的索引和任期号，那么它们之前的所有日志条目也全部相同。

因此，Raft 的日志追加大致可分为两个步骤：

1. **Leader 找到 Follower 与自己相同的最大日志项**，这意味着 Follower 之前的日志都与 Leader 的日志相同；
2. **Leader 强制覆盖之后不一致的日志**，实现日志的一致性。

看一个例子吧：

假设有一个 Leader 和一个 Follower ，它们的日志项复制情况如下：

![image-20220626195539162](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626195539162.png)

可以看出，Follower 在任期号 3 时是 Leader，在追加日志过程中崩溃了，重启之后成为 Follower，随后新的 Leader 向其追加日志，此时它的任期号为 3 最后的一个日志项将被覆盖。

下面是 AppendEntries RPC 请求的全部参数：

| 参数         | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| term         | Leader 的任期                                                |
| leaderId     | Leader ID<BR/>因此 Follower 可以对客户端进行重定向（Follower 根据 Leader ID 把客户端的请求重定向到 Leader，比如有时客户端把请求发给了 Follower 而不是 Leader） |
| prevLogIndex | 紧邻新日志条目之前的那个日志条目的索引                       |
| prevLogTerm  | 紧邻新日志条目之前的那个日志条目的任期                       |
| entries[]    | 需要被保存的日志条目（被当做心跳使用是 则日志条目内容为空；为了提高效率可能一次性发送多个） |
| leaderCommit | Leader 的已知已提交的最高的日志条目的索引                    |

则 Leader 追加并覆盖 Follower 日志的过程如下：

![image-20220626195743369](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626195743369.png)

1. Leader 通过日志 AppendEntries RPC 请求，将当前最新的要追加到 Follower 的日志项以及前一个它的 prevLogIndex=7、prevLogTerm=3 等信息发送跟 Follower ；
2. Follower 判断当前最新的日志的任期号与 prevLogTerm 不一致，拒绝追加；
3. Leader 继续递减需要复制的日志项的索引值，此时 prevLogIndex=6、prevLogTerm=3；
4. Follower 找到了 LogIndex=6、LogTerm=3 的日志项，Follower 接受追加请求；
5. Leader 接着会将 Follower LogIndex=6、LogTerm=3 的日志项之后的日志项进行追加并覆盖。

#### 3.5.6 安全性

Raft 有一些机制来保证安全性。

##### 3.5.6.1 选举限制

Leader 需要保证自己存储全部已经提交的日志条目。这样才可以使日志条目只有一个流向：从 Leader 流向 Follower，Leader 永远不会覆盖已经存在的日志条目。

每个 Candidate 发送 RequestVoteRPC 时，都会带上最后一个 entry 的信息。所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的更新，则拒绝投票给该 Candidate。

判断日志新旧的方式：如果两个日志的 term 不同，term 大的更新；如果 term 相同，更长的 index 更新

##### 3.5.6.2 节点崩溃

分两种情况：

* 如果 Leader 崩溃，集群中的节点在 electionTimeout 时间内没有收到 Leader 的心跳信息就会触发新一轮的选主，在选主期间整个集群对外是不可用的。

* 如果 Follower 和 Candidate 崩溃，处理方式会简单很多：之后发送给它的 RequestVoteRPC 和 AppendEntriesRPC 会失败。

  由于 Raft 的所有请求都是幂等的，所以失败的话会无限的重试。如果崩溃恢复后，就可以收到新的请求，然后选择追加或者拒绝 entry。

##### 3.5.6.3 时间与可用性

Raft 的要求之一就是安全性不依赖于时间：系统不能仅仅因为一些事件发生的比预想的快一些或者慢一些就产生错误。为了保证上述要求，最好能满足 `broadcastTime << electionTimeout << MTBF`，其中：

- `broadcastTime`：向其他节点并发发送消息的平均响应时间；
- `electionTimeout`：选举超时时间；
- `MTBF(mean time between failures)`：单台机器的平均健康时间；

`broadcastTime`应该比`electionTimeout`小一个数量级，为的是使`Leader`能够持续发送心跳信息（heartbeat）来阻止`Follower`开始选举；

`electionTimeout`也要比`MTBF`小几个数量级，为的是使得系统稳定运行。当`Leader`崩溃时，大约会在整个`electionTimeout`的时间内不可用；我们希望这种情况仅占全部时间的很小一部分。

由于`broadcastTime`和`MTBF`是由系统决定的属性，因此需要决定`electionTimeout`的时间。

一般来说，broadcastTime 一般为 `0.5～20ms`，electionTimeout 可以设置为 `10～500ms`，MTBF 一般为一两个月。

### 3.5  ZAB 协议

ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持**崩溃恢复**的**原子广播**协议。

在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。

![image-20220626222941230](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220626222941230.png)

#### 3.5.1 角色

ZAB 中也存在三个主要的角色：

- `Leader` ：集群中**唯一**的**写请求**处理者 ，能够发起投票（投票也是为了进行写请求）。
- `Follower`：能够接收客户端的请求，如果是读请求则可以自己处理，**如果是写请求则要转发给 `Leader`** 。在选举过程中会参与投票，**有选举权和被选举权** 。
- `Observer` ：就是没有选举权和被选举权的 `Follower` 。

在 ZAB 协议中对 zkServer（即上面我们说的三个角色的总称）还有两种模式的定义，分别是**消息广播**和**崩溃恢复** 。



## 4. 分布式事务

我们通常接触到的事务（比如数据库事务）都具有 ACID 的特性，要么全部成功，要么全部失败。

我们注意到，ACID 中也包含有一致性，在普通的单体应用中，事务一致性靠 AID 来保证，是一种强一致性；但是在分布式的架构中，我们也不可避免的涉及到事务，但是**分布式系统中的每个节点只知道自己在进行事务提交成功或失败，但无法直到其他节点的事务提交情况**。

为了保持分布式事务的 ACID 特性，需要引入协调者来统一调度分布式节点（参与者），这也就是我们要介绍的分布式事务协议，**分布式事务协议的终极目标就是保证系统中多个相关联的数据库中的数据的一致性**。

基于 CAP 和 BASE 理论，我们知道分布式事务是无法完全满足 ACID 特性的，只能选择一个比较折中的方案。

### 4.1 刚性事务

刚性事务满足 CAP 中的 CP，遵循 ACID，对数据要求**强一致性**。

常见的刚性事务方案有：

* **XA协议**：这是一个基于数据库层面的分布式事务协议，其分为两部分：**事务管理器（Transaction Manager）**和**本地资源管理器（Resource Manager）**。

  事务管理器作为一个全局的调度者，负责对各个本地资源管理器统一号令提交或者回滚。主流的数据库均已实现了 XA 接口。

  基于 XA 协议，又衍生出 2PC 和 3PC 协议：

  * **两阶段提交协议（2PC）**：引入一个作为协调者的组件来统一掌控所有参与者的操作结果并最终指示这些节点是否要把操作结果进行真正的提交；参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

    所谓的两个阶段分别是：第一阶段：准备阶段（投票阶段）；第二阶段：提交阶段（执行阶段）。

  * **三阶段提交协议（3PC）**：这是对两段提交（2PC）的一种升级优化，**3PC 在 2PC 的第一阶段和第二阶段中插入了一个准备阶段**，保证了在最后提交阶段之前，各参与者节点的状态都一致。

    同时，在协调者和参与者中都引入超时机制，当参与者各种原因未收到协调者的 commit 请求后，会对本地事务进行 commit，不会一直阻塞等待，解决了 2PC 的单点故障问题，但 3PC 还是没能从根本上解决数据一致性的问题。

* Java 事务规范：

  * **JTA**：Java事务 API（Java Transaction API）是一个 Java 企业版的应用程序接口，在 Java 环境中，允许完成跨越多个 XA 资源的分布式事务。
  * **JTS**：Java事务服务（Java Transaction Service）是 J2EE 平台提供了分布式事务服务的具体实现规范，J2EE 服务器提供商根据 JTS 规范实现事务并提供 JTA 接口。

刚性事务基本都需要依靠数据库层面的实现，因此也适用于数据库层。

#### 4.1.1 2PC（两阶段提交）

两阶段提交（2PC - Prepare & Commit）是指两个阶段的提交：

* 第一阶段：准备阶段
  * 协调者向所有参与者发送 REQUEST-TO-PREPARE
  * 当参与者收到 REQUEST-TO-PREPARE 消息后, 它向协调者发送消息 PREPARED 或者 NO，表示事务是否准备好；如果发送的是NO，那么事务要回滚；

* 第二阶段：提交阶段。
  * 协调者收集所有参与者的返回消息, 如果所有的参与者都回复的是 PREPARED， 那么协调者向所有参与者发送 COMMIT 消息；否则，协调者向所有回复 PREPARED 的参与者发送 ABORT 消息；
  * 参与者如果回复了 PREPARED 消息并且收到协调者发来的 COMMIT 消息，或者它收到 ABORT 消息，它将执行提交或回滚，并向协调者发送 DONE 消息以确认。

示意图如下：

![image-20220624231105201](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220624231105201.png)



2PC 看似能够提供原子性的操作，但它存在着严重的缺陷：

- **网络抖动导致的数据不一致**：第二阶段中协调者向参与者发送 commit 命令之后，一旦此时发生网络抖动，导致一部分参与者接收到了 commit 请求并执行，可其他未接到 commit 请求的参与者无法执行事务提交，进而导致整个分布式系统出现了数据不一致。
- **超时导致的同步阻塞问题**：2PC 中的所有的参与者节点都为事务阻塞型，当某一个参与者节点出现通信超时，其余参与者都会被动阻塞占用资源不能释放。
- **单点故障的风险**：由于严重的依赖协调者，一旦协调者发生故障，而此时参与者还都处于锁定资源的状态，无法完成事务 commit 操作。虽然协调者出现故障后，会重新选举一个协调者，可无法解决因前一个协调者宕机导致的参与者处于阻塞状态的问题。

#### 4.1.2 3PC（三阶段提交）

3PC 是对 2PC 的一种升级优化，**3PC 在 2PC 的第一阶段和第二阶段中插入一个准备阶段**，保证了在最后提交阶段之前，各参与者节点的状态都一致。

同时，在协调者和参与者中都**引入超时机制**，当参与者各种原因未收到协调者的 commit 请求后，会对本地事务进行 commit，不会一直阻塞等待，解决了 2PC 的单点故障问题，但 3PC 还是没能从根本上解决数据一致性的问题。

3PC 的三个阶段分别是 CanCommit、PreCommit、DoCommit：

- **CanCommit**：协调者向所有参与者发送 CanCommit 命令，询问是否可以执行事务提交操作。如果全部响应 Yes 则进入下一个阶段。

- **PreCommit**：协调者向所有参与者发送 PreCommit 命令，询问是否可以进行事务的预提交操作。

  参与者接收到 PreCommit 请求后，如参与者成功的执行了事务操作，则返回 Yes 响应，进入最终 DoCommit 阶段。

  一旦参与者中有向协调者发送了 No 响应，或因网络造成超时，协调者没有接到参与者的响应，协调者向所有参与者发送 ABORT 请求，参与者接受 ABORT 命令执行事务的中断。

- **DoCommit**：在前两个阶段中所有参与者的响应反馈均是 YES 后，协调者向参与者发送 DoCommit 命令正式提交事务，如协调者没有接收到参与者发送的 ACK 响应，会向所有参与者发送 ABORT 请求命令，执行事务的中断。

![image-20220624231718796](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220624231718796.png)

3PC 依然存在问题：3PC 工作在同步网络模型上，它假设消息传输时间是有上界的，只存在机器失败而不存在消息失败。

这个假设太强，现实的情形是：机器失败是无法完美地检测出来的，消息传输可能因为网络拥堵花费很多时间。同时，说阻塞是相对，存在协调者和参与者同时失败的情形下，3PC 事务依然会阻塞。

实际上，**很少会有系统实现 3PC，多数现实的系统会通过复制状态机解决 2PC 阻塞的问题**。

### 4.2 柔性事务

柔性事务满足分布式理论的 AP，遵循 BASE，允许一定时间内不同节点的数据不一致，但**要求最终一致**。

实际上，柔性事务就是 BASE 理论 + 业务实践。柔性事务追求的目标是：**根据自身业务特性，通过适当的方式来保证系统数据的最终一致性**。像 TCC、Saga、MQ 事务、本地消息表就属于柔性事务。

#### 4.2.1 TCC（补偿事务）

TCC 是 Try、Confirm、Cancel 三个单词的首字母缩写，这也意味着它分为三个阶段：

1. Try（尝试）：尝试执行。

   此阶段会完成业务检查，并预留好必需的业务资源。

2. Confirm（确认）：确认执行。

   当所有事务参与者的 Try 阶段执行成功就会执行 Confirm ，Confirm 阶段会处理 Try 阶段预留的业务资源；否则，就会执行 Cancel。

3. Cancel（取消）：取消执行，释放 Try 阶段预留的业务资源。

我们以下单扣库存的例子来解释一下：

* **Try 阶段**：下单时通过 Try 操作去扣除库存预留资源。
* **Confirm 阶段**：确认执行业务操作，在只预留的资源基础上，发起购买请求。
* **Cancel 阶段**：只要涉及到的相关业务中，有一个业务方预留资源未成功，则取消所有业务资源的预留请求。

也就是说，正常情况下会执行 try 和 confirm，出现异常则会执行 try 和 cancel，如下图所示：

![img](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/arch-z-trans-6.png)

因此，TCC 模式不需要依赖于底层数据资源的事务支持，但是需要我们手动实现更多的代码，属于**侵入业务代码**的一种分布式解决方案。 

针对 TCC 的实现，业界也有一些不错的开源框架。不同的框架对于 TCC 的实现可能略有不同，不过大致思想都一样。 

1. ByteTCC：ByteTCC 是基于 Try-Confirm-Cancel（TCC）机制的分布式事务管理器的实现。
2. Seata：Seata 是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。
3. Hmily：金融级分布式事务解决方案。

> **提示**
>
> 细心的读者已经发现了，TCC 与 2PC 的思想很相似，事务处理流程也很相似。
>
> 它们的区别在于：2PC 是应用在 DB 层面，TCC 则可以理解为在应用层面的 2PC，是需要我们编写业务逻辑来实现。

TCC 当然也存在一些缺陷：

1. **空回滚**：当一个分支事务所在的服务发生宕机或者网络异常导致调用失败，并未执行 try 方法，当恢复后事务执行回滚操作就会调用此分支事务的 cancel 方法，如果 cancel 方法不能处理此种情况就会出现空回滚。

   是否出现空回滚，我们需要需要判断是否执行了 try 方法，如果执行了就没有空回滚。

   解决方法就是当主业务发起事务时，**生成一个全局事务记录，并生成一个全局唯一 ID**，贯穿整个事务，再创建一张分支事务记录表，用于记录分支事务，try 执行时将全局事务 ID 和分支事务 ID 存入分支事务表中，表示执行了 try 阶段，当 cancel 执行时，先判断表中是否有该全局事务 ID 的数据，如果有则回滚，否则不做任何操作。

   > **全局唯一 ID**
   >
   > 由于分布式的特殊性，很多情况下我们需要一个全局唯一的 ID，有许多种算法可以产生一个这样的 ID，我们之后会详细介绍。

2. **幂等问题**：由于服务宕机或者网络问题，方法的调用可能出现超时，为了保证事务正常执行我们往往会加入重试的机制，因此就需要保证 confirm 和 cancel 阶段操作的幂等性。

   我们可以在分支事务记录表中增加事务执行状态，每次执行 confirm 和 cancel 方法时都查询该事务的执行状态，以此判断事务的幂等性。

3. **悬挂问题**：TCC 中，在调用 try 之前会先注册分支事务，注册分支事务之后，调用出现超时，此时 try 请求还未到达对应的服务，因为调用超时了，所以会执行 cancel 调用，此时 cancel 已经执行完了，然而这个时候 try 请求到达了，这个时候执行了 try 之后就没有后续的操作了，就会导致资源挂起，无法释放。

   解决方法是：执行 try 方法时我们可以判断 confirm 或者 cancel 方法是否执行，如果执行了那么就不执行 try 阶段；这同样需要借助分支事务表中事务的执行状态。

#### 4.2.2 Sega 事务

## 5. 分布式中的全局唯一 ID

## 6. 分布式锁

我们应该都见过或者用过编程语言中提供的锁，当然也有可能直接使用操作系统提供的锁，这些锁能够很好的完成并发控制。

但是在分布式环境下，这些锁就不能实现并发控制了，因为它们是单机的，这时就需要有分布式锁。**当多个进程不在同一个机器中（比如分布式系统中控制共享资源访问）时，可以使用分布式锁控制多个进程对资源的访问**。

分布式锁应该满足如下要求：

1. **互斥**：在任何给定时刻，只有一个客户端可以持有锁。
2. **无死锁**：即使锁定资源的客户端崩溃或被分区，也总是可以获得锁；通常通过超时机制实现。
3. **容错性**：只要大多数节点都启动，客户端就可以获取和释放锁。

除此之外，分布式锁的设计中还需要应该考虑：

1. 加锁解锁的**同源性**：A 加的锁，不能被 B 解锁
2. 获取锁是**非阻塞**的：如果获取不到锁，不能无限期等待；
3. **高性能**：加锁解锁是高性能的

常见的分布式锁实现方案有：

* **基于数据库实现分布式锁**

  - 基于数据库表（锁表，很少使用）

  - 乐观锁（基于版本号）

  - 悲观锁（基于排它锁）

* **基于 redis 实现分布式锁**:

  - 单个 Redis 实例：setnx(key, 当前时间 + 过期时间)+ Lua

  - Redis 集群：Redlock

* **基于 zookeeper实现分布式锁**
  - 临时有序节点来实现的分布式锁

### 6.1 基于数据库实现的分布式锁

#### 6.1.1 基于数据库表实现

最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现：当我们想要获得锁的时候，就可以在该表中增加一条记录，想要释放锁的时候就删除这条记录。

比如：

```mysql
CREATE TABLE database_lock (
    `id` BIGINT NOT NULL AUTO_INCREMENT,
    `resource` int NOT NULL COMMENT '锁定的资源',
    `description` varchar(1024) NOT NULL DEFAULT "" COMMENT '描述',
    PRIMARY KEY (id),
    UNIQUE KEY uiq_idx_resource (resource)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='数据库分布式锁表';
```

当我们想要获得锁时，可以插入一条数据：

```sql
INSERT INTO database_lock(resource, description) VALUES (1, 'lock');   
```

当需要释放锁的时，可以删除这条数据：

```sql
DELETE FROM database_lock WHERE resource = 1;
```

其实，我们主要是依赖数据库表的主键，因为它自增，且唯一。

它的优点自然是简单，缺点也显而易见：

1. 因为是基于数据库实现的，数据库的可用性和性能将直接影响分布式锁的可用性及性能；

2. 不具备可重入的特性，因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据；

   所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁。

3. 没有锁失效机制，因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除，当服务恢复后一直获取不到锁；

   所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据。

4. 不具备阻塞锁特性，获取不到锁直接返回失败；

   所以需要优化获取逻辑，循环多次去获取。

我们看到，这种方案下的分布式锁需要考虑很多额外的问题，为了解决这些问题，实现方式将会越来越复杂；依赖数据库需要一定的资源开销，性能问题需要考虑，因此实际上我们很少采用这种方案。

### 6.2 基于 Redis 实现的分布式锁

参见 Redis 系列教程。

### 6.3  基于 Zookeeper 实现的分布式锁



## 7. 分布式缓存

## 8. 分布式任务

## 9. 分布式会话