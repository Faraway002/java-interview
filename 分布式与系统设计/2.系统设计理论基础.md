[TOC]

# 系统设计理论基础

## 1. 概述

## 2. 高可用

### 2.1 集群

### 2.2 限流

### 2.3 服务降级 & 服务熔断

### 2.4 超时 & 重试机制

## 3. 高性能

### 3.1 读写分离

### 3.2 分库分表

### 3.3 负载均衡

负载均衡（Load Balance）是高并发、高可用系统必不可少的关键组件，目标是**尽力将网络流量平均分发到多个服务器上，以提高系统整体的响应速度和可用性**。

负载均衡的主要作用如下：

* **高并发**：负载均衡通过算法调整负载，尽力均匀的分配应用集群中各节点的工作量，以此提高应用集群的并发处理能力（吞吐量）。
* **伸缩性**：添加或减少服务器数量，然后由负载均衡进行分发控制。这使得应用集群具备伸缩性。
* **高可用**：负载均衡器可以监控候选服务器，当服务器不可用时，自动跳过，将请求分发给可用的服务器。这使得应用集群具备高可用的特性。
* **安全防护**：有些负载均衡软件或硬件提供了安全性功能，如：黑白名单处理、防火墙，防 DDos 攻击等。

#### 3.3.1 负载均衡的分类

##### 3.3.1.1 按载体维度分类

从支持负载均衡的载体来看，可以将负载均衡分为两类：**硬件负载均衡和软件负载均衡**。

###### 3.3.1.1.1 硬件负载均衡

硬件负载均衡一般是在定制处理器上运行的独立负载均衡服务器，价格昂贵。

**优点**：

- 功能强大：支持全局负载均衡并提供较全面的、复杂的负载均衡算法。
- 性能强悍：硬件负载均衡由于是在专用处理器上运行，因此吞吐量大，可支持单机百万以上的并发。
- 安全性高：往往具备防火墙，防 DDos 攻击等安全功能。

**缺点**：

- 成本昂贵：购买和维护硬件负载均衡的成本都很高。
- 扩展性差：当访问量突增时，超过限度不能动态扩容。

###### 3.3.1.1.2 软件负载均衡

应用最广泛，从软件层面实现负载均衡，一般可以在任何标准物理设备上运行（比如 Ngnix）。

**优点**：

- 扩展性好：适应动态变化，可以通过添加软件负载均衡实例，动态扩展到超出初始容量的能力。
- 成本低廉：软件负载均衡可以在任何标准物理设备上运行，降低了购买和运维的成本。

 **缺点**：

- 性能略差：相比于硬件负载均衡，软件负载均衡的性能要略低一些。

##### 3.3.1.2 按网络通信分类

软件负载均衡从通信层面来看，又可以分为四层和七层负载均衡。

* **七层负载均衡**：就是可以根据访问用户的 HTTP 请求头、URL 信息将请求转发到特定的主机。

  - DNS 重定向

  - HTTP 重定向

  - 反向代理

- **四层负载均衡**：基于 IP 地址和端口进行请求的转发。

  - 修改 IP 地址

  - 修改 MAC 地址

###### 3.3.1.2.1 DNS 负载均衡

DNS 负载均衡一般用于互联网公司，复杂的业务系统不适合使用。大型网站一般使用 DNS 负载均衡作为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。



###### 3.3.1.2.2 HTTP 负载均衡

###### 3.3.1.2.3 反向代理负载均衡

###### 3.3.1.2.4 IP 负载均衡

#### 3.3.2 负载均衡算法

##### 3.3.2.1 一致性 Hash

一致性 Hash 多用于分布式负载均衡中，对于分布式系统，不同机器上存储不同对象的数据，我们使用 Hash 函数建立从数据到服务器之间的映射关系。

哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 `hash(key) % 3` 公式对数据进行了映射。

如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点：

```scss
hash(key) % 3
```

如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。

假设我们有 10 个数据，哈希值分别为 1 ~ 10，则：

* 机器 0 上保存的数据有：3，6，9
* 机器 1 上保存的数据有：1，4，7，10
* 机器 2 上保存的数据有：2，5，8

但是有一个很致命的问题，**如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据**，否则会出现查询不到数据的问题。

要解决这个问题的办法，就需要我们进行**迁移数据**，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。

比如，当增加一台机器后，此时 n = 4，同样的 10 个数据，各个机器上存储的数据分别为：

* 机器 0 上保存的数据有：4，8
* 机器 1 上保存的数据有：1，5，9
* 机器 2 上保存的数据有：2，6，10
* 机器 3 上保存的数据有：3，7

这个例子中，我们看到，从 3 台机器增加到 4 台时，只有数据 1 和数据 2 没有移动。我们假设总数据条数为 M，哈希算法在面对节点数量变化时，**最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 $O(M)$**，这样数据的迁移成本太高了，**当集群中数据量很大时，采用一般的哈希函数，在节点数量动态变化的情况下会造成大量的数据迁移，导致网络通信压力的剧增，甚至导致数据库宕机**。

引入一致性 Hash 算法就可以解决此类问题，它可以保证当机器增加或者减少时，节点之间的数据迁移只限于两个节点之间，不会造成全局的网络问题。

##### 3.3.5.1 Hash 环

一致性哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而**一致哈希算法是对 $2^{32}$ 进行取模运算，是一个固定的值**。

我们可以把一致哈希算法是对 $2^{32}$ 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 $2^{32}$ 个点组成的圆，这个圆环被称为**哈希环**：

![image-20220625165804068](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165804068.png)

一致性哈希要进行两步哈希：

- 第一步：**对存储节点进行哈希计算**，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；
- 第二步：当对数据进行存储或访问时，**对数据进行哈希映射**；

所以，**一致性哈希是指将存储节点和数据都映射到一个首尾相连的哈希环上**，对数据哈希后得到的结果**按顺时针方向**找到第一个存储节点，然后进行读写。

举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置：

![image-20220625165920088](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165920088.png)

接着，对要查询的 key-01 进行哈希计算，确定此 key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该 key-01 数据的节点。

比如，下图中的 key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A：

![image-20220625165944314](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165944314.png)

所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址：

- 首先，对 key 进行哈希计算，确定此 key 在环上的位置；
- 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。

##### 3.3.5.2 节点的添加与删除

既然一致性 Hash 能够解决普通 Hash 增删机器导致移动数据过多的问题，那么我们就来看看它是否解决了这一问题。

假设节点数量从 3 减少到了 2，比如将节点 A 移除：

![image-20220625170103886](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625170103886.png)

可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。

假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置：

![image-20220625170029693](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625170029693.png)

可以看到，key-01、key-03 都不受影响，只有 key-02 需要被迁移节点 D。

因此，**在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响**。

##### 3.3.5.3 一致性 Hash 的问题

上面的简单的一致性 Hash 的方案在某些情况下但依旧存在问题：一个节点宕机之后，数据需要落到距离他最近的节点上，会导致下个节点的压力突然增大，可能导致**雪崩**，整个服务挂掉。

如下图所示：

![image-20220625152030031](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625152030031.png)

当节点 c3 摘除之后，之前在 c3 上的k1就要迁移到 c1 上，这时候带来了两部分的压力:

- 之前请求到 c3 上的流量转嫁到了 c1 上，会导致 c1 的流量增加，如果之前 c3 上存在热点数据，则可能导致 c1 扛不住压力挂掉。
- 之前存储到 c3 上的 key 值转义到了 c1，会导致 c1 的内容占用量增加，可能存在瓶颈。

当上面两个压力发生的时候，可能导致 c1 节点也宕机了。那么压力便会传递到 c2 上，又出现了类似滚雪球的情况，服务压力出现了雪崩，导致整个服务不可用。

除此之外，也可能出现节点在哈希空间中分布不平衡的问题：

![image-20220625152719687](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625152719687.png)

这种情况是由于服务器太少导致分配不均，被称作**倾斜**问题，倾斜问题如果发生了，更有可能导致雪崩问题（比如 c1 挂了）。

##### 3.3.5.4  改进——虚拟节点

改进的方法就是要让机器足够的多，从而让各个节点均匀的分布在 Hash 空间中，但问题是，实际中我们没有那么多节点。

虚拟节点（virtual node）是实际节点在 Hash 空间的复制品，一个实际节点对应了若干个虚拟节点，这个对应个数也成为复制个数，虚拟节点在 Hash 空间中以 Hash 值排列。

引入虚拟节点后，虚拟节点也像真实节点一样被散列在 Hash 环上，**如果某一个数据属于某一个虚拟节点，那么这个数据会直接存放到该虚拟节点所对应的真实节点上**。

假设有一些虚拟节点，它们和真实节点的对应关系如下：

* Visual 100—> Real 1，Visual 101—> Real 1
* Visual 200—> Real 2，Visual 201—> Real 2
* Visual 300—> Real 3，Visual 301—> Real 3

那么 Hash 之后，结果如下：

![image-20220625153605741](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220625153605741.png)