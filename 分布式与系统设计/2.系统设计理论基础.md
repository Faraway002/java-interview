# 系统设计理论基础

## 1. 概述

## 2. 高可用

### 2.1 集群

### 2.2 限流

### 2.3 服务降级 & 服务熔断

### 2.4 超时 & 重试机制

## 3. 高性能

### 3.1 读写分离

### 3.2 分库分表

### 3.3 负载均衡

#### 3.3.5 一致性 Hash

一致性 Hash 多用于分布式负载均衡中，对于分布式系统，不同机器上存储不同对象的数据，我们使用 Hash 函数建立从数据到服务器之间的映射关系。

哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 `hash(key) % 3` 公式对数据进行了映射。

如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点：

```scss
hash(key) % 3
```

如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。

假设我们有 10 个数据，哈希值分别为 1 ~ 10，则：

* 机器 0 上保存的数据有：3，6，9
* 机器 1 上保存的数据有：1，4，7，10
* 机器 2 上保存的数据有：2，5，8

但是有一个很致命的问题，**如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据**，否则会出现查询不到数据的问题。

要解决这个问题的办法，就需要我们进行**迁移数据**，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。

比如，当增加一台机器后，此时 n = 4，同样的 10 个数据，各个机器上存储的数据分别为：

* 机器 0 上保存的数据有：4，8
* 机器 1 上保存的数据有：1，5，9
* 机器 2 上保存的数据有：2，6，10
* 机器 3 上保存的数据有：3，7

这个例子中，我们看到，从 3 台机器增加到 4 台时，只有数据 1 和数据 2 没有移动。我们假设总数据条数为 M，哈希算法在面对节点数量变化时，**最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 $O(M)$**，这样数据的迁移成本太高了，**当集群中数据量很大时，采用一般的哈希函数，在节点数量动态变化的情况下会造成大量的数据迁移，导致网络通信压力的剧增，甚至导致数据库宕机**。

引入一致性 Hash 算法就可以解决此类问题，它可以保证当机器增加或者减少时，节点之间的数据迁移只限于两个节点之间，不会造成全局的网络问题。

##### 3.3.5.1 Hash 环

一致性哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而**一致哈希算法是对 $2^{32}$ 进行取模运算，是一个固定的值**。

我们可以把一致哈希算法是对 $2^{32}$ 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 $2^{32}$ 个点组成的圆，这个圆环被称为**哈希环**：

![image-20220625165804068](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165804068.png)

一致性哈希要进行两步哈希：

- 第一步：**对存储节点进行哈希计算**，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；
- 第二步：当对数据进行存储或访问时，**对数据进行哈希映射**；

所以，**一致性哈希是指将存储节点和数据都映射到一个首尾相连的哈希环上**，对数据哈希后得到的结果**按顺时针方向**找到第一个存储节点，然后进行读写。

举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置：

![image-20220625165920088](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165920088.png)

接着，对要查询的 key-01 进行哈希计算，确定此 key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该 key-01 数据的节点。

比如，下图中的 key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A：

![image-20220625165944314](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625165944314.png)

所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址：

- 首先，对 key 进行哈希计算，确定此 key 在环上的位置；
- 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。

##### 3.3.5.2 节点的添加与删除

既然一致性 Hash 能够解决普通 Hash 增删机器导致移动数据过多的问题，那么我们就来看看它是否解决了这一问题。

假设节点数量从 3 减少到了 2，比如将节点 A 移除：

![image-20220625170103886](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625170103886.png)

可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。

假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置：

![image-20220625170029693](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625170029693.png)

可以看到，key-01、key-03 都不受影响，只有 key-02 需要被迁移节点 D。

因此，**在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响**。

##### 3.3.5.3 一致性 Hash 的问题

上面的简单的一致性 Hash 的方案在某些情况下但依旧存在问题：一个节点宕机之后，数据需要落到距离他最近的节点上，会导致下个节点的压力突然增大，可能导致**雪崩**，整个服务挂掉。

如下图所示：

![image-20220625152030031](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625152030031.png)

当节点 c3 摘除之后，之前在 c3 上的k1就要迁移到 c1 上，这时候带来了两部分的压力:

- 之前请求到 c3 上的流量转嫁到了 c1 上，会导致 c1 的流量增加，如果之前 c3 上存在热点数据，则可能导致 c1 扛不住压力挂掉。
- 之前存储到 c3 上的 key 值转义到了 c1，会导致 c1 的内容占用量增加，可能存在瓶颈。

当上面两个压力发生的时候，可能导致 c1 节点也宕机了。那么压力便会传递到 c2 上，又出现了类似滚雪球的情况，服务压力出现了雪崩，导致整个服务不可用。

除此之外，也可能出现节点在哈希空间中分布不平衡的问题：

![image-20220625152719687](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625152719687.png)

这种情况是由于服务器太少导致分配不均，被称作**倾斜**问题，倾斜问题如果发生了，更有可能导致雪崩问题（比如 c1 挂了）。

##### 3.3.5.4  改进——虚拟节点

改进的方法就是要让机器足够的多，从而让各个节点均匀的分布在 Hash 空间中，但问题是，实际中我们没有那么多节点。

虚拟节点（virtual node）是实际节点在 Hash 空间的复制品，一个实际节点对应了若干个虚拟节点，这个对应个数也成为复制个数，虚拟节点在 Hash 空间中以 Hash 值排列。

引入虚拟节点后，虚拟节点也像真实节点一样被散列在 Hash 环上，**如果某一个数据属于某一个虚拟节点，那么这个数据会直接存放到该虚拟节点所对应的真实节点上**。

假设有一些虚拟节点，它们和真实节点的对应关系如下：

* Visual 100—> Real 1，Visual 101—> Real 1
* Visual 200—> Real 2，Visual 201—> Real 2
* Visual 300—> Real 3，Visual 301—> Real 3

那么 Hash 之后，结果如下：

![image-20220625153605741](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220625153605741.png)