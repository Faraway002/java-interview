# 计算机组成原理概述

计算机（computer），是现代一种用于高速计算的电子计算机器，可以进行数值计算，又可以进行逻辑计算，还具有存储记忆功能，是能够按照程序运行，自动、高速处理海量数据的现代化智能电子设备。

整个计算机系统分为两部分组成：

* 硬件系统
  * 主机：包括 CPU、主板、主存等。
  * 外设：包括输入设备、输出设备、外存等。
* 软件系统
  * 系统软件：包括操作系统、语言处理程序（编译器）、数据库管理系统等。
  * 应用软件：针对一般用户开发的应用，如 Web 浏览器，聊天软件等。

计算机的根本目的是帮助人类解决难以计算的计算性问题。

本章可以视作学习操作系统的前置知识。

## 冯诺依曼体系结构

在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出⽤电子元件构造计算机，并约定了**用二进制进行计算和存储**，还定义计算机基本结构为以下几个部分，分别是：运算器、控制器、存储器、输入设备、输出设备：

* 运算器和控制器：现代计算机中 CPU 包含了这两个部分。

  * **运算器负责数据的算术运算和逻辑运算，即数据的加工处理**。
  * **控制器负责对程序规定的控制信息进行分析，控制并协调输入，输出操作或内存访问**。

  由于它们本身不具备任何存储功能，为了保存结果，还需要**寄存器**的支持。

* 存储器：**实现记忆功能的部件，用来存放计算程序及参与运算的各种数据**。

  存储器一般分为主存和辅存，主存即我们常说的内存，其实还包括 CPU 内置的缓存和寄存器，以及主板上的 ROM（Read Only Memory）。

  辅存也叫外存，即外部存储设备，我们常见的硬盘、U 盘等都属于外存。

* 输入设备：**实现计算程序和原始数据的输入**。

  常见的输入设备有：键盘、鼠标、摄像头等等。

* 输出设备：**实现计算结果输出**。

  常见的输出设备有：显示器、音响、打印机等等。

现代计算机的结构如下图所示：

![image-20220117094847771](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220117094847771.png)

### 中央处理器（CPU）

**CPU 是计算机的核心，负责运算和操控其他硬件**。它的内部有一些组件，比如寄存器、控制器、运算器等等。这些组件负责不同的工作，而其中的控制器负责协调这些组件来完成 CPU 的工作。

CPU 的工作流程如下：

1. **取指令**。将一条指令从主存中取出到指令寄存器，有一个特殊的寄存器——程序计数器会指向取出的指令在主存中的位置，待该指令执行完毕之后，程序计数器会自己指向下一条指令所在的位置，然后 CPU 继续取这条指令。
2. **指令译码**。取出指令后，指令译码器按照预定的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别以及各种获取操作数的方法。
3. **执行指令**。具体实现指令的功能。
4. **访存取数**。根据指令需要访问主存、读取操作数，CPU 得到操作数在主存中的地址，并从主存中读取该操作数用于运算。部分指令不需要访问主存，则可以跳过该阶段。
5. **结果写回**。结果写回阶段把执行指令阶段的运行结果数据写回到某种存储形式。结果数据一般会被写到 CPU 的寄存器或缓存中，以便被后续的指令快速地存取。

6. 重复上述操作，直至所有指令执行完成。

CPU 内部有几个核心部件，其中的控制器和运算器我们已经初步接触过，下面介绍一下寄存器。

> **CPU 指令**
>
> CPU 并不是任何指令都能识别的，在 CPU 被设计完毕后，其能够识别的指令就已经被厂商所确定下来。假如使用 CPU 不支持的指令编程，那么 CPU 将无法识别这条指令。
>
> 现代的高级程序设计语言都要依靠编译器将高级语言翻译为 CPU 能够识别的指令，由于不同 CPU 架构指令很可能也不同，因此同一个程序在不同平台上编译的结果也不同，甚至不能被成功编译。
>
> 编译器也是一种程序，最基本的编译器是由汇编语言编写的，CPU 有内置的汇编语言到机器码的翻译器。

#### 寄存器

寄存器是 CPU 中很重要的一个组件，基本作用是存储数据，但是它又有很多种类，每种寄存器的作用也不尽相同。

以 8086 CPU 为例，常见的寄存器如下图所示：

![image-20220315145100825](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220315145100825.png)

1. 通用寄存器：通用寄存器的作用非常广泛，又分为好几种通用寄存器。

   1. 数据寄存器：用于传送和暂存数据，也可参与算术逻辑运算，并保存运算结果。

      8086 中，有 AX、BX、CX、DX 四种数据寄存器。

      * AX 称为累加器，更多的⽤来保存中间计算值和结果，同时也是负责传递系统调⽤号的。
      * BX 称为基地址寄存器，它可作为存储器指针来使⽤。
      * CX 称为计数寄存器，在循环和字符串操作时，要⽤它来控制循环次数。
      * DX 称为数据寄存器，在进行乘、除运算时，它可作为默认的操作数参与运算。

   2. 指针寄存器：和堆栈有关，存放堆栈内存储单元的偏移量。

      8086 中，有 SP 和 BP 两种指针寄存器。

   3. 变址寄存器：⽤于存放存储单元在段内的偏移量。

      8086 中，有 SI 和 DI 两种变址寄存器。

2. 段寄存器：用于内存分段的寄存器。

   内存一般会被分为四段：代码段、数据段、堆栈段以及附加段。其中每一段都有一个寄存器与之对应：

   * CS，代码段寄存器
   * DS，数据段寄存器
   * SS，堆栈段寄存器
   * ES，附加段寄存器

   其中很重要的是代码段寄存器，它将和接下来要介绍的 IP 寄存器一起完成程序计数器。

   > **程序计数器**
   >
   > 程序计数器（Program Counter）是存储 CPU 下一条应该执行的指令的寄存器。注意，PC 只是一个概念，而并非一个寄存器的名字，在不同的平台上有着不同的实现。
   >
   > 在 Intel X86 系列的 CPU 中，使用 CS + IP 两个寄存器一起完成 PC 的功能。

3. 控制寄存器：用于控制和确定处理器的操作模式以及当前执行任务的特性。

   * IP：指令指针寄存器，保存指令在代码段中的偏移量，而 CS 寄存器保存代码段的基地址，因此  CS + IP 寄存器就可以获取到下一条指令。
   * FLAG：标志寄存器，用于存放程序运行的状态信息，这些标志往往用作后续指令判断的依据。

### 存储器

存储器就是用来存储数据的，所有的程序（指令序列）和数据都需要被存储。

计算机中的任何数据都是二进制表示的，一个二进制数字称作 1 **位（bit）**，8 位是 1 **字节（Byte）**，1 KB（千字节）= $2 ^ {10}$ 字节 = 1024 字节，1 MB = 1024 KB，1 GB = 1024 MB，以此类推。

> **为什么是二进制？**
>
> 二进制是最容易被表示的，只要一种物质有两种形态，就可以表示二进制。在计算机的电路中，可以简单的使用低电压和高电压表示二进制，因此电路实现比较简单；而在存储器（如磁盘）中，可以通过磁极的取向，表面的凹凸来表示二进制，存储的实现也比较简单。

存储器分为主存和辅存，主存又包括 CPU 的寄存器、高速缓存、内存以及主板上的 ROM；我们常见的硬盘则属于辅存（外存）。

主存可以分为两大类，分别是只读存储器（ROM）和随机存取存储器（RAM）：

* 只读存储器，顾名思义，就是只能读取不能写入的存储器，它最大的特点就是能够长久保存数据，即使断电了也不会消失。

  在主板上的 ROM 存储着一个重要的程序 BIOS，这是由硬件厂商写好的程序，被内置在主板里，几乎不能修改它。

  BIOS 是 Basic Input Output System，它可以完成很多工作，其中最重要的一个就是加载存储在外存中的操作系统。

* 随机存取存储器，和 ROM 不同，它可以随时被读写，而且速度比较快，通常作为操作系统及其他程序的临时数据存储介质。但是它的一个最大的特点是：只要断电，它内部的数据都会被清零。

  RAM 一般采用线性排列，类似于编程语言中的数组，一般**一个单元就是一个字节**（注意不是 bit），采用线性排列的好处就是只需要知道某个数据在其中的位置，就能很快取到这个数据，因此叫做随机存取存储器。

最重要的主存就是内存，它是一种 RAM，容量介于 CPU 高速缓存以及外存之间，速度也介于这两者之间。因此被 CPU 选作临时存储数据的地方。

程序一般会保存在硬盘中（除了一些系统程序常驻内存中），然后被加载到内存中，CPU 再从内存中读取指令并执行。

### 总线

总线是连接各个部件的信息传输线，**是各个部件共享的传输介质，**是信号的公共传输线。

总线是有宽度的，总线的宽度定义为**一次能够传输多少位的数据**。

总线可分为 3 种：

* 地址总线，用于指定 CPU 将要操作的内存地址。
* 数据总线，用于读写内存的数据。
* 控制总线，⽤于发送和接收信号，比如中断、设备复位等信号。CPU 收到信号后进行响应，这时也需要控制总线。

地址总线通常和数据总线配合使用，地址总线用于**定位**，数据总线用于**传输**。因此，数据总线决定了 CPU 一次能操作的数据的大小（CPU 位宽），地址总线决定了 CPU 的寻址能力。

#### 位宽

**CPU 的位宽指的是 CPU 一次能够处理多大的数据**，在现代 CPU 中，常见的是 32 位和 64 位的 CPU，32 位意味着一次能处理 4 个字节大小的数，而 64 位则一次能处理 8 个字节，因此它们的指令长度也是不同的，64 位 CPU 的指令长度是 64 位，而 32 位的指令长度是 32 位的。这也意味着 **32 位的 CPU 上只能运行 32 位的软件，而 64 位的 CPU 上既可以运行 32 位的软件，也可以运行 64 位的软件**。

CPU 位宽越高，一次性可以计算的数值也就越大，性能也就越好。由于 CPU 存取数据主要是通过数据总线，因此在 CPU 的位宽确定的情况下，数据总线决定了 CPU 的数据处理能力。

#### 寻址能力

**寻址能力则和 CPU 的位宽没有直接关系**，因为寻址能力主要是看地址总线的宽度以及操作系统的位宽。

常见的地址总线一般是 32 位和 64 位的，如果是 32 位的，那么它能访问到的最大地址是 0xFFFFFFFF，也就是 4 GB 的内存空间（一个地址是一个字节，这里能表示 $2^{32}$ 个地址）；而如果是 64 位的，则是 $2 ^ {64}$ 个字节，也就是 16 EB，这个容量是非常大的，目前没有任何一个内存条能达到这个上限。

为什么说寻址能力还取决有操作系统呢？因为进程在访问地址时访问的其实是操作系统经过虚拟化后的逻辑地址，操作系统需要对逻辑地址和真实地址做一个转换。而**操作系统的位宽一般和 CPU 的位宽相同**，因此 32 位操作系统最大也就访问 4GB 的内存，如果你的内存是 8 GB 的，那么另外一半你可能永远都无法使用到。

### 输入/输出设备（I/O 设备）

**输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。**

常见的键盘，鼠标都属于 I/O 设备，它们连接到主机所用的接口叫做 I/O 接口。I/O 接口为了能够充当设备与计算机的桥梁，需要多个寄存器的协助。计算机可以通过寄存器，告诉设备要做什么，设备可以通过寄存器反映当前是否准备就绪。

输入输出设备一般是通过总线连接到计算机，CPU 通过总线可以直接控制 I/O 设备。使用这种方式，有两种方法可以获取设备中的数据：

* 轮询：CPU 不断询问 I/O 设备是否已经就绪。
* 中断：I/O 设备在就绪时或出现其他状况时向 CPU 发出中断信号，CPU 收到信号后会来处理数据。

有一种特殊的 DMA（Direct Memory Access）接口，表示直接地址访问，它通过 DMA 总线与主存直接相连，只要 CPU 告诉 DMA 接口要把数据存在主存中的哪个地址，DMA 接口就会根据地址把数据放进主存。

为了进一步解放 CPU，**通道**可以为 CPU 做一些基础操作，它像一个低级的 CPU，有自己的指令系统，能够执行一些有限的操作，当它收到 CPU 发出的 I/O 指令时，可以按照要求启动 I/O 设备，或者执行通道指令。

不同设备的 I/O 指令各不相同，如果你想要更好的使用这些设备，就需要安装其**驱动程序**。

## 存储器金字塔

计算机的存储器一般有：硬盘，内存，CPU 缓存和寄存器。其中，硬盘的空间最大，可以保存很多文件和数据；内存次之；CPU 缓存再次；寄存器是最小的。访问速率则相反，CPU 访问寄存器是最快的，CPU 缓存次之，内存再次，硬盘最慢。

这是由于**空间的局部性原理**，CPU 访问离它最近的东西是最快的，距离越远越慢。

**对于存储器，它的速度越快、能耗会越高、而且材料的成本也是越贵的，以至于速度快的存储器的容量都比较小。**

因此，我们可以得到一张存储器金字塔：

![image-20220117120037579](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220117120037579.png)

越往上存储量越少，但是访问越快，价格也越昂贵。

**CPU 在访问某一个数据时，它是按照金字塔的顺序进行访问的**，如果在某一级没有找到，就会往下继续寻找该数据。

因此，如果一个数据没有被**缓存**在高速的设备中，那么 CPU 就要花更多的时间访问低速的设备，这会极大的拖慢程序的运行速度。

> **为什么要这样设计存储器体系？**
>
> 首先是**成本**的原因，由于高性能的存储器需要高成本，因此如果一个 CPU 的寄存器（缓存）较大，那么它的价格会很昂贵。
>
> 其次是 **2/8 原则**，一个程序中，只有 20% 是核心代码，影响着整个程序的性能，而剩下的 80% 虽然占比高，但是却很少影响程序的性能。我们只需要把这 20% 核心数据存储到高速的设备中，整个程序的性能就能得到提高。
>
> 最后是**局部性原理**，局部性原理有时间局部性和空间局部性，时间局部性的是最近访问过的数据很可能在不久的将来会再次访问；空间局部性指的是最近访问过的数据的临近数据也有很大可能会被访问到。基于局部性原理，我们可以让少部分数据处于更高速的存储器中，其他数据放在稍微慢一点的存储器中。这样就形成了一个**缓存体系**。
>
> 缓存体系在普通的应用程序中也是很常见的：
>
> ![image-20220117121033500](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220117121033500.png)

### 存储器的物理结构

* 寄存器通常只有很少的几十个，访问速度最快，造价也最高，但是容量也最小。

  通常**访问寄存器只需要半个时钟周期即可完成**（纳秒级别）。

* CPU 缓存，一般有多个级别，级别越高容量越大，离 CPU 越远，速度越慢，价格也越低。

  一般来说，多核的 CPU 每一个核心都自己持有一个一级缓存和二级缓存，但是三级缓存是多个核心共享的。如下图所示：

  一级缓存通常又分为指令缓存和数据缓存。

  ![image-20220117115241296](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220117115241296.png)

  CPU 缓存通常使用 SRAM（Static Random Access Memory）芯片，它的特点是通电以后数据能一直存在，断电以后则不能存在。

  **一级缓存的访问时间通常在 2~4 个时钟周期之间，二级则需要 10~20 个时钟周期，三级需要 20~60 个时钟周期。**

* 内存使用了 DRAM（Dynamic Random Access Memory）芯片，相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。

  **DRAM 的访问时间通常在 200~300 个时钟周期之间。**

  >**DRAM 为什么比 SRAM 慢**
  >
  >在 SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度非常快。
  >
  >而 DRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要定时刷新电容，才能保证数据不会被丢失。因为这样，它的电路也十分复杂，而且要做更多的工作，所以更慢。

* 硬盘，通常分为固态硬盘（Solid State Disk，SSD）和机械硬盘（Hard Disk Drive）。固态硬盘的访问时间大约是访问内存的 10~1000 倍，而机械硬盘的访问时间是访问内存的 100000 倍。

  固态硬盘的存储数据方式和机械硬盘很不一样，这也导致了固态硬盘访问快，但是造价高。我们在之后还会详细介绍固态和机械硬盘。

## 利用硬件特性写出高性能的代码

代码都是由 CPU 跑起来的，我们代码写的好与坏就决定了 CPU 的执行效率，特别是编写计算密集型的程序，更要注重 CPU 的执行效率，否则将会⼤⼤影响系统性能。

由于程序的规模很大，想要用寄存器或 CPU 缓存完全存储一个程序是不可能的，因此只能存储在低级别的存储器中。如果我们根据之前介绍过的缓存体系，就可以**充分利用 CPU 的缓存，来编写高性能的程序**。

### 理解 CPU 缓存

CPU 缓存是从内存中读取数据的，它保存的内容只是内存的一个子集。CPU 缓存以一个缓存块（Cache Line）为单位和内存进行数据交换，并且交换的数据通常是内存中的一个连续的块。

在 Linux 操作系统中，一个 Cache Line 的大小通常为 64 字节，这意味着如果内存中有一个长度为 100 的 `int` 型数组 arr，那么一次能够被加载到 L1 中的元素只有 16 个，比如 arr[0] ~ arr[15]。

这样做的好处就是，由于时间局部性，当你读取 arr[0] 时，可能你很快就要读取 arr[1]，这时 CPU 直接从 L1 中取出该数据，而不必花费更多时间去内存中寻找。

CPU 总是先在缓存中寻找想要的数据，如果没找到（未命中），才会从内存中取；否则（命中），就可以直接读取。

那么，CPU 如何知道想要的数据在不在缓存中？如果在的话，如何找到缓存中对应的数据呢？这就不得不提到 CPU 的缓存**映射机制**。

#### CPU 缓存映射机制

##### 直接映射

**直接映射的思想是：根据 Cache Line 的大小给内存分块，然后编号，使用取模运算把结果相同的块总是放到同一个 Cache Line 中。**

假设内存为 2 MB，CPU 有 256 KB 的 L1 Cache，Cache Line 的大小为 64 KB，则内存共可以分为 32 块，编号为 0 ~ 31，而 CPU 共有 4 个 Cache Line。

如果我们想要取得内存中第 15 个块的位置，则一定先去找第四个 Cache Line 而不会去找其他的 Cache Line。因为 0、1、2、3 块的模 4 的余数分别为 0、1、2、3，则 3、7、11、15 块一定存在第四个 Cache Line 中，这样取余得到的结果称为**索引（Index）**。

那么就产生了一个问题，既然  3、7、11、15 块都会映射到第四个 Cache Line，那么如何辨别它们？为此，地址中需要有一部分作为**标记（Tag）**，便于识别。

CPU 想要取得的数据可能是一整个缓存块或是缓存块中的一部分，这时就需要知道这个缓存块的中该数据位于该缓存块的**偏移（Offset）**。

因此，**一个内存的访问地址会被视为有三个部分：tag + index + offset**。

CPU 的缓存中也需要编址，该地址也有三个部分：有效位（valid bit）+ tag + 实际数据（data）。有效位决定了该块是否有效，若无效则直接访问内存；tag 就是访问地址中的 tag，用于识别内存块，如果 tag 相同，则缓存未命中；实际数据就是缓存的内存块，由访问地址中的 offset 决定读取哪一部分。

图示如下：

![image-20220119103435142](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220119103435142.png)

* 优点：映射简单，访问时只需要检查 index。

* 缺点：因为有多个内存块映射到同一 Cache Line，块的替换会比较频繁。

##### 全相联映射

在理解了直接映射的方式后，全相联映射就变得很简单。**全相联映射没有取模运算，任一内存块可以被放置在任一缓存块中**。

这时，内存地址就被分为 Tag + Offset，需要对整个 CPU Cache 进行搜索找到对应的 Tag，一般会引入一个映射表加快搜索。

当一个新块要被放入满的缓存中，就需要决定替换掉哪一块，这就是之后的块替换算法的问题了。

##### 组相联映射

对内存和 CPU 缓存都进行分组，每个组的大小（含有的块的数目）称为**路**，称为 x 路组相联映射。**分组之后，对每个组进行直接映射**。

这样就解决了有多个内存块映射到同一缓存块，块的替换会比较频繁的问题。因为是以组为单位进行替换，组里有不止一个块，所以命中率会提高，但是地址需要有更多的标记来定位块。

当只有一组时，就是全相联映射；当每个组的大小为 1 时，就是直接映射。

### 如何写出高性能的代码

理解 CPU 缓存后，我们的问题就转变为“如何写出 CPU 缓存命中率高的代码？”。

CPU 的一级缓存分为数据缓存和指令缓存，因此我们只要对这两个方面都提高命中率，即可提高整体的命中率。

#### 提高数据的命中率

以遍历二维数组为例：

![image-20220119105632300](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220119105632300.png)

经过测试，形式一的执行速度⽐形式⼆快好⼏倍。

这是因为**二维数组在内存中的存放方式就是平铺后按顺序存放，因此按照行列的顺序访问就是按照内存中存放的顺序访问**。这是遵循时间局部性的，一次会有多个连续的数组元素被缓存，缓存不命中的次数少。

而以列行的顺序访问，很明显是跳着访问的，缓存不命中的次数就多了。

**我们编写的代码要尽量遵守时间局部性和空间局部性**。

#### 提高操作指令的命中率

假设有一个全是随机数，长度为 100 的数组，现在要对它进行一个遍历 + 排序的操作，试问**先遍历再排序**所花的时间少还是**先排序再遍历**所花的时间少？

在回答这个问题之前，我们先了解 CPU 的分⽀预测器：如果存在 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，如果分⽀预测器可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以**提前**把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快。

在这个例子中，由于数组元素完全随机，因此直接遍历的话，分支预测器是不工作的；而排序之后，分⽀预测器动态地根据历史命中数据对未来进行预测，这样命中率就会很高。因此，先排序再遍历的速度会更快。

**想办法让 CPU 的分支预测工作，也能够很好的提高代码的性能**。在 C/C++ 语言中，存在 `likely` 和 `unlikely` 宏，如果你确信执行某一条分支的概率更大，你可以使用 `likely` 宏；反之，使用 `unlikely` 宏。

#### 提高多核 CPU 的缓存命中率

虽然 L3 缓存是多核共享，但是 L1 和 L2 缓存是每个核心独有，**如果一个进程在不同核心来回切换，各个核心的缓存命中率就会受到影响**。

当有多个同时执行的**计算密集型**的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把线程绑定在某一个 CPU 核心上，这样性能可以得到非常可观的提升。

在 Linux 上，就提供了 `sched_setaffinity` 系统调用来将某一个线程绑定在某个 CPU 的核心上。

## CPU 缓存一致性

当缓存命中，计算完成时，CPU 会将变化的结果写回到缓存中，这时就会出现缓存和内存数据不一致的问题。而 CPU 访问内存的代价是比较高的，这时就涉及到把数据写回到内存的时机的问题。

为了应对这个问题，下面介绍两种针对写入数据的方法：

* **写直达**（Write Through）：直接把数据同时写回到内存和缓存中。

  写直达法很直观，也很简单，但是问题也很明显：⽆论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费⼤量的时间，性能会受到很⼤的影响。

* **写回**（Write Back）：当发生写操作时，写回时仅仅写回到 Cache 中，**只有当修改过的 Cache Line 被替换时，才会写回到内存中**。

  这样做的好处就是减少了写回内存的次数，性能得到了提升。

写回具体的实现如下：

* 当发生写操作时，如果数据原本已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个缓存块为 Dirty，这个脏标记意味着内存和缓存中数据不一致，但是不会立即写回到内存中。

* 如果数据所对应的缓存块里存放的是其他数据，则这个其他数据要被替换，首先要检查这个即将被替换的缓存块里的数据是否为 Dirty，如果是 Dirty，则写回到内存中。然后再把当前要写入的数据写入到这个缓存块中，同时也把它标记为脏的。

  如果缓存块里面的数据没有被标记为 Dirty，则就直接将数据写入到这个缓存块里，然后再把这个缓存块标记为 Dirty 就完成了。

### 多核 CPU 的缓存一致性

多核 CPU 只有 L3 是共享的，而 L1 和 L2 是独享的，这就会带来多核的缓存一致性问题。

举个例子，假设 A, B 两个核心运行两个线程，操作一个共享变量 i，其初始值为 0，示意图如下图所示：

![image-20220316105910521](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220316105910521.png)

假设 A 核心先执行 i++，使用写回策略，那么 A 的缓存 i 的值是 1，而内存中 i 的值是 0；此时如果 B 核心读取 i，则读取到的将是 0，和 A 中的变量值不一致。如下图所示：

![2](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/2.png)

要解决这一问题，需要一种同步机制，能够同步数据，这个机制至少要满足：

1. 某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，称为**写传播**。

2. 某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，称为**事务的串形化**。

   事务的串形化比较难理解，看下面的示例：

   A, B 同时修改 i 的值：

   ![image-20220316110320000](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220316110320000.png)

   由于写传播机制，C 和 D 最后都能收到修改后的值，但是传播接收的顺序可能不一样，C 核心可能先变为 100，再变为 200；D 核心可能先变为 200，再变为 100。

   我们要保证这个 C 和 D 看到的值一样，就要让并行的操作变为串行，并且保证串行的操作变为原子性的，则需要引入**锁**：如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到锁的核心才能进行更新。

下面介绍几种机制来实现写传播和事务串形化。

#### 总线嗅探

**写传播**的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是**总线嗅探（Bus Snooping）**。

我还是以前面的 i 变量例子来说明总线嗅探的工作机制，当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，**通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件**，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache。

可以发现，总线嗅探方法很简单， CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。

另外，总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是**并不能保证事务串形化**。

于是，有一个协议基于总线嗅探机制实现了事务串形化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。

#### MESI 协议

MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：

- Modified（已修改）：Modified 状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。
- Exclusive（独占）：Exclusive 表示数据只存在于一个 CPU 的核心里，其他 CPU 的 Cache 中不存在该数据。
- Shared（共享）：Shared 表示数据存在多个 CPU 的核心里。
- Invalidated（已失效）：Invalidated 状态表示的是这个 Cache Block 里的数据已经失效了，此时不可以读取该状态的数据。此状态只会在 Shared 和 Modified 之后出现。

**MESI 协议使用这四个状态来标记 Cache Line 四个不同的状态**。

Exclusive 和 Shared 状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。

Exclusive 有可能变为 Shared，当其他核心从内存中读取相同的数据到自己的 Cache 中时，就会变为 Shared。

以上面的例子来讲解这个协议是如何工作的：

1. 当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为 Exclusive，此时其 Cache 中的数据与内存是一致的。
2. 然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成 Shared，并且其 Cache 中的数据与内存也是一致的；
3. 当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，**要求先把其他核心的 Cache 中对应的 Cache Line 标记为 Invalidated 状态**，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为 Modified 状态，此时 Cache 中的数据就与内存不一致了。
4. 如果 A 号 CPU 核心继续修改 Cache 中 i 变量的值，由于此时的 Cache Line 是 Modified 状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。
5. 如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被替换，发现 Cache Line 状态是 Modified 状态，就会在替换前先把数据同步到内存。

**MESI 不仅仅保证了 CPU 缓存一致性，还将对数据内存的写回延迟到不能再延迟为止**。

整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件。

下图是 MESI 协议的状态图：

![3](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/3.png)

### 伪共享

伪共享指的是**物理地址连续的多个数据被缓存在同一个 Cache Line 中，而 CPU 的多个核心分别读写这些数据当中的不同的数据，由于 MESI 协议，导致 Cache Line 不断失效、更新、失效、更新，等价于没有缓存**。

比如有 AB 两个数据存放在物理地址连续的位置：

![image-20220323105635539](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323105635539.png)

AB 恰好都会被缓存到 Cache Line 中，现在假设 CPU 核心 1 读取了 A，此时 AB 都会被放在 Cache Line 中：

![image-20220323105722050](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323105722050.png)

现在 CPU 核心 2 要读取 B，则也是同样的过程：

![image-20220323105859323](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323105859323.png)

但是由于 MESI 协议针对的是 Cache Line，因此 Cache Line 会被标记为 Shared。此时如果核心 1 修改 A，就会导致核心 2 中的缓存失效：

![image-20220323110011239](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323110011239.png)

之后，核心 2 修改 B，在此之前，CPU 1 要把数据写回内存，核心 2 重新读取，然后修改，此时核心 1 的 Cache Line 又失效了：

![image-20220323110118229](https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323110118229.png)

如此反复，你会返现 CPU 的缓存根本没有起到作用。

#### 避免伪共享的方式

要避免伪共享，就要使得这些连续的数据不能放在同一个 Cache Line 中，即：

<img src="https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323110308779.png" alt="image-20220323110308779" style="zoom:50%;" />

要变为：

<img src="https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323110328070.png" alt="image-20220323110328070" style="zoom:50%;" />

在 Linux 系统上，有一个宏定义来解决这个问题：

```c
#ifdef CONFIG_SMP
#define __cacheline_aligned_in_smp __cacheline_aligned
#else
#define __cacheline_aligned_in_smp
#endif
```

注意，这个宏只存在于多核 CPU 的系统上，如果是单核，则宏定义为空。

这种思想是典型的空间换时间，在实际项目中也是经常使用的。

比如一个 Java 的并发框架 Disruptor，它在类中填充了许多无效数据，来避免数据被缓存在同一个 Cache Line 中：

<img src="https://fastly.jsdelivr.net/gh/Faraway002/typora/images/image-20220323110805071.png" alt="image-20220323110805071" style="zoom:80%;" />

一般 64 位 CPU 的 CPU Line 的大小是 64 个字节，一个 long 类型的数据是 8 个字节，所以 CPU 一下会加载 8 个 long 类型的数据。而在源代码中，我们看到有 7 个 `long` 型变量用于填充，从而保证真正要缓存的数据位于单独一个 Cache Line 中。

## 操作系统的加载过程

操作系统是一个特殊的程序，它和普通程序一样平常被放在硬盘中，需要被加载到内存中来执行。

整体流程如下：

1. CPU 加电/复位。
2. 执行 BIOS 自检
3. 系统引导
4. 启动内核
5. 内核初始化

### 实模式和保护模式

实模式和保护模式指的是 CPU 的两种地址寻址方式。

**计算机刚加电时处于实模式下**，程序按照 Intel 8086 的**直接寻址**方式，只能访问 0x00000 ~ 0xFFFFF 的 1 MB 空间。

**操作系统内核启动成功后处于保护模式**，对非法的地址访问会进行检测，寻址范围也大大提升。

在之后的内存管理章节中还会涉及到这两个模式。

### BIOS 自检

BIOS 是固化到 ROM 存储器上的，储存的是 BIOS 代码，即计算机最重要的**基本输入输出**的程序、开机后自检程序和系统自启动程序。

CPU加电之后，会把 CPU 所有寄存器的值设为默认值，除了 CS 寄存器的值改为 0xFFFF，其他寄存器的值都为 0，这样根据 CS 和 IP 的值就可以找到指令的物理地址 0xFFFF:0x0000，也就是0xFFFF0。

这时 CPU 就开始从默认地址的位置开始执行，这里存放了一条无条件跳转指令 JMP，跳转到 BIOS 的真正启动代码处。

BIOS 执行分两个步骤：

1. POST（Power-On Self Test，加电自检）程序是执行的第一个例行程序，主要是对 **CPU、内存、显卡、外设等硬件设备**（包括中断向量，设置寄存器）进行检测和初始化。

   **BIOS 自检阶段如果报错，系统就无法正常启动起来**。若正常启动，则 BIOS 开始从显卡代码存放地址加载显卡，并在屏幕上显示最初信息。

2. 引导程序执行一段小程序用来**枚举本地设备并对其初始化**。

   以硬盘启动为例，BIOS 此时去读取硬盘驱动器的第一个扇区（MBR，512字节），然后执行里面的代码。实际上这里 BIOS 并不关心启动设备第一个扇区中是什么内容，它只是负责读取该扇区内容、并执行。

至此，BIOS 的任务就完成了，此后将系统启动的控制权移交到 MBR 部分的代码。

### 系统引导

MBR 是 Master Boot Record 的缩写，也就是所谓的主引导扇区，它是硬盘的第一个扇区，由三个部分组成：主引导程序（Bootloader）、 硬盘分区表 DPT（Disk Partition table）和硬盘有效标志。

**主引导程序的核心工作是将内核从硬盘加载到内存中，并将其放在合适的位置中**。该过程中，启动引导程序要完成初始引导、内核初始化、全系统初始化。

以 grub 为例，grub 的全称是 GRand Unified Bootloader，一个 GNU 项目的多操作系统引导程序，用于在有多个操作系统的机器上，在刚开机的时候选择一个操作系统进行引导。

BIOS 加载 MBR 中的 grub 代码后就把 CPU 交给了 grub，grub 的工作就是一步一步的加载自身代码，从而识别文件系统，然后就能够将文件系统中的内核镜像文件加载到内存中，并将 CPU 控制权转交给操作系统内核。

### 启动内核

当控制权从 Bootloader 转到内核时，内核立即初始化系统中各设备并做相关的配置工作，其中包括 CPU、I/O、存储设备等。

### 初始化系统

内核启动，并进行完硬件检测与驱动程序加载之后，主机的硬件已经准备就绪了，这时内核会启动第一个程序，就是 systemd（Linux 下的初始化软件）。

systemd 最主要的功能就是准备软件执行的环境，包括系统的主机名称，网络，语言，档案格式以及其他服务的启动等。

> **systemd 流程**
>
> 1. 执行系统初始化脚本（`/etc/rc.d/rc.sysinit`），对系统进行基本的配置，以读写方式挂载根文件系统及其它文件系统。当脚本执行完后，系统就可以顺利工作了，只是还需要启动系统所需要的各种服务，这样主机才可以提供相关的网络和主机功能，因此便会执行下面的脚本
> 2. 执行 `/etc/rc.d/rc` 脚本。该文件定义了服务启动的顺序是先 K 后 S，rc.sysinit 通过分析 /etc/inittab 文件来确定系统的启动级别，然后才去执行 /etc/rc.d/rc*.d 下的文件
> 3. 执行用户自定义引导程序 `/etc/rc.d/rc.local`。一般来说，自定义的程序只需要将命令放在 rc.local 里面就可以了，这个 shell 脚本就是保留给用户自定义启动内容的；
> 4. 完成了系统所有的启动任务后，Linux 会启动终端或 X-Window 来等待用户登录。

当它执行完毕以后，整个操作系统就启动完成了。
