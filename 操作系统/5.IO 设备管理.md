[TOC]

# I/O 设备管理

除了提供抽象（例如，进程（和线程）、地址空间和文件）以外，操作系统还要控制计算机的所有 I/O（输入/输出）设备。操作系统必须向设备发送命令，捕捉中断，并处理设备的各种错误。它还应该在设备和系统的其他部分之间提供简单且易于使用的接口。

## 1. I/O 原理

不同的人对于 I/O 硬件的理解是不同的。对于电子工程师而言，I/O 硬件就是芯片、导线、电源、电机和其他组成硬件的物理部件。对程序员而言，则只注意 I/O 硬件提供给软件的接口，如硬件能够接收的命令、它能够完成的功能以及它能够报告的错误。

### 1.1 I/O 硬件设备

I/O 设备大致可以分为两类：块设备（block device）和字符设备（character device）。

* 块设备：**块设备把数据存储在固定大小的块中，每个块有自己的地址，且能独立于其它块进行读写**。硬盘、U 盘等都是最常见的块设备。
* 字符设备：**字符设备以字符为单位发送或接收一个字符流，而不考虑任何块结构**。打印机、网络接口、鼠标等与硬盘类型不同的大部分设备都能看作字符设备。

除上述分类之外，还有一些特殊的设备，比如时钟，它既不是块设备，也不是字符设备，但是**块设备和字符设备的抽象具有足够的一般性**。

这些 I/O 设备的速度有快有慢，快的能够达到几百 MB/S，而慢的设备只有几个字节/秒，要使软件在跨越这么多数量级的数据率下保证性能优良，给软件造成了相当大的压力。

I/O 设备一般由机械部件和电子部件两部分组成。通常可以将这两部分分开处理，以提供更加模块化和更加通用的设计。

**电子部件称作设备控制器（device controller）或适配器（adapter）**，而机械部件则是设备本身，我们重点关注设备控制器。

#### 1.1.1 设备控制器

设备控制器的示意图如下：

![image-20220325105806819](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220325105806819.png)

我们看到，设备控制器充当设备和操作系统之间的中间人的角色，控制器与设备本身的接口通常是很低级的，**操作系统并不涉及这些低级的接口，而是直接通过控制器来操作 I/O 设备**。

控制器内部是有芯片的，也有自己的寄存器，用来与 CPU 通信，**通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作**；**通过读取这些寄存器，操作系统可以了解设备的状态**，比如是否准备好接收一个新的命令等。

通常来说，控制器内部有三类寄存器：

- **数据寄存器**：CPU 向 I/O 设备写入**需要传输的数据**，比如要打印的内容是 `Hello`，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- **命令寄存器**：CPU 发送一个**命令**，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- **状态寄存器**：目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

![image-20220704160450878](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704160450878.png)

除此之外，块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**，这是为了减少对设备的频繁操作。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。

### 1.3 与 I/O 设备的通信方式

CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

- **端口 I/O**：每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器。

  I/O 端口是一个 8 位或 16 位的整数，所有的 I/O 端口形成 I/O 端口空间，并且受到保护使得普通的用户程序不能对其进行访问（只有 OS 能够进行访问）。

  这一方案的最大缺点就在于**进程地址空间和设备 I/O 地址空间是完全不同的**。如下图所示：

  ![image-20220325111809823](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220325111809823.png)

- **内存映射 I/O**：将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。

  每个控制寄存器被分配唯一的一个内存地址，并且不会有其他内存分配这一地址。通常，被分配给寄存器的地址位于地址空间的顶端。

更一般的情况是两种方式混用，这一方案具有内存映射 I/O 的数据缓冲区，而控制寄存器则具有单独的 I/O 端口。

### 1.4 I/O 控制方式

在前面的学习中，我们知道每种设备都有一个设备控制器，控制器相当于一个小 CPU，它可以自己处理一些事情，但有个问题是，当 CPU 给设备发送了一个指令，让设备控制器去读设备的数据，它读完的时候，要怎么通知 CPU 呢？

#### 1.4.1  程序控制 I/O

第一种方式也是最简单的方式：**CPU 发出指令以后，不断询问设备寄存器的状态，直到完成一条指令为止**。这种方式简单，但是会占用 CPU 的全部时间，也叫做轮询方式。

使用伪代码可以描述这个过程：

```c
copy_from_user(buffer, p, count);

for (int i = 0; i < count; ++i) {
    while (*reg_status != READY);
    
    *reg_data = p[i];
}
```

在一些简单的嵌入式系统中，CPU 无其他事可做，这种方式自然是合理的，但是**在复杂的系统中，CPU 有大量其他工作要做，则这种方式是十分低效的，需要有更好的手段，所以操作系统一般不使用这种方式控制 I/O**。

#### 1.4.2 中断驱动 I/O

我们已经介绍过中断，事实上，中断最初被发明出来就是为了 I/O 时解放 CPU，**当设备完成任务后，控制器发出一个中断，CPU 则会收到中断，然后处理这个中断**。

中断一定程度上解决了占用 CPU 的问题，但是**对于磁盘这种频繁读写的设备来说**，CPU 是很容易被中断的，中断就意味着要为程序的上下文**保存现场**，处理完毕以后要**恢复**，这个**开销是比较大的**。

#### 1.4.3 使用 DMA 的 I/O

对于上面描述的 I/O 频繁的设备来说，DMA（Direct Memory Access）的方式更适合它们。

DMA 是一类特殊的设备控制器，**它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存**。

以磁盘为例，DMA 的工作方式如下：

- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的位置就可以了。
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存。
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器。
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了。

示意图如下：

![image-20220328145719554](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328145719554.png)

可以看到，CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。

**使用 DMA 的 I/O 仅仅在传送开始和结束时需要 CPU 干预**。

### 1.5 I/O 软件层次

I/O 软件通常组织成如下层次：

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704162310821.png" alt="image-20220704162310821" style="zoom:67%;" />

其中，文件系统相关我们已经学习过了，这里就不再赘述。

接下来，我们从底到上再介绍两个比较重要的层次。

#### 1.5.1 设备驱动程序

虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽设备控制器的差异，引入了**设备驱动程序**。

示意图如下：

![image-20220328152551175](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152551175.png)

设备控制器不属于操作系统范畴，它是属于硬件，而**设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口**，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。

不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。如下图：

![image-20220328152706950](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152706950.png)

操作系统实际会在设备驱动程序里响应设备发出的中断请求，通常，设备驱动程序初始化的时候，要先注册一个该设备的中断服务例程。

![image-20220328152730849](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152730849.png)

#### 1.5.2 通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

通用块层是处于**文件系统**和**块设备驱动程序**中间的一个块设备抽象层，它主要有两个功能：

- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 支持 5 种 I/O 调度算法，分别是：

- 没有调度算法：它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。
- 先入先出调度算法：先进入 I/O 调度队列的 I/O 请求先发生
- 完全公平调度算法：大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。
- 优先级调度：优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。
- 最终期限调度算法：该算法分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。

#### 1.5.3 缓存层

存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

- 为了提高文件访问的效率，会使用**页缓存、inode 缓存、dentry 缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
- 为了提高块设备的访问效率，会使用**缓冲区**，来缓存块设备的数据。

### 1.6 I/O 方式

### 1.7 完整的 I/O 流程

软硬件都介绍完毕之后，我们来讲述一下完整的 I/O 流程。

使用中断驱动的 I/O 流程如下：

![image-20220704162936477](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704162936477.png)

而使用 DMA 的 I/O 流程如下：

![image-20220704163049691](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704163049691.png)

我们看到，无论是何种流程，它都有两个关键的系统调用：

```c
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

代码很简单，虽然就两行代码，但是这里面发生了不少的事情：

![image-20220704165941087](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704165941087.png)

* 首先，上述过程期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。

  上下文切换的成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。

* 其次，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的。

  这个过程如下：

  - 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
  - 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
  - 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
  - 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。

如果想要提高 I/O 的性能，就必须**减少上下文切换的次数**以及**减少拷贝数据的次数**。

#### 1.7.1 I/O 流程优化

##### 1.7.1.1 减少用户态与内核态的上下文切换的次数

每一次系统调用，从调用到返回都是必须要经过两次上下文切换的，首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。

所以，**要想减少上下文切换到次数，就要减少系统调用的次数**。

##### 1.7.1.2 减少数据拷贝的次数

传统的 I/O 流程会历经 4 次数据拷贝，但是实际上，**从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里，这个过程是没有必要的**。

因为**文件传输**的应用场景中，在用户空间我们并不会对数据再加工，所以数据实际上可以不用搬运到用户空间，因此**用户的缓冲区是没有必要存在的**。

##### 1.7.1.3 零拷贝

**零拷贝（Zero-copy）就是一种能提高 I/O 性能的技术，零拷贝可以做到不在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的**。

它的实现方式通常有两种：

- `mmap + write`：`mmap` 系统调用函数会直接把内核缓冲区里的数据**映射**到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

  示意图如下：

  ![image-20220704171946734](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704171946734.png)

  我们可以得知，通过使用 `mmap()` 来代替 `read()`， 可以减少一次数据拷贝的过程。

  但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。

- `sendfile`：`sendfile` 是用于代替 `read` 和 `write` 的新的系统调用，使用它可以**减少两次上下文切换**，而且它还能直接把数据从内核中的源缓冲区拷贝到目的缓冲区中，而不需要经过用户态的切换。

  `sendfile` 签名如下：

  ```c
  ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
  ```

  它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。

  ![image-20220328155134350](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328155134350.png)

我们发现，`sendfile` 相比于 `mmap` 来说，少了一次系统调用，但是拷贝次数还是三次。

那么有没有办法，能够让数据从内核缓冲区直接移到 socket 缓冲区呢？答案是有，如果网卡支持 SG-DMA 技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

>**如何检测你的服务器是否支持 SG-DMA？**
>
>你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持该特性：
>
>```bash
>$ ethtool -k eth0 | grep scatter-gather
>scatter-gather: on
>```

对于支持网卡支持 SG-DMA 技术的情况下， `sendfile` 系统调用的过程发生了点变化，具体过程如下：

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

所以，这个过程之中，只进行了 2 次数据拷贝，如下图：

![image-20220704173034118](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704173034118.png)

## 2. I/O 模型

I/O 模型可以表示应用程序中 I/O 的处理方式，和前面提到的 I/O 流程没有直接关系，因为任何一种模型都需要经历 I/O 流程。

传统的 I/O 模型有 5 种：

- 同步阻塞 I/O
- 同步非阻塞 I/O
- I/O 多路复用
- 信号驱动 I/O
- 异步 I/O

可以从以下两个维度来理解 I/O 模型：

- 同步或异步（synchronous / asynchronous）。简单来说，同步是一种可靠的有序运行机制，当我们进行同步操作时，后续的任务是等待当前调用返回，才会进行下一步；而异步则相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。
- 阻塞与非阻塞（blocking / non-blocking）。在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 ServerSocket 新连接建立完毕，或数据读取、写入操作完成；而非阻塞则是不管 I/O 操作是否结束，直接返回，相应操作在后台继续处理。

> 不能一概而论认为同步或阻塞就是低效，具体还要看应用和系统特征。

下面，我们以网络 I/O 为例，讲述 I/O 模型。

### 2.1 同步阻塞 I/O（Blocking I/O)

服务器接收一个网络请求后，开启一个线程，发起 `read` 调用后，线程就阻塞了，让出 CPU。

内核准备好数据后，再把线程叫醒。

![image-20220704194111383](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704194111383.png)

* 优点：程序简单，在阻塞等待数据期间进程/线程挂起，基本不会占用 CPU 资源；可以多线程处理多个请求。

* 缺点：每个请求需要独立的进程/线程单独处理，当并发请求量大时，内存占用很大，线程的切换过于频繁导致开销大，这种模型在实际生产中很少使用。

### 2.2 同步非阻塞 I/O（Non-Blocking I/O）

服务器接收一个网络请求后，开启一个线程，线程不断的发起 `read` 调用，数据没到内核空间时，每次都返回失败，直到数据到了内核空间。这一次 `read` 调用后，在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的，等数据到了用户空间再把线程叫醒。

![image-20220704194046033](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704194046033.png)

* 优点：不会阻塞在内核的等待数据过程，每次发起的 I/O 请求可以立即返回，不用阻塞等待，实时性较好。
* 缺点：轮询将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低，所以一般 Web 服务器不使用这种 I/O 模型。

### 2.3 I/O 多路复用（I/O Multiplexing）

线程的读取操作分成两步：

1. 线程先发起 `select/poll/epoll` 调用，目的是询问内核数据准备好了吗？
2. 等内核把数据准备好了，用户线程再发起 `read` 调用，在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的。

那为什么叫 I/O 多路复用呢？因为一次 `select/poll/epoll` 调用只使用一个进程/线程来管理请求（**复用**一个进程/线程），但是可以向内核查多个数据通道（Channel）的状态（多路），只要有一个通道有数据，这些函数都能叫醒服务器线程让线程开始 `read` 数据。

![image-20220704194023244](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704194023244.png)

* 优点：可以基于一个线程，同时在多个通道上等待，而不是使用多个线程，这样可以大大节省系统资源。

* 缺点：当连接数较少时效率相比多线程 + 阻塞 I/O 模型效率较低，可能延迟更大，因为单个连接处理需要 2 次系统调用，占用时间会有增加。

最著名的多路复用实现当属 Linux 的三大系统调用 `select`、`poll` 以及 `epoll`。三者对比表格如下，我们在下一小节会详细讲解：

|                      | `select`                                              | `poll`                                              | `epoll`                                                      |
| -------------------- | ----------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| 操作方式             | 遍历                                                  | 遍历                                                | 回调                                                         |
| 数据结构             | 位图                                                  | 链表                                                | 链表 + 红黑树                                                |
| 默认最大连接数       | 1024                                                  | 无上限                                              | 无上限                                                       |
| 最大支持文件描述符数 | 一般有最大值限制                                      | 65535                                               | 65535                                                        |
| fd 拷贝              | 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态 | 每次调用 poll，都需要把 fd 集合从用户态拷贝到内核态 | 首次调用 `epoll_ctl`  被拷贝，之后每次调用`epoll_wait` 不拷贝 |
| 工作效率             | O(n)                                                  | O(n)                                                | 回调时 O(1)，在红黑树中查找时 O(logn)                        |

#### 2.3.1 `select`

`select` 是 Linux 提供的最早的 I/O 多路复用函数，它实现多路复用的方式是：将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝到内核里，让内核来检查是否有网络事件产生**，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写，接着再把整个文件描述符集合**拷贝回用户态**里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

所以，对于 `select` 这种方式，需要进行 **2 次遍历文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次拷贝文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

所以 **`select` 具有 `O(n)` 的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

它的函数签名如下：

```c
int select(int maxfdp, fd_set *readfds, 
           fd_set *writefds, fd_set *errorfds, 
           struct timeval *timeout);
```

* `maxfdp`：集合中所有文件描述符的范围，即所有文件描述符中最大的那个再 + 1。
* `readfds`：发生**写事件**的文件描述符集合。Linux 中万物皆文件，网络流也是一个文件，用文件描述符表示。
* `writefds`：发生**读事件**的文件描述符集合。
* `errorfds`：发生错误的文件描述符集合。
* `timeout`：超时时间，超过这个时间不管有没有数据到来，一律返回。

要理解 `select` 的原理，就要理解 `fd_set` 这个数据结构，它的每一位都可以对应一个文件描述符：

```c
typedef __kernel_fd_set fd_set;

typedef struct {
	unsigned long fds_bits [__FDSET_LONGS]; //定义一个数组
} __kernel_fd_set;
```

`fd_set` 其实就是一个数组，内核用一个位来表示一个文件描述符。

缺点：

1. 单个进程所打开的 fd 是有限制的，通过 `FD_SETSIZE` 设置，默认 1024；
2. 每次调用 `select`，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大；
3. 扫描是线性扫描，采用轮询的方法，效率较低

#### 2.3.2 `poll`

`poll` 本质上和 `select` 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 fd 对应的设备状态，**但是它没有最大连接数的限制**，原因是它是基于**链表**来存储的。

函数签名：

```c
int poll(struct pollfd fds[], nfds_t nfds, int timeout);
```

```c
struct pollfd {
    int fd;                         // 需要监视的文件描述符
    short events;                   // 需要内核监视的事件
    short revents;                  // 实际发生的事件
};
```

它依然是线性扫描，以此效率并不比 `select` 高。

缺点：除了不限制 fd 数目，`select` 有的缺点它都有。

#### 2.3.3 `epoll`

epoll 通过两个方面，很好解决了 select/poll 的问题。

1. epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里。

   红黑树是个高效的数据结构，增删改一般时间复杂度是 $O(logn)$。而 select/poll 没有在内核里维护类似红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket，所以**只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配，同时，查找速度也比直接遍历所有文件描述符要快**。

2. epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

Linux 提供了三个关于 `epoll` 的函数：

```c
// 创建一个 eventpoll 对象，把所有需要监听的 socket 都放到该对象中
int epoll_create(int size); 

// epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 

// epoll_wait 负责检测可读队列，没有可读 socket 则阻塞进程
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

其的基本使用方式如下图所示：

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328160231315.png" alt="image-20220328160231315" style="zoom:80%;" />

优点：

1. 没有最大并发连接的限制，能打开的 fd 的上限远大于 1024。

2. 效率提升，不是轮询的方式，不会随着 fd 数目的增加效率下降。

   只有活跃可用的 fd 才会调用 callback 函数，即 `epoll` 最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，`epoll` 的效率就会远远高于 `select` 和 `poll`。

3. 内存拷贝，利用 `mmap` 文件映射内存加速与内核空间的消息传递；即 `epoll` 使用 `mmap` 减少复制开销。

缺点：`epoll` 占用的空间相对较高，且只能工作在 Linux 下，代码复用性比较差。

##### 2.3.3.1 源码简要解析

`epoll_create` 创建一个 eventpoll 结构体，这个结构体定义如下：

```c
struct eventpoll {
  	// ...
  
    // 红黑树的根节点，这颗树中存储着所有添加到 epoll 中的需要监控的事件
    struct rb_root  rbr;
    // 双链表中则存放着将要通过 epoll_wait 返回给用户的满足条件的事件
    struct list_head rdlist;
  
  	// ...
};
```

在 `epoll` 中，对于每一个放在 `epoll` 中的事件，都会建立一个 `epitem` 结构体：

```c
struct epitem{    
    // 红黑树节点 
    struct rb_node rbn;   
    // 双向链表节点 
    struct list_head rdllink;  
    // 事件句柄信息
    struct epoll_filefd ffd;  
    // 指向其所属的 eventpoll 对象
    struct eventpoll *ep;     
    // 期待发生的事件类型
    struct epoll_event event;
}
```

当调用 `epoll_wait` 检查是否有事件发生时，只需要检查 `eventpoll` 对象中的 `rdlist` 双链表中是否有 `epitem` 元素即可。如果 `rdlist` 不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。

示意图如下：

![image-20220308221959605](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220308221959605.png)

由于需要监控的事件很多，因此将这类事件放在红黑树中是高效的；而需要满足用户的事件相对较少，因此可以放在链表中。

##### 2.3.3.2 epoll 事件触发方式

epoll 支持两种事件触发模式，分别是**边缘触发（edge-triggered，ET）**和**水平触发（level-triggered，LT）**。

- **使用边缘触发模式时**，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；
- **使用水平触发模式时**，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

好像有点难以理解，举个例子：你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。

水平触发的意思是：只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是：只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

### 2.4 信号驱动 I/O（Signal-Driven I/O）

信号驱动 I/O 是指进程预先告知内核，使得当某个描述符上发生某事时，内核使用信号通知相关进程。

首先开启信号驱动 I/O 功能，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个 `SIGIO` 信号，可以在信号处理函数中调用 I/O 操作函数处理数据。

![image-20220704193949612](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704193949612.png)

* 优点：线程并没有在等待数据时被阻塞，可以提高资源的利用率。
* 缺点：信号 I/O 在大量 I/O 操作时可能会因为信号队列溢出导致没法通知。

### 2.5 异步 I/O（Asynchronous I/O）

用户线程发起 `read` 调用的同时注册一个回调函数，`read` 立即返回，等内核将数据准备好后，再调用指定的回调函数完成处理。在这个过程中，用户线程一直没有阻塞。

![image-20220704193925570](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704193925570.png)

* 优点：异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠。

* 缺点：要实现真正的异步 I/O，操作系统需要做大量的工作。

  目前 Windows 下通过 IOCP 实现了真正的异步 I/O；而 Linux 还不完善，目前一般使用 I/O 多路复用。

## 3. 网络模式

现代应用程序大多是网络应用程序，一个高性能的服务器就必不可少了。

网络模式就是基于传统 I/O 模型发展出的服务端处理网络请求的 I/O 模式。

### 3.1 单线程模式

如果要让服务器服务多个客户端，那么最直接的方式就是为每一条连接创建线程。

其实创建进程也是可以的，原理是一样的，进程和线程的区别在于线程比较轻量级些，线程的创建和线程间切换的成本要小些，为了描述简述，后面都以线程为例。

处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，而且如果要连接几万条连接，创建几万个线程去应对也是不现实的。

### 3.2 单线程 + 线程池模式

我们其实不用为每个连接创建线程，而是创建一个线程池，将连接分配给线程，然后一个线程可以复用，处理多个连接的业务，这样就避免了大量线程创建和销毁的开销。

不过，这样又引来一个新的问题，线程怎样才能高效地处理多个连接的业务？

当一个连接对应一个线程时，线程一般采用 read -> 业务处理 -> send 的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 `read` 操作上（socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。

但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 `read` 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。

要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 `read` 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。

上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 `read` 去试探，所以，我们可以考虑采用 **I/O 多路复用**。

但是，用过 I/O 多路复用接口写网络程序的同学肯定知道，I/O 多路复用（select/poll/epoll）是面向过程的方式写代码的，这样的开发的效率不高。

于是，大佬们基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写，这就是我们即将要介绍的 Reactor 模式。

### 3.3 Reactor 模式

Reactor 翻译过来的意思是反应堆，可能大家会联想到物理学里的核反应堆，实际上并不是的这个意思。

这里的反应指的是**对事件反应**，也就是**来了一个事件，Reactor 就有相对应的反应/响应**。

事实上，Reactor 模式也叫 `Dispatcher` 模式，我觉得这个名字更贴合该模式的含义，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有一个，也可以有多个；
- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

将上面的两个因素排列组设一下，理论上就可以有 4 种方案选择：

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多进程 / 线程；
- 多 Reactor 单进程 / 线程；
- 多 Reactor 多进程 / 线程；

其中，“多 Reactor 单进程 / 线程”实现方案相比“单 Reactor 单进程 / 线程“方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。

剩下的 3 个方案都是比较经典的，且都有应用在实际的项目中，方案具体使用进程还是线程，要看使用的编程语言以及平台有关：

- Java 语言一般使用线程，比如 Netty;
- C 语言使用进程和线程都可以，例如 Nginx 使用的是进程，Memcache 使用的是线程。

#### 3.3.1 单 Reactor 单进程 / 线程

示意图如下：

![image-20220704192057123](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704192057123.png)

可以看到进程里有 **Reactor、Acceptor、Handler** 这三个对象：

- Reactor 对象的作用是监听和分发事件；
- Acceptor 对象的作用是获取连接；
- Handler 对象的作用是处理业务；

这个方案详细如下：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。

但是，这种方案存在 2 个缺点：

- 第一个缺点，因为只有一个进程，**无法充分利用多核 CPU 的性能**；
- 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

所以，单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。

> 在 Redis 6.0 版本之前采用的正是单 Reactor 单进程的方案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的方案。

#### 3.3.2 单 Reactor 多进程 / 多线程

如果要克服上一个方案的缺点，那么就需要引入多线程 / 多进程，这样就产生了**单 Reactor 多线程 / 多进程**的方案。

![image-20220704192351876](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704192351876.png)

详细说一下这个方案：

- Reactor 对象通过 select （I/O 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；

上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：

- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
- 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

单 Reator 多线程的方案优势在于**能够充分利用多核 CPU**，但是既然引入了多线程，那么自然就带来了多线程竞争资源的问题。

单 Reactor 的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能瓶颈**。

#### 3.3.3 多 Reactor 多进程 / 线程

要解决单 Reactor 性能瓶颈的问题，就是将单 Reactor 实现成多 Reactor，这样就产生了**多 Reactor 多进程 / 线程**的方案。

![image-20220704192723862](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704192723862.png)

方案详细说明如下：

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

> 大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了多 Reactor 多线程的方案。

### 3.4 Proactor

前面提到的 Reactor 是**非阻塞同步**网络模式，而 **Proactor 是异步网络模式**。

它和 Reactor 的区别如下：

- Reactor 是非阻塞同步网络模式，感知的是就绪**可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- Proactor 是异步网络模式，感知的是**已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。

因此，Reactor 可以理解为：**来了事件操作系统通知应用进程，让应用进程来处理**，而 Proactor 可以理解为**来了事件操作系统来处理，处理完再通知应用进程**。这里的事件就是有新连接、有数据可读、有数据可写的这些 I/O 事件，这里的处理包含从驱动读取到内核以及从内核读取到用户空间。

Proactor 的示意图如下：

![image-20220704194525453](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220704194525453.png)

Proactor 模式的工作流程：

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；
- Handler 完成业务处理；

可惜的是，在 Linux 下的异步 I/O 是不完善的， aio 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。

而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 IOCP，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。
