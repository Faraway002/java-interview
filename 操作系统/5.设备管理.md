# 设备管理

除了提供抽象（例如，进程（和线程）、地址空间和文件）以外，操作系统还要控制计算机的所有 I/O（输入/输出）设备。操作系统必须向设备发送命令，捕捉中断，并处理设备的各种错误。它还应该在设备和系统的其他部分之间提供简单且易于使用的接口。

## I/O 硬件原理

不同的人对于 I/O 硬件的理解是不同的。对于电子工程师而言，I/O 硬件就是芯片、导线、电源、电机和其他组成硬件的物理部件。对程序员而言，则只注意 I/O 硬件提供给软件的接口，如硬件能够接收的命令、它能够完成的功能以及它能够报告的错误。

### I/O 设备

I/O 设备大致可以分为两类：块设备（block device）和字符设备（character device）。

* 块设备：**块设备把数据存储在固定大小的块中，每个块有自己的地址，且能独立于其它块进行读写**。硬盘、U 盘等都是最常见的块设备。
* 字符设备：**字符设备以字符为单位发送或接收一个字符流，而不考虑任何块结构**。打印机、网络接口、鼠标等与硬盘类型不同的大部分设备都能看作字符设备。

除上述分类之外，还有一些特殊的设备，比如时钟，它既不是块设备，也不是字符设备，但是**块设备和字符设备的抽象具有足够的一般性**。

这些 I/O 设备的速度有快有慢，快的能够达到几百 MB/S，而慢的设备只有几个字节/秒，要使软件在跨越这么多数量级的数据率下保证性能优良，给软件造成了相当大的压力。

### 设备控制器

I/O 设备一般由机械部件和电子部件两部分组成。通常可以将这两部分分开处理，以提供更加模块化和更加通用的设计。

**电子部件称作设备控制器（device controller）或适配器（adapter）**，而机械部件则是设备本身。

示意图如下：

![image-20220325105806819](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220325105806819.png)

控制器与设备本身的接口通常是很低级的，**操作系统并不涉及这些低级的接口，而是直接通过控制器来操作 I/O 设备**。

控制器内部是有芯片的，也有自己的寄存器，用来与 CPU 通信，**通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作**；**通过读取这些寄存器，操作系统可以了解设备的状态**，比如是否准备好接收一个新的命令等。

通常来说，控制器内部有三类寄存器：

- **数据寄存器**：CPU 向 I/O 设备写入**需要传输的数据**，比如要打印的内容是 `Hello`，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- **命令寄存器**：CPU 发送一个**命令**，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- **状态寄存器**：目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

除此之外，块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**，这是为了减少对设备的频繁操作。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。

### 通信方式

CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：

- **端口 I/O**：每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器。

  I/O 端口是一个 8 位或 16 位的整数，所有的 I/O 端口形成 I/O 端口空间，并且受到保护使得普通的用户程序不能对其进行访问（只有 OS 能够进行访问）。

  这一方案的最大缺点就在于**进程地址空间和设备 I/O 地址空间是完全不同的**。如下图所示：

  ![image-20220325111809823](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220325111809823.png)

- **内存映射 I/O**：将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。

  每个控制寄存器被分配唯一的一个内存地址，并且不会有其他内存分配这一地址。通常，被分配给寄存器的地址位于地址空间的顶端。

更一般的情况是两种方式混用，这一方案具有内存映射 I/O 的数据缓冲区，而控制寄存器则具有单独的 I/O 端口。

## I/O 软件原理

I/O 软件的设计目标有如下几点：

1. **设备独立性**。I/O 软件可以访问任何 I/O 设备而无需事先指定设备，同样的，用户应该能使用相同的一套命令对任何设备生效。

2. **统一命名**。一个文件或一个设备的名字应该是 一个简单的字符串或一个整数，它不应依赖于设备。

   在 UNIX 系统中，所有存储盘都能以任意方式集成到文件系统层次结构中，因此，用户不必知道哪个名字对应于哪台设备。

3. **错误处理**。一般来说，错误应该尽可能地在接近硬件的层面得到处理。当控制器发现了一个读错误时，如果它能够处理那么就应该自己设法纠正这一错误。如果控制器处理不了，那么设备驱动程序应当予以处理，可能只需重读一次这块数据就正确了。

4. **同步和异步传输**。大多数物理 I/O 是异步的，即 CPU 启动传输后便转去做其他工作，直到在 I/O 完成。如果 I/O 是阻塞的，那么用户程序就更加容易编写，发起 I/O 请求后，程序将直接进入阻塞，直到数据准备完成。

5. **缓冲**。数据离开一个设备之后通常并不能直接存放到其最终的目的地。某些设备具有严格的实时约束（例如，数字音频设备），所以数据必须预先放置到输出缓冲区之中，从而消除缓冲区填满速率和缓冲区清空速率之间的相互影响，以避免缓冲区欠载。

6. **设备的共享性和独占性**。有些 I/O 设备（如磁盘）能够同时让多个用户使用，多个用户同时在同一磁盘上打开文件不会引起什么问题；其他设备（如磁带机）则必须由单个用户独占使用，直到该用户使用完。

### 程序控制 I/O

**CPU 发出指令以后，不断询问设备寄存器的状态，直到完成一条指令为止**。这种方式简单，但是会占用 CPU 的全部时间，也叫做轮询方式。

使用伪代码可以描述这个过程：

```c
copy_from_user(buffer, p, count);

for (int i = 0; i < count; ++i) {
    while (*reg_status != READY);
    
    *reg_data = p[i];
}
```

在一些简单的嵌入式系统中，CPU 无其他事可做，这种方式自然是合理的，但是在复杂的系统中，CPU 有大量其他工作要做，则这种方式是十分低效的，需要有更好的手段。

### 中断驱动 I/O

我们已经介绍过中断，事实上，中断最初被发明出来就是为了 I/O 时解放 CPU，**当设备完成任务后，控制器发出一个中断，CPU 则会收到中断，然后处理这个中断**。

中断一定程度上解决了占用 CPU 的问题，但是对于磁盘这种频繁读写的设备来说，CPU 是很容易被中断的，中断时就意味着要为程序的上下文保存现场，处理完毕以后要恢复，这个开销是比较大的。

### 使用 DMA 的 IO

对于上面描述的 I/O 频繁的设备来说，DMA（Direct Memory Access）的方式更适合它们，**DMA 可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存**。

这是由 **DMA 控制器**做到的，事实上也就是 DMA 代替 CPU 完成 I/O 工作，本质上属于程序控制 I/O。

那要实现 DMA 功能，就要有 DMA 控制器硬件的支持。

以磁盘为例，DMA 的工作方式如下：

- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的位置就可以了。
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存。
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器。
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了。

示意图如下：

![image-20220328145719554](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328145719554.png)

可以看到， CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。仅仅在传送开始和结束时需要 CPU 干预。

### 零拷贝

普通的 I/O 是非常慢的，以一次网络 I/O 为例：

![image-20220328154400617](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328154400617.png)

* 首先，上述过程期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。

  上下文切换的成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。

* 其次，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的。

  这个过程如下：

  - 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
  - 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
  - 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
  - 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。

如果想要提高 I/O 的性能，就必须减少上下文切换的次数以及减少拷贝数据的次数。

**零拷贝就是一种能给提高 I/O 性能的技术**，它的实现方式通常有两种：

- `mmap`：`mmap` 系统调用函数会直接把内核缓冲区里的数据**映射**到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

- `sendfile`：`sendfile` 是用于代替 `read` 和 `write` 的新的系统调用，使用它可以减少两次上下文切换，而且它还能直接把数据从内核中的源缓冲区拷贝到目的缓冲区中，而不需要经过用户态的切换：

  ![image-20220328155134350](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328155134350.png)

这就是所谓的**零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。**

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，**只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。**

所以，总体来看，**零拷贝技术可以把文件传输的性能提高至少一倍以上**。

## I/O 软件层次

I/O软件通常组织成四个层次，如下图所示：

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328150544817.png" alt="image-20220328150544817" style="zoom:67%;" />

其中，设备的驱动程序是很重要的，它介于操作系统和硬件之间，虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽设备控制器的差异，引入了**设备驱动程序**。

### 设备驱动程序

![image-20220328152551175](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152551175.png)

设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。

不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。如下图：

![image-20220328152706950](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152706950.png)

操作系统实际会在设备驱动程序里响应设备发出的中断请求，通常，设备驱动程序初始化的时候，要先注册一个该设备的中断服务例程。

![image-20220328152730849](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328152730849.png)

### 通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

通用块层是处于**文件系统**和**块设备驱动程序**中间的一个块设备抽象层，它主要有两个功能：

- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 内存支持 5 种 I/O 调度算法，分别是：

- 没有调度算法：它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。
- 先入先出调度算法：先进入 I/O 调度队列的 I/O 请求先发生
- 完全公平调度算法：大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。
- 优先级调度：优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。
- 最终期限调度算法：该算法分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。

## I/O 模型

I/O 模型可以表示应用程序中 I/O 的处理方式，传统的 I/O 模型有 5 种，大部分应用程序和操作系统都实现了其中的几种：

- 同步阻塞 I/O
- 同步非阻塞 I/O
- I/O 多路复用
- 信号驱动 I/O
- 异步 I/O

可以从以下两个维度来理解 I/O 模型：

- 同步或异步（synchronous / asynchronous）。简单来说，同步是一种可靠的有序运行机制，当我们进行同步操作时，后续的任务是等待当前调用返回，才会进行下一步；而异步则相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。
- 阻塞与非阻塞（blocking / non-blocking）。在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 ServerSocket 新连接建立完毕，或数据读取、写入操作完成；而非阻塞则是不管 I/O 操作是否结束，直接返回，相应操作在后台继续处理。

不能一概而论认为同步或阻塞就是低效，具体还要看应用和系统特征。

对于一个网络 I/O 通信过程，比如网络数据读取，会涉及两个对象，一个是调用这个 I/O 操作的用户线程，另外一个就是操作系统内核。一个进程的地址空间分为用户空间和内核空间，用户线程不能直接访问内核空间。

当用户线程发起 I/O 操作后，网络数据读取操作会经历两个步骤：

- **用户线程等待内核将数据从网卡拷贝到内核空间。**
- **内核将数据从内核空间拷贝到用户空间。**

**各种 I/O 模型的区别就是：它们实现这两个步骤的方式是不一样的。**以下模型中的讲述都以服务器处理网络请求为例。

### 同步阻塞 I/O（Blocking I/O)

服务器接收一个网络请求后，开启一个线程，发起 `read` 调用后，线程就阻塞了，让出 CPU。

内核把数据从网卡/磁盘拷贝到内核空间，接着把数据拷贝到用户空间，再把线程叫醒。

![image-20220308200311764](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220308200311764.png)

* 优点：程序简单，在阻塞等待数据期间进程/线程挂起，基本不会占用 CPU 资源；可以多线程处理多个请求。

* 缺点：每个请求需要独立的进程/线程单独处理，当并发请求量大时，内存占用很大，线程的切换过于频繁导致开销大，这种模型在实际生产中很少使用。

### 同步非阻塞 I/O（Non-Blocking I/O）

服务器接收一个网络请求后，开启一个线程，线程不断的发起 `read` 调用，数据没到内核空间时，每次都返回失败，直到数据到了内核空间。这一次 `read` 调用后，在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的，等数据到了用户空间再把线程叫醒。

![image-20220308200535171](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220308200535171.png)

* 优点：不会阻塞在内核的等待数据过程，每次发起的 I/O 请求可以立即返回，不用阻塞等待，实时性较好。
* 缺点：轮询将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低，所以一般 Web 服务器不使用这种 I/O 模型。

### I/O 多路复用（I/O Multiplexing）

线程的读取操作分成两步：

1. 线程先发起 `select/poll/epoll` 调用，目的是询问内核数据准备好了吗？
2. 等内核把数据准备好了，用户线程再发起 `read` 调用，在等待数据从内核空间拷贝到用户空间这段时间里，线程还是阻塞的。

那为什么叫 I/O 多路复用呢？因为一次 `select/poll/epoll` 调用只使用一个进程/线程来管理请求（**复用**一个进程/线程），但是可以向内核查多个数据通道（Channel）的状态（多路），只要有一个通道有数据，这些函数都能叫醒服务器线程让线程开始 `read` 数据。

![image-20220308201018344](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220308201018344.png)

* 优点：可以基于一个线程，同时在多个通道上等待，而不是使用多个线程，这样可以大大节省系统资源。

* 缺点：当连接数较少时效率相比多线程 + 阻塞 I/O 模型效率较低，可能延迟更大，因为单个连接处理需要 2 次系统调用，占用时间会有增加。

最著名的多路复用实现当属 Linux 的三大系统调用 `select`、`poll` 以及 `epoll`。三者对比表格如下，我们在下一小节会详细讲解：

|                      | `select`                                              | `poll`                                              | `epoll`                                                      |
| -------------------- | ----------------------------------------------------- | --------------------------------------------------- | ------------------------------------------------------------ |
| 操作方式             | 遍历                                                  | 遍历                                                | 回调                                                         |
| 数据结构             | 位图（数组）                                          | 链表                                                | 链表 + 红黑树                                                |
| 最大连接数           | 1024                                                  | 无上限                                              | 无上限                                                       |
| 最大支持文件描述符数 | 一般有最大值限制                                      | 65535                                               | 65535                                                        |
| fd 拷贝              | 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态 | 每次调用 poll，都需要把 fd 集合从用户态拷贝到内核态 | 首次调用 `epoll_ctl`  被拷贝，之后每次调用`epoll_wait` 不拷贝 |
| 工作效率             | O(n)                                                  | O(n)                                                | 回调时 O(1)，在红黑树中查找时 O(logn)                        |

#### `select`

`select` 是 Linux 提供的最早的 I/O 多路复用函数，它仅仅知道了，有 I/O 事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能**无差别轮询所有流**，找出能读出数据，或者写入数据的流，对他们进行操作。

所以 **`select` 具有 `O(n)` 的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

它的函数签名如下：

```c
int select(int maxfdp, fd_set *readfds, 
           fd_set *writefds, fd_set *errorfds, 
           struct timeval *timeout);
```

* `maxfdp`：集合中所有文件描述符的范围，即所有文件描述符中最大的那个再 + 1。
* `readfds`：发生**写事件**的文件描述符集合。Linux 中万物皆文件，网络流也是一个文件，用文件描述符表示。
* `writefds`：发生**读事件**的文件描述符集合。
* `errorfds`：发生错误的文件描述符集合。
* `timeout`：超时时间，超过这个时间不管有没有数据到来，一律返回。

要理解 `select` 的原理，就要理解 `fd_set` 这个数据结构，它的每一位都可以对应一个文件描述符：

```c
typedef __kernel_fd_set fd_set;

typedef struct {
	unsigned long fds_bits [__FDSET_LONGS]; //定义一个数组
} __kernel_fd_set;
```

`fd_set` 其实就是一个数组，内核用一个位来表示一个文件描述符。

缺点：

1. 单个进程所打开的 fd 是有限制的，通过 `FD_SETSIZE` 设置，默认 1024；
2. 每次调用 `select`，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大；
3. 扫描是线性扫描，采用轮询的方法，效率较低

#### `poll`

`poll` 本质上和 `select` 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 fd 对应的设备状态，**但是它没有最大连接数的限制**，原因是它是基于**链表**来存储的。

函数签名：

```c
int poll(struct pollfd fds[], nfds_t nfds, int timeout);
```

```c
struct pollfd {
    int fd;                         // 需要监视的文件描述符
    short events;                   // 需要内核监视的事件
    short revents;                  // 实际发生的事件
};
```

它依然是线性扫描，以此效率并不比 `select` 高。

缺点：除了不限制 fd 数目，`select` 有的缺点它都有。

#### `epoll`

**`epoll`可以理解为 event poll**，不同于忙轮询和无差别轮询，`epoll` 会把哪个流发生了怎样的 I/O 事件通知我们。所以我们说 `epoll` 实际上是**事件驱动（每个事件关联上fd）**的，此时我们对这些流的操作都是有意义的，这使得复杂度降低到了 `O(1)`。

Linux 提供了三个关于 `epoll` 的函数：

```c
// 创建一个 eventpoll 对象，把所有需要监听的 socket 都放到该对象中
int epoll_create(int size); 

// epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 

// epoll_wait 负责检测可读队列，没有可读 socket 则阻塞进程
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

`epoll` 的基本使用方式如下图所示：

<img src="https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220328160231315.png" alt="image-20220328160231315" style="zoom:80%;" />

`epoll_event` 结构体定义如下：

```c
struct eventpoll {
    // 红黑树的根节点，这颗树中存储着所有添加到 epoll 中的需要监控的事件
    struct rb_root  rbr;
    // 双链表中则存放着将要通过 epoll_wait 返回给用户的满足条件的事件
    struct list_head rdlist;
};
```

`epoll` 的原理是：每一个 `epoll` 对象都有一个独立的 `eventpoll` 结构体，用于存放通过 `epoll_ctl` 方法向 `epoll` 对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来（红黑树的插入时间效率是 `O(logn)`）。

所有添加到 `epoll` 中的事件都会与设备驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫 `ep_poll_callback`，它会将发生的事件添加到 `rdlist` 双链表中。

在 `epoll` 中，对于每一个放在 `epoll` 中的事件，都会建立一个 `epitem` 结构体：

```c
struct epitem{    
    // 红黑树节点 
    struct rb_node rbn;   
    // 双向链表节点 
    struct list_head rdllink;  
    // 事件句柄信息
    struct epoll_filefd ffd;  
    // 指向其所属的 eventpoll 对象
    struct eventpoll *ep;     
    // 期待发生的事件类型
    struct epoll_event event;
}
```

当调用 `epoll_wait` 检查是否有事件发生时，只需要检查 `eventpoll` 对象中的 `rdlist` 双链表中是否有 `epitem` 元素即可。如果 `rdlist` 不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。

示意图如下：

![image-20220308221959605](https://cdn.jsdelivr.net/gh/Faraway002/typora/images/image-20220308221959605.png)

由于需要监控的事件很多，因此将这类事件放在红黑树中是十分聪明的；而需要满足用户的事件相对较少，因此可以放在链表中。

优点：

1. 没有最大并发连接的限制，能打开的 fd 的上限远大于 1024。

2. 效率提升，不是轮询的方式，不会随着 fd 数目的增加效率下降。

   只有活跃可用的 fd 才会调用 callback 函数，即 `epoll` 最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，`epoll` 的效率就会远远高于 `select` 和 `poll`。

3. 内存拷贝，利用 `mmap` 文件映射内存加速与内核空间的消息传递；即 `epoll` 使用 `mmap` 减少复制开销。

缺点：`epoll` 占用的空间相对较高，且只能工作在 Linux 下，代码复用性比较差。

### 信号驱动 I/O（Signal-Driven I/O）

信号驱动 I/O 是指进程预先告知内核，使得当某个描述符上发生某事时，内核使用信号通知相关进程。

首先开启信号驱动 I/O 功能，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个 `SIGIO` 信号，可以在信号处理函数中调用 I/O 操作函数处理数据。

* 优点：线程并没有在等待数据时被阻塞，可以提高资源的利用率。
* 缺点：信号 I/O 在大量 I/O 操作时可能会因为信号队列溢出导致没法通知。

### 异步 I/O（Asynchronous I/O）

用户线程发起 `read` 调用的同时注册一个回调函数，`read` 立即返回，等内核将数据准备好后，再调用指定的回调函数完成处理。在这个过程中，用户线程一直没有阻塞。

![image-20220308200917162](https://cdn.jsdelivr.net/gh/Faraway002/typora/imagesimage-20220308200917162.png)

* 优点：异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠。

* 缺点：要实现真正的异步 I/O，操作系统需要做大量的工作。

  目前 Windows 下通过 IOCP 实现了真正的异步 I/O；而 Linux 还不完善，目前一般使用 I/O 多路复用
